+ /gorunner
sep is " " args are "--progress-report-url=http://localhost:8099/progress"split [--progress-report-url=http://localhost:8099/progress]
2022/05/30 06:08:30 Running command:
Command env: []
Run from directory: 
Executable path: /usr/local/bin/ginkgo
Args (comma-delimited): /usr/local/bin/ginkgo,--focus=\[Conformance\],--skip=\[Disruptive\]|NoExecuteTaintManager,--noColor=true,/usr/local/bin/e2e.test,--,--disable-log-dump,--repo-root=/kubernetes,--provider=local,--report-dir=/tmp/sonobuoy/results,--kubeconfig=,--progress-report-url=http://localhost:8099/progress
2022/05/30 06:08:30 Now listening for interrupts
I0530 06:08:34.682320      21 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-592517539
I0530 06:08:34.683197      21 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0530 06:08:34.683431      21 e2e.go:129] Starting e2e run "105f8b04-2c20-4bce-a6e6-8087de21f14f" on Ginkgo node 1
{"msg":"Test Suite starting","total":303,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1653890910 - Will randomize all specs
Will run 303 of 5484 specs

May 30 06:08:34.736: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:08:34.812: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 30 06:08:34.853: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 30 06:08:34.929: INFO: 8 / 8 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 30 06:08:34.929: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 30 06:08:34.929: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 30 06:08:34.947: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
May 30 06:08:34.947: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 30 06:08:34.947: INFO: e2e test version: v1.19.16
May 30 06:08:34.949: INFO: kube-apiserver version: v1.19.16
May 30 06:08:34.949: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:08:34.964: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:08:34.964: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
May 30 06:08:35.137: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 30 06:08:35.167: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-a1934c80-c866-41cc-86e6-87495531f617
STEP: Creating secret with name s-test-opt-upd-1bc9187a-5337-4648-a905-99fde664708e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a1934c80-c866-41cc-86e6-87495531f617
STEP: Updating secret s-test-opt-upd-1bc9187a-5337-4648-a905-99fde664708e
STEP: Creating secret with name s-test-opt-create-6c0ffced-57ad-48ea-a3f4-67ffd07cc8b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:08:53.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-250" for this suite.

• [SLOW TEST:19.009 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":1,"skipped":15,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:08:53.977: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 30 06:08:54.087: INFO: Waiting up to 5m0s for pod "pod-d56ccb9b-9429-4f29-8847-070b53675149" in namespace "emptydir-8900" to be "Succeeded or Failed"
May 30 06:08:54.099: INFO: Pod "pod-d56ccb9b-9429-4f29-8847-070b53675149": Phase="Pending", Reason="", readiness=false. Elapsed: 11.091923ms
May 30 06:08:56.105: INFO: Pod "pod-d56ccb9b-9429-4f29-8847-070b53675149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017676333s
May 30 06:08:58.109: INFO: Pod "pod-d56ccb9b-9429-4f29-8847-070b53675149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021203394s
May 30 06:09:00.466: INFO: Pod "pod-d56ccb9b-9429-4f29-8847-070b53675149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.378695302s
STEP: Saw pod success
May 30 06:09:00.467: INFO: Pod "pod-d56ccb9b-9429-4f29-8847-070b53675149" satisfied condition "Succeeded or Failed"
May 30 06:09:00.484: INFO: Trying to get logs from node my-node pod pod-d56ccb9b-9429-4f29-8847-070b53675149 container test-container: <nil>
STEP: delete the pod
May 30 06:09:01.463: INFO: Waiting for pod pod-d56ccb9b-9429-4f29-8847-070b53675149 to disappear
May 30 06:09:01.980: INFO: Pod pod-d56ccb9b-9429-4f29-8847-070b53675149 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:09:01.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8900" for this suite.

• [SLOW TEST:8.349 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":2,"skipped":74,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:09:02.326: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-bb5e6873-449a-4b69-b9d5-8078a1da9505
STEP: Creating a pod to test consume secrets
May 30 06:09:02.784: INFO: Waiting up to 5m0s for pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8" in namespace "secrets-5161" to be "Succeeded or Failed"
May 30 06:09:02.795: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.938865ms
May 30 06:09:04.801: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017049996s
May 30 06:09:06.811: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027327411s
May 30 06:09:08.817: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033458689s
May 30 06:09:10.822: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038876783s
May 30 06:09:12.866: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.082885747s
May 30 06:09:14.873: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.089757117s
STEP: Saw pod success
May 30 06:09:14.874: INFO: Pod "pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8" satisfied condition "Succeeded or Failed"
May 30 06:09:14.876: INFO: Trying to get logs from node my-node pod pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8 container secret-env-test: <nil>
STEP: delete the pod
May 30 06:09:15.142: INFO: Waiting for pod pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8 to disappear
May 30 06:09:15.145: INFO: Pod pod-secrets-9ab2873c-e9d0-4bd7-b20a-be8ff11acda8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:09:15.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5161" for this suite.

• [SLOW TEST:12.836 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":303,"completed":3,"skipped":87,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:09:15.162: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:09:15.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4681" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":4,"skipped":103,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:09:15.262: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 30 06:09:15.477: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:09:15.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-820" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":303,"completed":5,"skipped":114,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:09:15.517: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-b4a4e059-5567-47a5-a763-0659c5661712
STEP: Creating a pod to test consume configMaps
May 30 06:09:15.592: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b" in namespace "projected-3263" to be "Succeeded or Failed"
May 30 06:09:15.600: INFO: Pod "pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.57873ms
May 30 06:09:17.681: INFO: Pod "pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087929173s
May 30 06:09:19.760: INFO: Pod "pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.166640299s
STEP: Saw pod success
May 30 06:09:19.760: INFO: Pod "pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b" satisfied condition "Succeeded or Failed"
May 30 06:09:19.775: INFO: Trying to get logs from node my-node pod pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 30 06:09:19.812: INFO: Waiting for pod pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b to disappear
May 30 06:09:19.833: INFO: Pod pod-projected-configmaps-09d67cde-f718-4112-94a7-a2419f94866b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:09:19.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3263" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":6,"skipped":116,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:09:19.855: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
May 30 06:09:19.928: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-5535 proxy --unix-socket=/tmp/kubectl-proxy-unix174893734/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:09:21.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5535" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":303,"completed":7,"skipped":122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:09:21.501: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:09:21.617: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411" in namespace "projected-8695" to be "Succeeded or Failed"
May 30 06:09:21.628: INFO: Pod "downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411": Phase="Pending", Reason="", readiness=false. Elapsed: 11.460231ms
May 30 06:09:23.636: INFO: Pod "downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018977497s
May 30 06:09:25.639: INFO: Pod "downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022444782s
STEP: Saw pod success
May 30 06:09:25.639: INFO: Pod "downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411" satisfied condition "Succeeded or Failed"
May 30 06:09:25.642: INFO: Trying to get logs from node my-node pod downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411 container client-container: <nil>
STEP: delete the pod
May 30 06:09:25.702: INFO: Waiting for pod downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411 to disappear
May 30 06:09:25.711: INFO: Pod downwardapi-volume-82ded4dc-6ac3-47ed-8050-16d42813f411 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:09:25.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8695" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":8,"skipped":152,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:09:25.725: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-dcc6ea0e-7cdc-4376-874c-08051c38ff5f
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-dcc6ea0e-7cdc-4376-874c-08051c38ff5f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:10:52.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2239" for this suite.

• [SLOW TEST:86.724 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":9,"skipped":170,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:10:52.449: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-4c67919a-c58b-46a8-91af-1bed27adea65
STEP: Creating configMap with name cm-test-opt-upd-69f584e0-2cd3-4fca-9309-f9850958deb2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4c67919a-c58b-46a8-91af-1bed27adea65
STEP: Updating configmap cm-test-opt-upd-69f584e0-2cd3-4fca-9309-f9850958deb2
STEP: Creating configMap with name cm-test-opt-create-a93bff3c-a369-4981-b813-c97eb2e609c7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:11:02.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2552" for this suite.

• [SLOW TEST:10.315 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":10,"skipped":179,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:11:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-aacec3e2-ae41-4c6d-ada0-1c7fe8a60399 in namespace container-probe-828
May 30 06:11:06.923: INFO: Started pod busybox-aacec3e2-ae41-4c6d-ada0-1c7fe8a60399 in namespace container-probe-828
STEP: checking the pod's current state and verifying that restartCount is present
May 30 06:11:06.926: INFO: Initial restart count of pod busybox-aacec3e2-ae41-4c6d-ada0-1c7fe8a60399 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:15:07.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-828" for this suite.

• [SLOW TEST:245.173 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":11,"skipped":179,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:15:07.938: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-b9cn6 in namespace proxy-698
I0530 06:15:08.137828      21 runners.go:190] Created replication controller with name: proxy-service-b9cn6, namespace: proxy-698, replica count: 1
I0530 06:15:09.193551      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:15:10.193885      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:15:11.195429      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:15:12.197580      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:13.198436      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:14.200094      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:15.201222      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:16.202870      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:17.203707      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:18.205663      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:19.206235      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0530 06:15:20.207406      21 runners.go:190] proxy-service-b9cn6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 06:15:20.211: INFO: setup took 12.206749879s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 30 06:15:20.297: INFO: (0) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 84.584219ms)
May 30 06:15:20.297: INFO: (0) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 84.272711ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 96.845537ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 97.192348ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 96.936171ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 97.246345ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 96.933252ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 96.949201ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 96.986812ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 96.990515ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 96.878998ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 96.911393ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 98.0973ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 97.500466ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 97.019957ms)
May 30 06:15:20.310: INFO: (0) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 98.168091ms)
May 30 06:15:20.316: INFO: (1) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 5.126963ms)
May 30 06:15:20.316: INFO: (1) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 5.057461ms)
May 30 06:15:20.321: INFO: (1) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 9.824989ms)
May 30 06:15:20.321: INFO: (1) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 8.97939ms)
May 30 06:15:20.323: INFO: (1) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 11.31093ms)
May 30 06:15:20.324: INFO: (1) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 12.659969ms)
May 30 06:15:20.324: INFO: (1) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 12.410383ms)
May 30 06:15:20.324: INFO: (1) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 13.140397ms)
May 30 06:15:20.324: INFO: (1) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 13.084956ms)
May 30 06:15:20.324: INFO: (1) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 12.852753ms)
May 30 06:15:20.324: INFO: (1) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 13.398158ms)
May 30 06:15:20.325: INFO: (1) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 12.477116ms)
May 30 06:15:20.325: INFO: (1) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 13.804511ms)
May 30 06:15:20.328: INFO: (1) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 16.713475ms)
May 30 06:15:20.328: INFO: (1) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 16.860988ms)
May 30 06:15:20.328: INFO: (1) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 16.869719ms)
May 30 06:15:20.342: INFO: (2) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 12.94361ms)
May 30 06:15:20.342: INFO: (2) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 13.431803ms)
May 30 06:15:20.343: INFO: (2) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 13.941562ms)
May 30 06:15:20.345: INFO: (2) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 15.953773ms)
May 30 06:15:20.345: INFO: (2) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 16.354702ms)
May 30 06:15:20.345: INFO: (2) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 16.071984ms)
May 30 06:15:20.348: INFO: (2) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 19.007015ms)
May 30 06:15:20.348: INFO: (2) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 18.705761ms)
May 30 06:15:20.348: INFO: (2) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 18.702666ms)
May 30 06:15:20.348: INFO: (2) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 19.050961ms)
May 30 06:15:20.349: INFO: (2) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 19.709624ms)
May 30 06:15:20.349: INFO: (2) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 19.640045ms)
May 30 06:15:20.349: INFO: (2) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 19.664906ms)
May 30 06:15:20.349: INFO: (2) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 19.799976ms)
May 30 06:15:20.349: INFO: (2) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 20.44484ms)
May 30 06:15:20.354: INFO: (2) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 24.605706ms)
May 30 06:15:20.361: INFO: (3) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 7.877013ms)
May 30 06:15:20.362: INFO: (3) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 7.980597ms)
May 30 06:15:20.362: INFO: (3) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 7.759151ms)
May 30 06:15:20.362: INFO: (3) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 7.614583ms)
May 30 06:15:20.362: INFO: (3) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 8.074289ms)
May 30 06:15:20.362: INFO: (3) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 8.228807ms)
May 30 06:15:20.363: INFO: (3) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 8.99378ms)
May 30 06:15:20.368: INFO: (3) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 14.026719ms)
May 30 06:15:20.368: INFO: (3) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 14.294285ms)
May 30 06:15:20.368: INFO: (3) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 14.842583ms)
May 30 06:15:20.368: INFO: (3) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 14.825854ms)
May 30 06:15:20.377: INFO: (3) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 22.467911ms)
May 30 06:15:20.377: INFO: (3) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 22.856072ms)
May 30 06:15:20.377: INFO: (3) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 21.409743ms)
May 30 06:15:20.377: INFO: (3) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 23.241503ms)
May 30 06:15:20.377: INFO: (3) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 22.894422ms)
May 30 06:15:20.388: INFO: (4) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 9.551327ms)
May 30 06:15:20.388: INFO: (4) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 9.784929ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 11.592712ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 11.711643ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 11.753988ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 11.873381ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 11.788327ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 11.866971ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 11.621516ms)
May 30 06:15:20.390: INFO: (4) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 12.995897ms)
May 30 06:15:20.392: INFO: (4) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 14.533828ms)
May 30 06:15:20.395: INFO: (4) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 16.521734ms)
May 30 06:15:20.399: INFO: (4) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 22.465644ms)
May 30 06:15:20.399: INFO: (4) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 22.406762ms)
May 30 06:15:20.399: INFO: (4) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 21.446419ms)
May 30 06:15:20.399: INFO: (4) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 21.411528ms)
May 30 06:15:20.409: INFO: (5) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 7.782663ms)
May 30 06:15:20.409: INFO: (5) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 7.762372ms)
May 30 06:15:20.409: INFO: (5) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 8.484603ms)
May 30 06:15:20.416: INFO: (5) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 14.376939ms)
May 30 06:15:20.417: INFO: (5) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 16.295967ms)
May 30 06:15:20.417: INFO: (5) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 15.972811ms)
May 30 06:15:20.417: INFO: (5) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 15.94958ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 16.81987ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 16.77214ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 17.138252ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 16.893807ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 18.365063ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 18.598126ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 17.358552ms)
May 30 06:15:20.418: INFO: (5) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 17.249951ms)
May 30 06:15:20.419: INFO: (5) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 18.342895ms)
May 30 06:15:20.426: INFO: (6) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 6.454293ms)
May 30 06:15:20.426: INFO: (6) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 5.797566ms)
May 30 06:15:20.426: INFO: (6) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 6.208611ms)
May 30 06:15:20.426: INFO: (6) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 5.828679ms)
May 30 06:15:20.432: INFO: (6) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 12.08509ms)
May 30 06:15:20.432: INFO: (6) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 12.433366ms)
May 30 06:15:20.432: INFO: (6) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 12.631911ms)
May 30 06:15:20.433: INFO: (6) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 12.942105ms)
May 30 06:15:20.438: INFO: (6) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 17.224177ms)
May 30 06:15:20.438: INFO: (6) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 17.991994ms)
May 30 06:15:20.438: INFO: (6) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 17.37606ms)
May 30 06:15:20.438: INFO: (6) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 18.058007ms)
May 30 06:15:20.438: INFO: (6) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 17.69307ms)
May 30 06:15:20.440: INFO: (6) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 18.844864ms)
May 30 06:15:20.440: INFO: (6) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 20.978345ms)
May 30 06:15:20.441: INFO: (6) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 19.903908ms)
May 30 06:15:20.452: INFO: (7) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 11.740978ms)
May 30 06:15:20.468: INFO: (7) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 26.053861ms)
May 30 06:15:20.470: INFO: (7) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 28.219265ms)
May 30 06:15:20.472: INFO: (7) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 30.084066ms)
May 30 06:15:20.472: INFO: (7) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 30.26843ms)
May 30 06:15:20.472: INFO: (7) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 30.246354ms)
May 30 06:15:20.472: INFO: (7) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 30.161ms)
May 30 06:15:20.472: INFO: (7) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 30.181432ms)
May 30 06:15:20.476: INFO: (7) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 34.192995ms)
May 30 06:15:20.476: INFO: (7) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 34.288102ms)
May 30 06:15:20.476: INFO: (7) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 34.363229ms)
May 30 06:15:20.476: INFO: (7) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 34.392231ms)
May 30 06:15:20.476: INFO: (7) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 34.284487ms)
May 30 06:15:20.476: INFO: (7) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 34.261077ms)
May 30 06:15:20.476: INFO: (7) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 34.388853ms)
May 30 06:15:20.477: INFO: (7) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 34.57262ms)
May 30 06:15:20.483: INFO: (8) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 6.197066ms)
May 30 06:15:20.483: INFO: (8) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 6.748948ms)
May 30 06:15:20.483: INFO: (8) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 6.425609ms)
May 30 06:15:20.484: INFO: (8) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 6.324953ms)
May 30 06:15:20.491: INFO: (8) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 13.459637ms)
May 30 06:15:20.491: INFO: (8) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 13.821553ms)
May 30 06:15:20.491: INFO: (8) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 13.811125ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 18.062704ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 18.19013ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 18.232039ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 18.202382ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 18.477376ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 18.506866ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 18.215925ms)
May 30 06:15:20.495: INFO: (8) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 18.253551ms)
May 30 06:15:20.496: INFO: (8) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 18.098232ms)
May 30 06:15:20.507: INFO: (9) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 9.716917ms)
May 30 06:15:20.507: INFO: (9) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 9.840221ms)
May 30 06:15:20.507: INFO: (9) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 9.963731ms)
May 30 06:15:20.510: INFO: (9) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 13.051492ms)
May 30 06:15:20.510: INFO: (9) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 12.946579ms)
May 30 06:15:20.510: INFO: (9) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 13.055597ms)
May 30 06:15:20.511: INFO: (9) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 13.881376ms)
May 30 06:15:20.511: INFO: (9) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 14.252995ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 16.88484ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 17.07441ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 17.054152ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 17.427418ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 17.954417ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 17.394282ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 17.388244ms)
May 30 06:15:20.514: INFO: (9) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 16.883867ms)
May 30 06:15:20.518: INFO: (10) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 3.340775ms)
May 30 06:15:20.523: INFO: (10) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 8.873522ms)
May 30 06:15:20.525: INFO: (10) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 10.743413ms)
May 30 06:15:20.525: INFO: (10) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 10.67553ms)
May 30 06:15:20.525: INFO: (10) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 10.874616ms)
May 30 06:15:20.527: INFO: (10) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 12.174856ms)
May 30 06:15:20.527: INFO: (10) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 12.253638ms)
May 30 06:15:20.527: INFO: (10) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 12.221678ms)
May 30 06:15:20.527: INFO: (10) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 12.218897ms)
May 30 06:15:20.529: INFO: (10) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 14.877665ms)
May 30 06:15:20.530: INFO: (10) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 14.967477ms)
May 30 06:15:20.533: INFO: (10) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 18.480933ms)
May 30 06:15:20.533: INFO: (10) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 18.455408ms)
May 30 06:15:20.533: INFO: (10) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 18.497198ms)
May 30 06:15:20.533: INFO: (10) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 18.763317ms)
May 30 06:15:20.538: INFO: (10) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 23.737338ms)
May 30 06:15:20.548: INFO: (11) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 8.57025ms)
May 30 06:15:20.548: INFO: (11) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 8.963874ms)
May 30 06:15:20.548: INFO: (11) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 9.671631ms)
May 30 06:15:20.548: INFO: (11) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 10.143746ms)
May 30 06:15:20.549: INFO: (11) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 9.553767ms)
May 30 06:15:20.549: INFO: (11) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 9.59871ms)
May 30 06:15:20.549: INFO: (11) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 10.119633ms)
May 30 06:15:20.549: INFO: (11) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 9.589819ms)
May 30 06:15:20.549: INFO: (11) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 10.234351ms)
May 30 06:15:20.549: INFO: (11) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 10.233127ms)
May 30 06:15:20.556: INFO: (11) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 17.121328ms)
May 30 06:15:20.556: INFO: (11) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 17.505969ms)
May 30 06:15:20.557: INFO: (11) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 18.517543ms)
May 30 06:15:20.557: INFO: (11) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 18.547548ms)
May 30 06:15:20.557: INFO: (11) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 18.056419ms)
May 30 06:15:20.562: INFO: (11) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 23.116479ms)
May 30 06:15:20.575: INFO: (12) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 12.970193ms)
May 30 06:15:20.577: INFO: (12) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 14.719862ms)
May 30 06:15:20.577: INFO: (12) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 14.201136ms)
May 30 06:15:20.577: INFO: (12) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 14.271169ms)
May 30 06:15:20.577: INFO: (12) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 14.270619ms)
May 30 06:15:20.577: INFO: (12) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 15.112727ms)
May 30 06:15:20.579: INFO: (12) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 15.970427ms)
May 30 06:15:20.579: INFO: (12) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 16.739209ms)
May 30 06:15:20.579: INFO: (12) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 16.33139ms)
May 30 06:15:20.580: INFO: (12) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 16.854116ms)
May 30 06:15:20.580: INFO: (12) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 17.106257ms)
May 30 06:15:20.582: INFO: (12) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 19.397963ms)
May 30 06:15:20.582: INFO: (12) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 20.08243ms)
May 30 06:15:20.582: INFO: (12) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 19.15784ms)
May 30 06:15:20.582: INFO: (12) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 19.183691ms)
May 30 06:15:20.582: INFO: (12) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 19.296727ms)
May 30 06:15:20.592: INFO: (13) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 9.104595ms)
May 30 06:15:20.593: INFO: (13) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 10.247959ms)
May 30 06:15:20.593: INFO: (13) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 10.384862ms)
May 30 06:15:20.593: INFO: (13) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 10.248274ms)
May 30 06:15:20.593: INFO: (13) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 10.243859ms)
May 30 06:15:20.593: INFO: (13) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 10.178932ms)
May 30 06:15:20.596: INFO: (13) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 13.236089ms)
May 30 06:15:20.598: INFO: (13) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 14.923667ms)
May 30 06:15:20.599: INFO: (13) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 15.602037ms)
May 30 06:15:20.599: INFO: (13) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 16.420606ms)
May 30 06:15:20.599: INFO: (13) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 16.322937ms)
May 30 06:15:20.599: INFO: (13) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 16.278514ms)
May 30 06:15:20.600: INFO: (13) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 17.348974ms)
May 30 06:15:20.600: INFO: (13) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 17.350997ms)
May 30 06:15:20.600: INFO: (13) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 18.0705ms)
May 30 06:15:20.600: INFO: (13) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 17.317522ms)
May 30 06:15:20.608: INFO: (14) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 7.01327ms)
May 30 06:15:20.608: INFO: (14) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 7.110154ms)
May 30 06:15:20.608: INFO: (14) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 7.06658ms)
May 30 06:15:20.608: INFO: (14) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 7.048597ms)
May 30 06:15:20.608: INFO: (14) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 7.039662ms)
May 30 06:15:20.608: INFO: (14) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 7.041874ms)
May 30 06:15:20.614: INFO: (14) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 12.507643ms)
May 30 06:15:20.614: INFO: (14) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 12.773166ms)
May 30 06:15:20.615: INFO: (14) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 13.889508ms)
May 30 06:15:20.616: INFO: (14) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 14.048182ms)
May 30 06:15:20.616: INFO: (14) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 14.082713ms)
May 30 06:15:20.616: INFO: (14) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 14.838787ms)
May 30 06:15:20.616: INFO: (14) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 14.37762ms)
May 30 06:15:20.618: INFO: (14) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 16.780627ms)
May 30 06:15:20.618: INFO: (14) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 16.450627ms)
May 30 06:15:20.618: INFO: (14) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 16.721047ms)
May 30 06:15:20.624: INFO: (15) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 5.624156ms)
May 30 06:15:20.626: INFO: (15) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 7.524251ms)
May 30 06:15:20.627: INFO: (15) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 8.871574ms)
May 30 06:15:20.630: INFO: (15) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 12.241928ms)
May 30 06:15:20.631: INFO: (15) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 12.249082ms)
May 30 06:15:20.631: INFO: (15) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 12.507832ms)
May 30 06:15:20.631: INFO: (15) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 12.298677ms)
May 30 06:15:20.631: INFO: (15) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 12.254486ms)
May 30 06:15:20.631: INFO: (15) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 12.251679ms)
May 30 06:15:20.631: INFO: (15) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 12.577336ms)
May 30 06:15:20.642: INFO: (15) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 23.795381ms)
May 30 06:15:20.643: INFO: (15) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 24.599867ms)
May 30 06:15:20.645: INFO: (15) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 26.787271ms)
May 30 06:15:20.645: INFO: (15) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 26.832099ms)
May 30 06:15:20.646: INFO: (15) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 27.629017ms)
May 30 06:15:20.646: INFO: (15) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 27.663534ms)
May 30 06:15:20.653: INFO: (16) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 6.609616ms)
May 30 06:15:20.653: INFO: (16) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 6.387629ms)
May 30 06:15:20.653: INFO: (16) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 7.041899ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 7.632725ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 7.449135ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 7.435759ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 7.893204ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 8.329541ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 7.991221ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 7.594316ms)
May 30 06:15:20.654: INFO: (16) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 7.845709ms)
May 30 06:15:20.656: INFO: (16) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 8.764879ms)
May 30 06:15:20.658: INFO: (16) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 11.477283ms)
May 30 06:15:20.660: INFO: (16) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 13.547368ms)
May 30 06:15:20.662: INFO: (16) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 15.290247ms)
May 30 06:15:20.663: INFO: (16) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 16.412879ms)
May 30 06:15:20.670: INFO: (17) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 6.607082ms)
May 30 06:15:20.671: INFO: (17) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 7.850955ms)
May 30 06:15:20.671: INFO: (17) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 7.899696ms)
May 30 06:15:20.672: INFO: (17) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 8.366596ms)
May 30 06:15:20.674: INFO: (17) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 9.039713ms)
May 30 06:15:20.674: INFO: (17) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 9.980134ms)
May 30 06:15:20.674: INFO: (17) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 9.44762ms)
May 30 06:15:20.674: INFO: (17) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 9.399309ms)
May 30 06:15:20.674: INFO: (17) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 9.784802ms)
May 30 06:15:20.675: INFO: (17) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 10.41606ms)
May 30 06:15:20.675: INFO: (17) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 10.235756ms)
May 30 06:15:20.679: INFO: (17) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 14.698545ms)
May 30 06:15:20.679: INFO: (17) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 14.719433ms)
May 30 06:15:20.679: INFO: (17) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 14.621394ms)
May 30 06:15:20.681: INFO: (17) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 16.926281ms)
May 30 06:15:20.682: INFO: (17) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 17.606267ms)
May 30 06:15:20.688: INFO: (18) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 6.054548ms)
May 30 06:15:20.688: INFO: (18) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 5.340639ms)
May 30 06:15:20.693: INFO: (18) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 10.341568ms)
May 30 06:15:20.693: INFO: (18) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 9.958023ms)
May 30 06:15:20.693: INFO: (18) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 10.046212ms)
May 30 06:15:20.693: INFO: (18) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 9.991228ms)
May 30 06:15:20.693: INFO: (18) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 9.977878ms)
May 30 06:15:20.693: INFO: (18) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 10.452861ms)
May 30 06:15:20.698: INFO: (18) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 15.005398ms)
May 30 06:15:20.698: INFO: (18) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 15.065369ms)
May 30 06:15:20.698: INFO: (18) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 15.090803ms)
May 30 06:15:20.698: INFO: (18) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 15.077544ms)
May 30 06:15:20.700: INFO: (18) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 17.180496ms)
May 30 06:15:20.700: INFO: (18) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 17.287122ms)
May 30 06:15:20.700: INFO: (18) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 17.224755ms)
May 30 06:15:20.702: INFO: (18) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 19.143517ms)
May 30 06:15:20.706: INFO: (19) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">t... (200; 3.898926ms)
May 30 06:15:20.710: INFO: (19) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 7.499946ms)
May 30 06:15:20.710: INFO: (19) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname2/proxy/: bar (200; 7.667971ms)
May 30 06:15:20.710: INFO: (19) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:462/proxy/: tls qux (200; 7.482849ms)
May 30 06:15:20.713: INFO: (19) /api/v1/namespaces/proxy-698/pods/http:proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 11.032667ms)
May 30 06:15:20.714: INFO: (19) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:460/proxy/: tls baz (200; 11.971293ms)
May 30 06:15:20.714: INFO: (19) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:162/proxy/: bar (200; 11.836063ms)
May 30 06:15:20.720: INFO: (19) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:160/proxy/: foo (200; 18.091143ms)
May 30 06:15:20.721: INFO: (19) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2:1080/proxy/rewriteme">test</... (200; 18.282996ms)
May 30 06:15:20.721: INFO: (19) /api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/proxy-service-b9cn6-9z7h2/proxy/rewriteme">test</a> (200; 18.281106ms)
May 30 06:15:20.721: INFO: (19) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname2/proxy/: bar (200; 19.134815ms)
May 30 06:15:20.732: INFO: (19) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname1/proxy/: tls baz (200; 29.75532ms)
May 30 06:15:20.733: INFO: (19) /api/v1/namespaces/proxy-698/services/https:proxy-service-b9cn6:tlsportname2/proxy/: tls qux (200; 30.323561ms)
May 30 06:15:20.733: INFO: (19) /api/v1/namespaces/proxy-698/services/http:proxy-service-b9cn6:portname1/proxy/: foo (200; 30.321681ms)
May 30 06:15:20.733: INFO: (19) /api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/: <a href="/api/v1/namespaces/proxy-698/pods/https:proxy-service-b9cn6-9z7h2:443/proxy/tlsrewriteme... (200; 30.378069ms)
May 30 06:15:20.753: INFO: (19) /api/v1/namespaces/proxy-698/services/proxy-service-b9cn6:portname1/proxy/: foo (200; 50.164262ms)
STEP: deleting ReplicationController proxy-service-b9cn6 in namespace proxy-698, will wait for the garbage collector to delete the pods
May 30 06:15:20.945: INFO: Deleting ReplicationController proxy-service-b9cn6 took: 136.242055ms
May 30 06:15:23.247: INFO: Terminating ReplicationController proxy-service-b9cn6 pods took: 2.301928065s
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:15:26.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-698" for this suite.

• [SLOW TEST:18.319 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":303,"completed":12,"skipped":181,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:15:26.257: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:15:26.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1479 version'
May 30 06:15:26.454: INFO: stderr: ""
May 30 06:15:26.454: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.16\", GitCommit:\"e37e4ab4cc8dcda84f1344dda47a97bb1927d074\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T16:25:59Z\", GoVersion:\"go1.15.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.16\", GitCommit:\"e37e4ab4cc8dcda84f1344dda47a97bb1927d074\", GitTreeState:\"clean\", BuildDate:\"2021-10-27T16:20:18Z\", GoVersion:\"go1.15.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:15:26.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1479" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":303,"completed":13,"skipped":199,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:15:26.466: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:15:30.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5292" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":303,"completed":14,"skipped":211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:15:30.544: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-zjwv
STEP: Creating a pod to test atomic-volume-subpath
May 30 06:15:30.615: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-zjwv" in namespace "subpath-9188" to be "Succeeded or Failed"
May 30 06:15:30.635: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Pending", Reason="", readiness=false. Elapsed: 17.656093ms
May 30 06:15:32.646: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028732888s
May 30 06:15:34.653: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 4.035254058s
May 30 06:15:36.657: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 6.039680005s
May 30 06:15:38.663: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 8.04546232s
May 30 06:15:40.750: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 10.132925164s
May 30 06:15:42.754: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 12.137113902s
May 30 06:15:44.760: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 14.142615994s
May 30 06:15:46.772: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 16.154921551s
May 30 06:15:48.779: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 18.16179423s
May 30 06:15:50.784: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 20.166455341s
May 30 06:15:52.789: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 22.171516766s
May 30 06:15:54.792: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Running", Reason="", readiness=true. Elapsed: 24.17454199s
May 30 06:15:56.800: INFO: Pod "pod-subpath-test-secret-zjwv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.183044615s
STEP: Saw pod success
May 30 06:15:56.801: INFO: Pod "pod-subpath-test-secret-zjwv" satisfied condition "Succeeded or Failed"
May 30 06:15:56.803: INFO: Trying to get logs from node my-node pod pod-subpath-test-secret-zjwv container test-container-subpath-secret-zjwv: <nil>
STEP: delete the pod
May 30 06:15:56.832: INFO: Waiting for pod pod-subpath-test-secret-zjwv to disappear
May 30 06:15:56.837: INFO: Pod pod-subpath-test-secret-zjwv no longer exists
STEP: Deleting pod pod-subpath-test-secret-zjwv
May 30 06:15:56.838: INFO: Deleting pod "pod-subpath-test-secret-zjwv" in namespace "subpath-9188"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:15:56.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9188" for this suite.

• [SLOW TEST:26.345 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":303,"completed":15,"skipped":235,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:15:56.889: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:15:57.446: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 06:15:59.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488157, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488157, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488157, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488157, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:16:02.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:16:02.585: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5201-crds.webhook.example.com via the AdmissionRegistration API
May 30 06:16:03.762: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:16:04.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-412" for this suite.
STEP: Destroying namespace "webhook-412-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.128 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":303,"completed":16,"skipped":236,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:16:05.017: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
May 30 06:16:05.064: INFO: Waiting up to 5m0s for pod "var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375" in namespace "var-expansion-8086" to be "Succeeded or Failed"
May 30 06:16:05.070: INFO: Pod "var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375": Phase="Pending", Reason="", readiness=false. Elapsed: 4.190763ms
May 30 06:16:07.074: INFO: Pod "var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008216489s
May 30 06:16:09.082: INFO: Pod "var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016366627s
May 30 06:16:11.087: INFO: Pod "var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021185213s
STEP: Saw pod success
May 30 06:16:11.087: INFO: Pod "var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375" satisfied condition "Succeeded or Failed"
May 30 06:16:11.090: INFO: Trying to get logs from node my-node pod var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375 container dapi-container: <nil>
STEP: delete the pod
May 30 06:16:11.132: INFO: Waiting for pod var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375 to disappear
May 30 06:16:11.138: INFO: Pod var-expansion-0e47202f-cea6-49a3-81bc-ab6cdd6f7375 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:16:11.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8086" for this suite.

• [SLOW TEST:6.142 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":303,"completed":17,"skipped":244,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:16:11.160: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:16:11.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680" in namespace "projected-793" to be "Succeeded or Failed"
May 30 06:16:11.246: INFO: Pod "downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680": Phase="Pending", Reason="", readiness=false. Elapsed: 11.403285ms
May 30 06:16:13.251: INFO: Pod "downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016031256s
May 30 06:16:15.267: INFO: Pod "downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031930328s
May 30 06:16:17.306: INFO: Pod "downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071158336s
STEP: Saw pod success
May 30 06:16:17.306: INFO: Pod "downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680" satisfied condition "Succeeded or Failed"
May 30 06:16:17.312: INFO: Trying to get logs from node my-node pod downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680 container client-container: <nil>
STEP: delete the pod
May 30 06:16:17.345: INFO: Waiting for pod downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680 to disappear
May 30 06:16:17.347: INFO: Pod downwardapi-volume-895c0b60-29c7-4bca-84ff-5e4f37c43680 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:16:17.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-793" for this suite.

• [SLOW TEST:6.204 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":303,"completed":18,"skipped":252,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:16:17.364: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-f1d239fe-e7e7-49c2-9623-424659c696cc
STEP: Creating a pod to test consume configMaps
May 30 06:16:17.424: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85" in namespace "projected-379" to be "Succeeded or Failed"
May 30 06:16:17.431: INFO: Pod "pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.575649ms
May 30 06:16:19.560: INFO: Pod "pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134278929s
May 30 06:16:21.564: INFO: Pod "pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.138467308s
STEP: Saw pod success
May 30 06:16:21.565: INFO: Pod "pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85" satisfied condition "Succeeded or Failed"
May 30 06:16:21.574: INFO: Trying to get logs from node my-node pod pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 30 06:16:21.629: INFO: Waiting for pod pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85 to disappear
May 30 06:16:21.637: INFO: Pod pod-projected-configmaps-c95c8a11-44e2-4c42-a693-f3b5573d3b85 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:16:21.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-379" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":19,"skipped":258,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:16:21.660: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7165.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7165.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7165.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7165.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7165.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7165.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 06:16:48.039: INFO: DNS probes using dns-7165/dns-test-4bb75ba8-9a83-4461-b5dc-715e1c9c4241 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:16:48.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7165" for this suite.

• [SLOW TEST:26.538 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":303,"completed":20,"skipped":260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:16:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:16:55.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9696" for this suite.

• [SLOW TEST:7.229 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":303,"completed":21,"skipped":282,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:16:55.428: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
May 30 06:16:55.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-235 api-versions'
May 30 06:16:55.640: INFO: stderr: ""
May 30 06:16:55.640: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:16:55.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-235" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":303,"completed":22,"skipped":287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:16:55.652: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
May 30 06:16:55.702: INFO: Waiting up to 5m0s for pod "client-containers-533d2501-c4df-4c76-b705-5d781c762e9a" in namespace "containers-3528" to be "Succeeded or Failed"
May 30 06:16:55.711: INFO: Pod "client-containers-533d2501-c4df-4c76-b705-5d781c762e9a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.954187ms
May 30 06:16:57.716: INFO: Pod "client-containers-533d2501-c4df-4c76-b705-5d781c762e9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014622983s
May 30 06:16:59.721: INFO: Pod "client-containers-533d2501-c4df-4c76-b705-5d781c762e9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018969455s
May 30 06:17:02.000: INFO: Pod "client-containers-533d2501-c4df-4c76-b705-5d781c762e9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.298012927s
STEP: Saw pod success
May 30 06:17:02.000: INFO: Pod "client-containers-533d2501-c4df-4c76-b705-5d781c762e9a" satisfied condition "Succeeded or Failed"
May 30 06:17:02.004: INFO: Trying to get logs from node my-node pod client-containers-533d2501-c4df-4c76-b705-5d781c762e9a container test-container: <nil>
STEP: delete the pod
May 30 06:17:02.456: INFO: Waiting for pod client-containers-533d2501-c4df-4c76-b705-5d781c762e9a to disappear
May 30 06:17:02.462: INFO: Pod client-containers-533d2501-c4df-4c76-b705-5d781c762e9a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:17:02.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3528" for this suite.

• [SLOW TEST:6.819 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":303,"completed":23,"skipped":316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:17:02.472: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:17:02.647: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8941
I0530 06:17:02.833771      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8941, replica count: 1
I0530 06:17:03.906059      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:17:04.906832      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:17:05.917151      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:17:06.917356      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:17:07.918945      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 06:17:08.045: INFO: Created: latency-svc-b85b2
May 30 06:17:08.063: INFO: Got endpoints: latency-svc-b85b2 [43.772927ms]
May 30 06:17:08.109: INFO: Created: latency-svc-5f72t
May 30 06:17:08.118: INFO: Got endpoints: latency-svc-5f72t [46.282824ms]
May 30 06:17:08.126: INFO: Created: latency-svc-nvnkz
May 30 06:17:08.149: INFO: Got endpoints: latency-svc-nvnkz [76.689505ms]
May 30 06:17:08.164: INFO: Created: latency-svc-hwqdh
May 30 06:17:08.168: INFO: Got endpoints: latency-svc-hwqdh [96.055169ms]
May 30 06:17:08.185: INFO: Created: latency-svc-vqqqf
May 30 06:17:08.208: INFO: Got endpoints: latency-svc-vqqqf [135.241661ms]
May 30 06:17:08.219: INFO: Created: latency-svc-6bpzp
May 30 06:17:08.228: INFO: Got endpoints: latency-svc-6bpzp [154.6032ms]
May 30 06:17:08.236: INFO: Created: latency-svc-p52ht
May 30 06:17:08.244: INFO: Got endpoints: latency-svc-p52ht [170.8658ms]
May 30 06:17:08.265: INFO: Created: latency-svc-cmg58
May 30 06:17:08.272: INFO: Got endpoints: latency-svc-cmg58 [198.398116ms]
May 30 06:17:08.299: INFO: Created: latency-svc-qhlpr
May 30 06:17:08.304: INFO: Got endpoints: latency-svc-qhlpr [231.227444ms]
May 30 06:17:08.340: INFO: Created: latency-svc-r5cx7
May 30 06:17:08.342: INFO: Got endpoints: latency-svc-r5cx7 [268.631634ms]
May 30 06:17:08.362: INFO: Created: latency-svc-glp9m
May 30 06:17:08.368: INFO: Got endpoints: latency-svc-glp9m [295.024227ms]
May 30 06:17:08.395: INFO: Created: latency-svc-nj2n7
May 30 06:17:08.403: INFO: Got endpoints: latency-svc-nj2n7 [330.51297ms]
May 30 06:17:08.421: INFO: Created: latency-svc-pj92f
May 30 06:17:08.432: INFO: Got endpoints: latency-svc-pj92f [358.67839ms]
May 30 06:17:08.522: INFO: Created: latency-svc-jz568
May 30 06:17:08.534: INFO: Got endpoints: latency-svc-jz568 [460.934614ms]
May 30 06:17:08.548: INFO: Created: latency-svc-p5xpr
May 30 06:17:08.554: INFO: Got endpoints: latency-svc-p5xpr [480.922481ms]
May 30 06:17:08.593: INFO: Created: latency-svc-rxxxm
May 30 06:17:08.600: INFO: Got endpoints: latency-svc-rxxxm [527.126399ms]
May 30 06:17:08.612: INFO: Created: latency-svc-6gngf
May 30 06:17:08.676: INFO: Got endpoints: latency-svc-6gngf [558.300889ms]
May 30 06:17:08.688: INFO: Created: latency-svc-m4szb
May 30 06:17:08.692: INFO: Got endpoints: latency-svc-m4szb [542.756148ms]
May 30 06:17:08.713: INFO: Created: latency-svc-l7zzx
May 30 06:17:08.720: INFO: Got endpoints: latency-svc-l7zzx [551.444357ms]
May 30 06:17:08.732: INFO: Created: latency-svc-6gbrf
May 30 06:17:08.744: INFO: Got endpoints: latency-svc-6gbrf [534.625563ms]
May 30 06:17:08.760: INFO: Created: latency-svc-5lfvz
May 30 06:17:08.765: INFO: Got endpoints: latency-svc-5lfvz [536.902512ms]
May 30 06:17:08.836: INFO: Created: latency-svc-8gcn4
May 30 06:17:08.843: INFO: Got endpoints: latency-svc-8gcn4 [598.85231ms]
May 30 06:17:08.859: INFO: Created: latency-svc-bcr7m
May 30 06:17:08.861: INFO: Got endpoints: latency-svc-bcr7m [588.918253ms]
May 30 06:17:08.922: INFO: Created: latency-svc-sxgtv
May 30 06:17:09.007: INFO: Got endpoints: latency-svc-sxgtv [703.121354ms]
May 30 06:17:09.020: INFO: Created: latency-svc-hsxzw
May 30 06:17:09.030: INFO: Got endpoints: latency-svc-hsxzw [688.459909ms]
May 30 06:17:09.037: INFO: Created: latency-svc-xcbjp
May 30 06:17:09.053: INFO: Got endpoints: latency-svc-xcbjp [684.277921ms]
May 30 06:17:09.096: INFO: Created: latency-svc-dc2vz
May 30 06:17:09.096: INFO: Got endpoints: latency-svc-dc2vz [692.789311ms]
May 30 06:17:09.126: INFO: Created: latency-svc-shhx4
May 30 06:17:09.136: INFO: Got endpoints: latency-svc-shhx4 [703.52162ms]
May 30 06:17:09.148: INFO: Created: latency-svc-k5bhf
May 30 06:17:09.171: INFO: Got endpoints: latency-svc-k5bhf [636.187184ms]
May 30 06:17:09.181: INFO: Created: latency-svc-fxrhg
May 30 06:17:09.191: INFO: Got endpoints: latency-svc-fxrhg [636.120069ms]
May 30 06:17:09.201: INFO: Created: latency-svc-hzp57
May 30 06:17:09.285: INFO: Created: latency-svc-zvz5f
May 30 06:17:09.291: INFO: Got endpoints: latency-svc-hzp57 [690.141888ms]
May 30 06:17:09.297: INFO: Got endpoints: latency-svc-zvz5f [620.00891ms]
May 30 06:17:09.313: INFO: Created: latency-svc-5fqm2
May 30 06:17:09.322: INFO: Got endpoints: latency-svc-5fqm2 [629.727156ms]
May 30 06:17:09.337: INFO: Created: latency-svc-qtwvd
May 30 06:17:09.347: INFO: Got endpoints: latency-svc-qtwvd [626.643162ms]
May 30 06:17:09.359: INFO: Created: latency-svc-95nf9
May 30 06:17:09.393: INFO: Got endpoints: latency-svc-95nf9 [648.788024ms]
May 30 06:17:09.401: INFO: Created: latency-svc-l2bqr
May 30 06:17:09.424: INFO: Got endpoints: latency-svc-l2bqr [658.828571ms]
May 30 06:17:09.430: INFO: Created: latency-svc-qn6nm
May 30 06:17:09.440: INFO: Got endpoints: latency-svc-qn6nm [597.073754ms]
May 30 06:17:09.456: INFO: Created: latency-svc-z5879
May 30 06:17:09.458: INFO: Got endpoints: latency-svc-z5879 [596.957669ms]
May 30 06:17:09.471: INFO: Created: latency-svc-bhlcw
May 30 06:17:09.480: INFO: Got endpoints: latency-svc-bhlcw [472.445792ms]
May 30 06:17:09.580: INFO: Created: latency-svc-vtd9g
May 30 06:17:09.592: INFO: Got endpoints: latency-svc-vtd9g [560.757721ms]
May 30 06:17:09.600: INFO: Created: latency-svc-nkz2x
May 30 06:17:09.611: INFO: Got endpoints: latency-svc-nkz2x [558.459586ms]
May 30 06:17:09.631: INFO: Created: latency-svc-gg95n
May 30 06:17:09.631: INFO: Got endpoints: latency-svc-gg95n [535.017643ms]
May 30 06:17:09.649: INFO: Created: latency-svc-pdw54
May 30 06:17:09.659: INFO: Got endpoints: latency-svc-pdw54 [523.031057ms]
May 30 06:17:09.672: INFO: Created: latency-svc-lb2rq
May 30 06:17:09.715: INFO: Got endpoints: latency-svc-lb2rq [543.93696ms]
May 30 06:17:09.723: INFO: Created: latency-svc-ll9jk
May 30 06:17:09.735: INFO: Got endpoints: latency-svc-ll9jk [544.265313ms]
May 30 06:17:09.755: INFO: Created: latency-svc-865r2
May 30 06:17:09.771: INFO: Got endpoints: latency-svc-865r2 [480.134653ms]
May 30 06:17:09.785: INFO: Created: latency-svc-57c2g
May 30 06:17:09.791: INFO: Got endpoints: latency-svc-57c2g [492.062906ms]
May 30 06:17:09.803: INFO: Created: latency-svc-b4524
May 30 06:17:09.812: INFO: Got endpoints: latency-svc-b4524 [490.165783ms]
May 30 06:17:09.878: INFO: Created: latency-svc-5wl2j
May 30 06:17:09.892: INFO: Got endpoints: latency-svc-5wl2j [545.107057ms]
May 30 06:17:09.905: INFO: Created: latency-svc-mjn85
May 30 06:17:09.923: INFO: Got endpoints: latency-svc-mjn85 [529.971279ms]
May 30 06:17:09.935: INFO: Created: latency-svc-4lcls
May 30 06:17:09.943: INFO: Got endpoints: latency-svc-4lcls [519.037193ms]
May 30 06:17:09.963: INFO: Created: latency-svc-bxxc4
May 30 06:17:09.967: INFO: Got endpoints: latency-svc-bxxc4 [526.205575ms]
May 30 06:17:09.990: INFO: Created: latency-svc-r8xnp
May 30 06:17:10.009: INFO: Got endpoints: latency-svc-r8xnp [550.946631ms]
May 30 06:17:10.017: INFO: Created: latency-svc-vsrrz
May 30 06:17:10.027: INFO: Got endpoints: latency-svc-vsrrz [546.583173ms]
May 30 06:17:10.044: INFO: Created: latency-svc-lb5kf
May 30 06:17:10.057: INFO: Got endpoints: latency-svc-lb5kf [464.752365ms]
May 30 06:17:10.065: INFO: Created: latency-svc-w9nbt
May 30 06:17:10.079: INFO: Got endpoints: latency-svc-w9nbt [468.238113ms]
May 30 06:17:10.092: INFO: Created: latency-svc-twz2t
May 30 06:17:10.112: INFO: Created: latency-svc-95qqs
May 30 06:17:10.112: INFO: Got endpoints: latency-svc-twz2t [480.780422ms]
May 30 06:17:10.202: INFO: Got endpoints: latency-svc-95qqs [542.711145ms]
May 30 06:17:10.212: INFO: Created: latency-svc-fv5hn
May 30 06:17:10.219: INFO: Got endpoints: latency-svc-fv5hn [503.108239ms]
May 30 06:17:10.237: INFO: Created: latency-svc-8c88g
May 30 06:17:10.241: INFO: Got endpoints: latency-svc-8c88g [506.053823ms]
May 30 06:17:10.257: INFO: Created: latency-svc-8gbpt
May 30 06:17:10.357: INFO: Got endpoints: latency-svc-8gbpt [585.749179ms]
May 30 06:17:10.364: INFO: Created: latency-svc-b76lj
May 30 06:17:10.451: INFO: Got endpoints: latency-svc-b76lj [659.700872ms]
May 30 06:17:10.521: INFO: Created: latency-svc-rd84f
May 30 06:17:10.538: INFO: Got endpoints: latency-svc-rd84f [726.145907ms]
May 30 06:17:10.557: INFO: Created: latency-svc-tfc8x
May 30 06:17:10.565: INFO: Got endpoints: latency-svc-tfc8x [672.956881ms]
May 30 06:17:10.592: INFO: Created: latency-svc-9czsm
May 30 06:17:10.593: INFO: Got endpoints: latency-svc-9czsm [669.869627ms]
May 30 06:17:10.603: INFO: Created: latency-svc-nztn2
May 30 06:17:10.611: INFO: Got endpoints: latency-svc-nztn2 [668.003412ms]
May 30 06:17:10.621: INFO: Created: latency-svc-5kft2
May 30 06:17:10.637: INFO: Got endpoints: latency-svc-5kft2 [669.664144ms]
May 30 06:17:10.647: INFO: Created: latency-svc-jks52
May 30 06:17:10.651: INFO: Got endpoints: latency-svc-jks52 [642.672107ms]
May 30 06:17:10.714: INFO: Created: latency-svc-n8lf4
May 30 06:17:10.721: INFO: Got endpoints: latency-svc-n8lf4 [694.346787ms]
May 30 06:17:10.741: INFO: Created: latency-svc-fs8th
May 30 06:17:10.841: INFO: Got endpoints: latency-svc-fs8th [783.984705ms]
May 30 06:17:10.849: INFO: Created: latency-svc-7fgpk
May 30 06:17:10.861: INFO: Got endpoints: latency-svc-7fgpk [781.941885ms]
May 30 06:17:10.869: INFO: Created: latency-svc-m4xt2
May 30 06:17:10.891: INFO: Created: latency-svc-9hjt7
May 30 06:17:10.895: INFO: Got endpoints: latency-svc-m4xt2 [782.920497ms]
May 30 06:17:10.899: INFO: Got endpoints: latency-svc-9hjt7 [696.952691ms]
May 30 06:17:10.913: INFO: Created: latency-svc-69h44
May 30 06:17:10.926: INFO: Got endpoints: latency-svc-69h44 [705.848239ms]
May 30 06:17:10.966: INFO: Created: latency-svc-tnnd9
May 30 06:17:10.982: INFO: Got endpoints: latency-svc-tnnd9 [740.070349ms]
May 30 06:17:10.995: INFO: Created: latency-svc-jn6s5
May 30 06:17:11.001: INFO: Got endpoints: latency-svc-jn6s5 [641.635583ms]
May 30 06:17:11.016: INFO: Created: latency-svc-nz4z2
May 30 06:17:11.035: INFO: Created: latency-svc-9tpp9
May 30 06:17:11.038: INFO: Got endpoints: latency-svc-nz4z2 [585.210808ms]
May 30 06:17:11.046: INFO: Got endpoints: latency-svc-9tpp9 [507.998984ms]
May 30 06:17:11.052: INFO: Created: latency-svc-njm42
May 30 06:17:11.180: INFO: Created: latency-svc-tzmp4
May 30 06:17:11.182: INFO: Got endpoints: latency-svc-njm42 [615.105209ms]
May 30 06:17:11.187: INFO: Got endpoints: latency-svc-tzmp4 [593.582831ms]
May 30 06:17:11.204: INFO: Created: latency-svc-fxfhj
May 30 06:17:11.214: INFO: Got endpoints: latency-svc-fxfhj [602.276554ms]
May 30 06:17:11.230: INFO: Created: latency-svc-zv9fb
May 30 06:17:11.237: INFO: Got endpoints: latency-svc-zv9fb [599.425391ms]
May 30 06:17:11.249: INFO: Created: latency-svc-6ws5g
May 30 06:17:11.254: INFO: Got endpoints: latency-svc-6ws5g [602.377379ms]
May 30 06:17:11.265: INFO: Created: latency-svc-mbwk2
May 30 06:17:11.317: INFO: Got endpoints: latency-svc-mbwk2 [595.145444ms]
May 30 06:17:11.330: INFO: Created: latency-svc-jnch9
May 30 06:17:11.343: INFO: Got endpoints: latency-svc-jnch9 [501.759652ms]
May 30 06:17:11.351: INFO: Created: latency-svc-n5xz9
May 30 06:17:11.494: INFO: Created: latency-svc-5f6lq
May 30 06:17:11.495: INFO: Got endpoints: latency-svc-n5xz9 [633.347595ms]
May 30 06:17:11.504: INFO: Got endpoints: latency-svc-5f6lq [608.525193ms]
May 30 06:17:11.521: INFO: Created: latency-svc-qxv59
May 30 06:17:11.534: INFO: Created: latency-svc-tn5qm
May 30 06:17:11.537: INFO: Got endpoints: latency-svc-qxv59 [638.354248ms]
May 30 06:17:11.542: INFO: Got endpoints: latency-svc-tn5qm [616.299874ms]
May 30 06:17:11.560: INFO: Created: latency-svc-w565t
May 30 06:17:11.568: INFO: Got endpoints: latency-svc-w565t [582.590208ms]
May 30 06:17:11.584: INFO: Created: latency-svc-xwpc7
May 30 06:17:11.648: INFO: Got endpoints: latency-svc-xwpc7 [645.608259ms]
May 30 06:17:11.649: INFO: Created: latency-svc-vqsts
May 30 06:17:11.659: INFO: Got endpoints: latency-svc-vqsts [621.057146ms]
May 30 06:17:11.676: INFO: Created: latency-svc-p8smd
May 30 06:17:11.677: INFO: Got endpoints: latency-svc-p8smd [630.636437ms]
May 30 06:17:11.694: INFO: Created: latency-svc-6b9zf
May 30 06:17:11.702: INFO: Got endpoints: latency-svc-6b9zf [519.246355ms]
May 30 06:17:11.828: INFO: Created: latency-svc-fxz8p
May 30 06:17:11.843: INFO: Got endpoints: latency-svc-fxz8p [655.442659ms]
May 30 06:17:11.849: INFO: Created: latency-svc-9sv5b
May 30 06:17:11.863: INFO: Got endpoints: latency-svc-9sv5b [648.676571ms]
May 30 06:17:11.868: INFO: Created: latency-svc-crbjk
May 30 06:17:11.873: INFO: Got endpoints: latency-svc-crbjk [636.040014ms]
May 30 06:17:11.893: INFO: Created: latency-svc-d9sh4
May 30 06:17:11.898: INFO: Got endpoints: latency-svc-d9sh4 [643.703953ms]
May 30 06:17:11.911: INFO: Created: latency-svc-9zcj2
May 30 06:17:12.005: INFO: Got endpoints: latency-svc-9zcj2 [688.748163ms]
May 30 06:17:12.023: INFO: Created: latency-svc-69574
May 30 06:17:12.023: INFO: Got endpoints: latency-svc-69574 [678.714699ms]
May 30 06:17:12.092: INFO: Created: latency-svc-2b7wf
May 30 06:17:12.095: INFO: Got endpoints: latency-svc-2b7wf [599.862534ms]
May 30 06:17:12.191: INFO: Created: latency-svc-lgsxn
May 30 06:17:12.200: INFO: Got endpoints: latency-svc-lgsxn [695.427438ms]
May 30 06:17:12.363: INFO: Created: latency-svc-8bbqd
May 30 06:17:12.368: INFO: Got endpoints: latency-svc-8bbqd [830.470832ms]
May 30 06:17:12.384: INFO: Created: latency-svc-vjjr5
May 30 06:17:12.392: INFO: Got endpoints: latency-svc-vjjr5 [849.620981ms]
May 30 06:17:12.402: INFO: Created: latency-svc-8xq6j
May 30 06:17:12.407: INFO: Got endpoints: latency-svc-8xq6j [838.43416ms]
May 30 06:17:12.524: INFO: Created: latency-svc-hjrhw
May 30 06:17:12.537: INFO: Created: latency-svc-7nzpr
May 30 06:17:12.542: INFO: Got endpoints: latency-svc-7nzpr [882.564996ms]
May 30 06:17:12.542: INFO: Got endpoints: latency-svc-hjrhw [893.528188ms]
May 30 06:17:12.559: INFO: Created: latency-svc-sp7x9
May 30 06:17:12.568: INFO: Got endpoints: latency-svc-sp7x9 [891.012961ms]
May 30 06:17:12.581: INFO: Created: latency-svc-xpbrf
May 30 06:17:12.588: INFO: Got endpoints: latency-svc-xpbrf [884.919073ms]
May 30 06:17:12.614: INFO: Created: latency-svc-zmgcm
May 30 06:17:12.743: INFO: Got endpoints: latency-svc-zmgcm [894.04417ms]
May 30 06:17:12.745: INFO: Created: latency-svc-86zjx
May 30 06:17:12.755: INFO: Got endpoints: latency-svc-86zjx [889.274373ms]
May 30 06:17:12.762: INFO: Created: latency-svc-59jj9
May 30 06:17:12.770: INFO: Got endpoints: latency-svc-59jj9 [896.834107ms]
May 30 06:17:12.791: INFO: Created: latency-svc-nmn5k
May 30 06:17:12.793: INFO: Got endpoints: latency-svc-nmn5k [895.276191ms]
May 30 06:17:12.807: INFO: Created: latency-svc-mbdpx
May 30 06:17:12.821: INFO: Created: latency-svc-vtdlt
May 30 06:17:12.827: INFO: Got endpoints: latency-svc-mbdpx [821.032696ms]
May 30 06:17:13.021: INFO: Got endpoints: latency-svc-vtdlt [997.878277ms]
May 30 06:17:13.031: INFO: Created: latency-svc-l9k84
May 30 06:17:13.052: INFO: Created: latency-svc-dg4tf
May 30 06:17:13.052: INFO: Got endpoints: latency-svc-l9k84 [956.004659ms]
May 30 06:17:13.058: INFO: Got endpoints: latency-svc-dg4tf [856.773271ms]
May 30 06:17:13.088: INFO: Created: latency-svc-rwtjj
May 30 06:17:13.100: INFO: Got endpoints: latency-svc-rwtjj [731.434031ms]
May 30 06:17:13.113: INFO: Created: latency-svc-fvt2n
May 30 06:17:13.150: INFO: Got endpoints: latency-svc-fvt2n [757.571973ms]
May 30 06:17:13.154: INFO: Created: latency-svc-pbtrb
May 30 06:17:13.165: INFO: Got endpoints: latency-svc-pbtrb [757.228919ms]
May 30 06:17:13.172: INFO: Created: latency-svc-7f7jq
May 30 06:17:13.178: INFO: Got endpoints: latency-svc-7f7jq [635.672629ms]
May 30 06:17:13.192: INFO: Created: latency-svc-fgc6q
May 30 06:17:13.200: INFO: Got endpoints: latency-svc-fgc6q [658.066395ms]
May 30 06:17:13.284: INFO: Created: latency-svc-rf2j9
May 30 06:17:13.285: INFO: Got endpoints: latency-svc-rf2j9 [716.194048ms]
May 30 06:17:13.298: INFO: Created: latency-svc-2r8lp
May 30 06:17:13.309: INFO: Got endpoints: latency-svc-2r8lp [718.459376ms]
May 30 06:17:13.318: INFO: Created: latency-svc-pq5pc
May 30 06:17:13.323: INFO: Got endpoints: latency-svc-pq5pc [579.448356ms]
May 30 06:17:13.337: INFO: Created: latency-svc-jpctx
May 30 06:17:13.347: INFO: Got endpoints: latency-svc-jpctx [591.288633ms]
May 30 06:17:13.501: INFO: Created: latency-svc-z9wnd
May 30 06:17:13.503: INFO: Got endpoints: latency-svc-z9wnd [732.869766ms]
May 30 06:17:13.522: INFO: Created: latency-svc-n7l26
May 30 06:17:13.527: INFO: Got endpoints: latency-svc-n7l26 [733.187145ms]
May 30 06:17:13.546: INFO: Created: latency-svc-64z5s
May 30 06:17:13.553: INFO: Got endpoints: latency-svc-64z5s [723.502416ms]
May 30 06:17:13.562: INFO: Created: latency-svc-2kpvq
May 30 06:17:13.571: INFO: Got endpoints: latency-svc-2kpvq [549.371992ms]
May 30 06:17:13.624: INFO: Created: latency-svc-krp4n
May 30 06:17:13.658: INFO: Got endpoints: latency-svc-krp4n [606.219391ms]
May 30 06:17:13.667: INFO: Created: latency-svc-mw4jb
May 30 06:17:13.698: INFO: Got endpoints: latency-svc-mw4jb [639.724909ms]
May 30 06:17:13.703: INFO: Created: latency-svc-jpn8x
May 30 06:17:13.710: INFO: Got endpoints: latency-svc-jpn8x [609.615469ms]
May 30 06:17:13.721: INFO: Created: latency-svc-ll99m
May 30 06:17:13.729: INFO: Got endpoints: latency-svc-ll99m [579.127053ms]
May 30 06:17:13.825: INFO: Created: latency-svc-n4j9h
May 30 06:17:13.831: INFO: Got endpoints: latency-svc-n4j9h [665.60481ms]
May 30 06:17:13.845: INFO: Created: latency-svc-986cs
May 30 06:17:13.853: INFO: Got endpoints: latency-svc-986cs [675.093222ms]
May 30 06:17:13.882: INFO: Created: latency-svc-gw5rf
May 30 06:17:13.886: INFO: Got endpoints: latency-svc-gw5rf [685.282432ms]
May 30 06:17:13.900: INFO: Created: latency-svc-mvvr5
May 30 06:17:13.905: INFO: Got endpoints: latency-svc-mvvr5 [620.068905ms]
May 30 06:17:13.921: INFO: Created: latency-svc-xwdf7
May 30 06:17:13.922: INFO: Got endpoints: latency-svc-xwdf7 [612.26851ms]
May 30 06:17:13.993: INFO: Created: latency-svc-6wbtt
May 30 06:17:14.053: INFO: Got endpoints: latency-svc-6wbtt [730.150873ms]
May 30 06:17:14.072: INFO: Created: latency-svc-hf597
May 30 06:17:14.080: INFO: Got endpoints: latency-svc-hf597 [733.0224ms]
May 30 06:17:14.195: INFO: Created: latency-svc-kvn2n
May 30 06:17:14.195: INFO: Got endpoints: latency-svc-kvn2n [692.102765ms]
May 30 06:17:14.212: INFO: Created: latency-svc-stlg5
May 30 06:17:14.216: INFO: Got endpoints: latency-svc-stlg5 [136.275659ms]
May 30 06:17:14.233: INFO: Created: latency-svc-v2hnj
May 30 06:17:14.238: INFO: Got endpoints: latency-svc-v2hnj [711.134459ms]
May 30 06:17:14.251: INFO: Created: latency-svc-8rj7m
May 30 06:17:14.256: INFO: Got endpoints: latency-svc-8rj7m [702.676017ms]
May 30 06:17:14.270: INFO: Created: latency-svc-mbprn
May 30 06:17:14.444: INFO: Got endpoints: latency-svc-mbprn [872.88253ms]
May 30 06:17:14.453: INFO: Created: latency-svc-pb2mw
May 30 06:17:14.460: INFO: Got endpoints: latency-svc-pb2mw [802.100696ms]
May 30 06:17:14.466: INFO: Created: latency-svc-chwvx
May 30 06:17:14.469: INFO: Got endpoints: latency-svc-chwvx [768.273603ms]
May 30 06:17:14.481: INFO: Created: latency-svc-g9mdp
May 30 06:17:14.547: INFO: Created: latency-svc-bccp7
May 30 06:17:14.552: INFO: Got endpoints: latency-svc-g9mdp [842.788874ms]
May 30 06:17:14.707: INFO: Got endpoints: latency-svc-bccp7 [978.234507ms]
May 30 06:17:14.716: INFO: Created: latency-svc-9hlmm
May 30 06:17:14.731: INFO: Got endpoints: latency-svc-9hlmm [899.897096ms]
May 30 06:17:14.755: INFO: Created: latency-svc-mrdrq
May 30 06:17:14.760: INFO: Got endpoints: latency-svc-mrdrq [906.641862ms]
May 30 06:17:14.783: INFO: Created: latency-svc-2gmrm
May 30 06:17:14.789: INFO: Got endpoints: latency-svc-2gmrm [902.496037ms]
May 30 06:17:14.797: INFO: Created: latency-svc-68g7c
May 30 06:17:14.807: INFO: Got endpoints: latency-svc-68g7c [901.439286ms]
May 30 06:17:14.886: INFO: Created: latency-svc-pl2z5
May 30 06:17:14.895: INFO: Got endpoints: latency-svc-pl2z5 [972.307237ms]
May 30 06:17:14.909: INFO: Created: latency-svc-jslv5
May 30 06:17:14.914: INFO: Got endpoints: latency-svc-jslv5 [860.73892ms]
May 30 06:17:14.932: INFO: Created: latency-svc-mk5k9
May 30 06:17:14.940: INFO: Got endpoints: latency-svc-mk5k9 [745.315444ms]
May 30 06:17:15.124: INFO: Created: latency-svc-lpclx
May 30 06:17:15.152: INFO: Created: latency-svc-fl5dk
May 30 06:17:15.153: INFO: Got endpoints: latency-svc-lpclx [937.056343ms]
May 30 06:17:15.163: INFO: Got endpoints: latency-svc-fl5dk [925.09424ms]
May 30 06:17:15.182: INFO: Created: latency-svc-4g25w
May 30 06:17:15.198: INFO: Got endpoints: latency-svc-4g25w [941.857531ms]
May 30 06:17:15.214: INFO: Created: latency-svc-x2ll9
May 30 06:17:15.229: INFO: Created: latency-svc-stb76
May 30 06:17:15.231: INFO: Got endpoints: latency-svc-x2ll9 [786.855839ms]
May 30 06:17:15.289: INFO: Created: latency-svc-dnb5q
May 30 06:17:15.289: INFO: Got endpoints: latency-svc-stb76 [829.21465ms]
May 30 06:17:15.296: INFO: Got endpoints: latency-svc-dnb5q [826.656708ms]
May 30 06:17:15.296: INFO: Created: latency-svc-zx9zg
May 30 06:17:15.336: INFO: Got endpoints: latency-svc-zx9zg [783.010728ms]
May 30 06:17:15.346: INFO: Created: latency-svc-sf5s5
May 30 06:17:15.362: INFO: Got endpoints: latency-svc-sf5s5 [654.344835ms]
May 30 06:17:15.529: INFO: Created: latency-svc-lskxv
May 30 06:17:15.543: INFO: Got endpoints: latency-svc-lskxv [812.189535ms]
May 30 06:17:15.569: INFO: Created: latency-svc-6bmbg
May 30 06:17:15.569: INFO: Got endpoints: latency-svc-6bmbg [808.858883ms]
May 30 06:17:15.582: INFO: Created: latency-svc-5gtlh
May 30 06:17:15.597: INFO: Got endpoints: latency-svc-5gtlh [808.702268ms]
May 30 06:17:15.609: INFO: Created: latency-svc-qmw4q
May 30 06:17:15.614: INFO: Got endpoints: latency-svc-qmw4q [806.702282ms]
May 30 06:17:15.684: INFO: Created: latency-svc-nwz79
May 30 06:17:15.697: INFO: Got endpoints: latency-svc-nwz79 [801.069519ms]
May 30 06:17:15.722: INFO: Created: latency-svc-59sgj
May 30 06:17:15.727: INFO: Got endpoints: latency-svc-59sgj [813.286183ms]
May 30 06:17:15.751: INFO: Created: latency-svc-wh5n4
May 30 06:17:15.753: INFO: Got endpoints: latency-svc-wh5n4 [812.517561ms]
May 30 06:17:15.768: INFO: Created: latency-svc-5557s
May 30 06:17:15.770: INFO: Got endpoints: latency-svc-5557s [616.427265ms]
May 30 06:17:16.009: INFO: Created: latency-svc-cvq4s
May 30 06:17:16.018: INFO: Got endpoints: latency-svc-cvq4s [853.98877ms]
May 30 06:17:16.030: INFO: Created: latency-svc-bpnbz
May 30 06:17:16.037: INFO: Got endpoints: latency-svc-bpnbz [839.034794ms]
May 30 06:17:16.051: INFO: Created: latency-svc-hln6l
May 30 06:17:16.059: INFO: Got endpoints: latency-svc-hln6l [826.613421ms]
May 30 06:17:16.088: INFO: Created: latency-svc-pcqt2
May 30 06:17:16.268: INFO: Got endpoints: latency-svc-pcqt2 [978.575437ms]
May 30 06:17:16.277: INFO: Created: latency-svc-6krxd
May 30 06:17:16.281: INFO: Got endpoints: latency-svc-6krxd [985.144153ms]
May 30 06:17:16.402: INFO: Created: latency-svc-mlff4
May 30 06:17:16.442: INFO: Got endpoints: latency-svc-mlff4 [1.106345682s]
May 30 06:17:16.467: INFO: Created: latency-svc-gt2h9
May 30 06:17:16.472: INFO: Got endpoints: latency-svc-gt2h9 [1.109804592s]
May 30 06:17:16.497: INFO: Created: latency-svc-rvxk2
May 30 06:17:16.739: INFO: Got endpoints: latency-svc-rvxk2 [1.19641685s]
May 30 06:17:16.757: INFO: Created: latency-svc-d4nhm
May 30 06:17:16.778: INFO: Got endpoints: latency-svc-d4nhm [1.209292636s]
May 30 06:17:16.807: INFO: Created: latency-svc-7792k
May 30 06:17:16.943: INFO: Got endpoints: latency-svc-7792k [1.345383351s]
May 30 06:17:16.976: INFO: Created: latency-svc-5qk62
May 30 06:17:16.982: INFO: Got endpoints: latency-svc-5qk62 [1.368970341s]
May 30 06:17:17.005: INFO: Created: latency-svc-m5r6h
May 30 06:17:17.009: INFO: Got endpoints: latency-svc-m5r6h [1.312237322s]
May 30 06:17:17.046: INFO: Created: latency-svc-csfl2
May 30 06:17:17.118: INFO: Got endpoints: latency-svc-csfl2 [1.390566508s]
May 30 06:17:17.133: INFO: Created: latency-svc-4v76n
May 30 06:17:17.148: INFO: Got endpoints: latency-svc-4v76n [1.394677663s]
May 30 06:17:17.177: INFO: Created: latency-svc-bghrl
May 30 06:17:17.207: INFO: Got endpoints: latency-svc-bghrl [1.437431681s]
May 30 06:17:17.351: INFO: Created: latency-svc-vb8n2
May 30 06:17:17.373: INFO: Got endpoints: latency-svc-vb8n2 [1.355325056s]
May 30 06:17:17.532: INFO: Created: latency-svc-mmp2p
May 30 06:17:17.532: INFO: Got endpoints: latency-svc-mmp2p [1.494840915s]
May 30 06:17:17.550: INFO: Created: latency-svc-q7tlr
May 30 06:17:17.550: INFO: Got endpoints: latency-svc-q7tlr [1.491462288s]
May 30 06:17:17.615: INFO: Created: latency-svc-mdcl4
May 30 06:17:17.641: INFO: Got endpoints: latency-svc-mdcl4 [1.373067001s]
May 30 06:17:17.647: INFO: Created: latency-svc-dprcb
May 30 06:17:17.658: INFO: Got endpoints: latency-svc-dprcb [1.376373053s]
May 30 06:17:17.672: INFO: Created: latency-svc-b6x9z
May 30 06:17:17.686: INFO: Got endpoints: latency-svc-b6x9z [1.243949249s]
May 30 06:17:17.715: INFO: Created: latency-svc-l4xhw
May 30 06:17:17.715: INFO: Got endpoints: latency-svc-l4xhw [1.243499927s]
May 30 06:17:17.736: INFO: Created: latency-svc-mmzfc
May 30 06:17:17.838: INFO: Got endpoints: latency-svc-mmzfc [1.098289485s]
May 30 06:17:17.851: INFO: Created: latency-svc-4n5h9
May 30 06:17:17.859: INFO: Got endpoints: latency-svc-4n5h9 [1.080861636s]
May 30 06:17:17.882: INFO: Created: latency-svc-j2ctv
May 30 06:17:17.890: INFO: Got endpoints: latency-svc-j2ctv [946.254004ms]
May 30 06:17:17.899: INFO: Created: latency-svc-lg5kz
May 30 06:17:18.026: INFO: Got endpoints: latency-svc-lg5kz [1.041993162s]
May 30 06:17:18.036: INFO: Created: latency-svc-bxjsp
May 30 06:17:18.047: INFO: Got endpoints: latency-svc-bxjsp [1.037855143s]
May 30 06:17:18.057: INFO: Created: latency-svc-fvzhk
May 30 06:17:18.070: INFO: Got endpoints: latency-svc-fvzhk [951.648615ms]
May 30 06:17:18.070: INFO: Latencies: [46.282824ms 76.689505ms 96.055169ms 135.241661ms 136.275659ms 154.6032ms 170.8658ms 198.398116ms 231.227444ms 268.631634ms 295.024227ms 330.51297ms 358.67839ms 460.934614ms 464.752365ms 468.238113ms 472.445792ms 480.134653ms 480.780422ms 480.922481ms 490.165783ms 492.062906ms 501.759652ms 503.108239ms 506.053823ms 507.998984ms 519.037193ms 519.246355ms 523.031057ms 526.205575ms 527.126399ms 529.971279ms 534.625563ms 535.017643ms 536.902512ms 542.711145ms 542.756148ms 543.93696ms 544.265313ms 545.107057ms 546.583173ms 549.371992ms 550.946631ms 551.444357ms 558.300889ms 558.459586ms 560.757721ms 579.127053ms 579.448356ms 582.590208ms 585.210808ms 585.749179ms 588.918253ms 591.288633ms 593.582831ms 595.145444ms 596.957669ms 597.073754ms 598.85231ms 599.425391ms 599.862534ms 602.276554ms 602.377379ms 606.219391ms 608.525193ms 609.615469ms 612.26851ms 615.105209ms 616.299874ms 616.427265ms 620.00891ms 620.068905ms 621.057146ms 626.643162ms 629.727156ms 630.636437ms 633.347595ms 635.672629ms 636.040014ms 636.120069ms 636.187184ms 638.354248ms 639.724909ms 641.635583ms 642.672107ms 643.703953ms 645.608259ms 648.676571ms 648.788024ms 654.344835ms 655.442659ms 658.066395ms 658.828571ms 659.700872ms 665.60481ms 668.003412ms 669.664144ms 669.869627ms 672.956881ms 675.093222ms 678.714699ms 684.277921ms 685.282432ms 688.459909ms 688.748163ms 690.141888ms 692.102765ms 692.789311ms 694.346787ms 695.427438ms 696.952691ms 702.676017ms 703.121354ms 703.52162ms 705.848239ms 711.134459ms 716.194048ms 718.459376ms 723.502416ms 726.145907ms 730.150873ms 731.434031ms 732.869766ms 733.0224ms 733.187145ms 740.070349ms 745.315444ms 757.228919ms 757.571973ms 768.273603ms 781.941885ms 782.920497ms 783.010728ms 783.984705ms 786.855839ms 801.069519ms 802.100696ms 806.702282ms 808.702268ms 808.858883ms 812.189535ms 812.517561ms 813.286183ms 821.032696ms 826.613421ms 826.656708ms 829.21465ms 830.470832ms 838.43416ms 839.034794ms 842.788874ms 849.620981ms 853.98877ms 856.773271ms 860.73892ms 872.88253ms 882.564996ms 884.919073ms 889.274373ms 891.012961ms 893.528188ms 894.04417ms 895.276191ms 896.834107ms 899.897096ms 901.439286ms 902.496037ms 906.641862ms 925.09424ms 937.056343ms 941.857531ms 946.254004ms 951.648615ms 956.004659ms 972.307237ms 978.234507ms 978.575437ms 985.144153ms 997.878277ms 1.037855143s 1.041993162s 1.080861636s 1.098289485s 1.106345682s 1.109804592s 1.19641685s 1.209292636s 1.243499927s 1.243949249s 1.312237322s 1.345383351s 1.355325056s 1.368970341s 1.373067001s 1.376373053s 1.390566508s 1.394677663s 1.437431681s 1.491462288s 1.494840915s]
May 30 06:17:18.070: INFO: 50 %ile: 678.714699ms
May 30 06:17:18.070: INFO: 90 %ile: 1.041993162s
May 30 06:17:18.070: INFO: 99 %ile: 1.491462288s
May 30 06:17:18.070: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:17:18.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8941" for this suite.

• [SLOW TEST:15.646 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":303,"completed":24,"skipped":396,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:17:18.194: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2355
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 30 06:17:18.283: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 30 06:17:18.342: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:17:20.347: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:17:22.353: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:17:24.352: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:26.372: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:28.374: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:30.349: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:32.350: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:34.350: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:36.351: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:38.349: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:40.348: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:17:42.351: INFO: The status of Pod netserver-0 is Running (Ready = true)
STEP: Creating test pods
May 30 06:17:48.420: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.30:8080/dial?request=hostname&protocol=udp&host=10.244.0.29&port=8081&tries=1'] Namespace:pod-network-test-2355 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:17:48.429: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:17:49.447: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:17:49.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2355" for this suite.

• [SLOW TEST:31.268 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":303,"completed":25,"skipped":407,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:17:49.462: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:17:49.653: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a50918e6-3fe5-4912-aade-1f72d3743498", Controller:(*bool)(0xc0028bf6ea), BlockOwnerDeletion:(*bool)(0xc0028bf6eb)}}
May 30 06:17:49.694: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a8123219-2300-4f0a-9311-b2439d122178", Controller:(*bool)(0xc0025e53f6), BlockOwnerDeletion:(*bool)(0xc0025e53f7)}}
May 30 06:17:49.706: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"65be0063-ed62-4e02-9d84-5db8ed2172e2", Controller:(*bool)(0xc0028bf8f6), BlockOwnerDeletion:(*bool)(0xc0028bf8f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:17:54.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5080" for this suite.

• [SLOW TEST:6.114 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":303,"completed":26,"skipped":410,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:17:55.577: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:17:56.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904" in namespace "downward-api-9918" to be "Succeeded or Failed"
May 30 06:17:56.234: INFO: Pod "downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904": Phase="Pending", Reason="", readiness=false. Elapsed: 85.479019ms
May 30 06:17:58.292: INFO: Pod "downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904": Phase="Pending", Reason="", readiness=false. Elapsed: 2.143666795s
May 30 06:18:00.554: INFO: Pod "downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904": Phase="Pending", Reason="", readiness=false. Elapsed: 4.405935576s
May 30 06:18:02.673: INFO: Pod "downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904": Phase="Pending", Reason="", readiness=false. Elapsed: 6.525090442s
May 30 06:18:04.680: INFO: Pod "downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.532226863s
STEP: Saw pod success
May 30 06:18:04.680: INFO: Pod "downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904" satisfied condition "Succeeded or Failed"
May 30 06:18:04.683: INFO: Trying to get logs from node my-node pod downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904 container client-container: <nil>
STEP: delete the pod
May 30 06:18:04.714: INFO: Waiting for pod downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904 to disappear
May 30 06:18:04.719: INFO: Pod downwardapi-volume-16f439fe-044c-4ea7-a2fe-57f551041904 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:18:04.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9918" for this suite.

• [SLOW TEST:9.171 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":27,"skipped":414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:18:04.824: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:18:05.559: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 30 06:18:08.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488285, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488285, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488285, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488285, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:18:11.238: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:18:11.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4281" for this suite.
STEP: Destroying namespace "webhook-4281-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.754 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":303,"completed":28,"skipped":448,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:18:11.581: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:18:11.685: INFO: Creating ReplicaSet my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e
May 30 06:18:11.711: INFO: Pod name my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e: Found 0 pods out of 1
May 30 06:18:16.717: INFO: Pod name my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e: Found 1 pods out of 1
May 30 06:18:16.717: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e" is running
May 30 06:18:18.731: INFO: Pod "my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e-kzwth" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 06:18:11 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 06:18:11 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 06:18:11 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 06:18:11 +0000 UTC Reason: Message:}])
May 30 06:18:18.809: INFO: Trying to dial the pod
May 30 06:18:23.823: INFO: Controller my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e: Got expected result from replica 1 [my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e-kzwth]: "my-hostname-basic-ce415932-a346-4a8d-bc6a-4e6734e34a9e-kzwth", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:18:23.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2788" for this suite.

• [SLOW TEST:12.253 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":29,"skipped":450,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:18:23.848: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-155
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-155
I0530 06:18:23.983383      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-155, replica count: 2
I0530 06:18:27.034319      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:18:30.034746      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 06:18:30.035: INFO: Creating new exec pod
May 30 06:18:37.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-155 exec execpodz4lvm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 30 06:18:38.296: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 30 06:18:38.296: INFO: stdout: ""
May 30 06:18:38.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-155 exec execpodz4lvm -- /bin/sh -x -c nc -zv -t -w 2 10.110.21.104 80'
May 30 06:18:38.661: INFO: stderr: "+ nc -zv -t -w 2 10.110.21.104 80\nConnection to 10.110.21.104 80 port [tcp/http] succeeded!\n"
May 30 06:18:38.661: INFO: stdout: ""
May 30 06:18:38.661: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:18:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-155" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.949 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":303,"completed":30,"skipped":465,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:18:38.796: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-klvr
STEP: Creating a pod to test atomic-volume-subpath
May 30 06:18:39.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-klvr" in namespace "subpath-3478" to be "Succeeded or Failed"
May 30 06:18:39.790: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Pending", Reason="", readiness=false. Elapsed: 18.967506ms
May 30 06:18:41.800: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029047228s
May 30 06:18:43.810: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039790186s
May 30 06:18:45.916: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 6.145356309s
May 30 06:18:47.925: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 8.154699689s
May 30 06:18:50.306: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 10.534975002s
May 30 06:18:52.313: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 12.542780936s
May 30 06:18:54.318: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 14.546922866s
May 30 06:18:56.322: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 16.551064917s
May 30 06:18:58.327: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 18.555943976s
May 30 06:19:00.330: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 20.559036706s
May 30 06:19:02.336: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Running", Reason="", readiness=true. Elapsed: 22.565125649s
May 30 06:19:04.353: INFO: Pod "pod-subpath-test-configmap-klvr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.582434732s
STEP: Saw pod success
May 30 06:19:04.354: INFO: Pod "pod-subpath-test-configmap-klvr" satisfied condition "Succeeded or Failed"
May 30 06:19:04.359: INFO: Trying to get logs from node my-node pod pod-subpath-test-configmap-klvr container test-container-subpath-configmap-klvr: <nil>
STEP: delete the pod
May 30 06:19:04.478: INFO: Waiting for pod pod-subpath-test-configmap-klvr to disappear
May 30 06:19:04.561: INFO: Pod pod-subpath-test-configmap-klvr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-klvr
May 30 06:19:04.561: INFO: Deleting pod "pod-subpath-test-configmap-klvr" in namespace "subpath-3478"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:19:04.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3478" for this suite.

• [SLOW TEST:25.826 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":303,"completed":31,"skipped":465,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:19:04.624: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 30 06:19:08.747: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6128 PodName:var-expansion-9c308ec5-2133-4f71-8513-4e73d5e26e20 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:19:08.747: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: test for file in mounted path
May 30 06:19:09.000: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6128 PodName:var-expansion-9c308ec5-2133-4f71-8513-4e73d5e26e20 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:19:09.000: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: updating the annotation value
May 30 06:19:09.721: INFO: Successfully updated pod "var-expansion-9c308ec5-2133-4f71-8513-4e73d5e26e20"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 30 06:19:09.726: INFO: Deleting pod "var-expansion-9c308ec5-2133-4f71-8513-4e73d5e26e20" in namespace "var-expansion-6128"
May 30 06:19:09.742: INFO: Wait up to 5m0s for pod "var-expansion-9c308ec5-2133-4f71-8513-4e73d5e26e20" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:19:55.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6128" for this suite.

• [SLOW TEST:51.143 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":303,"completed":32,"skipped":467,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:19:55.777: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:20:06.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9237" for this suite.

• [SLOW TEST:11.162 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":303,"completed":33,"skipped":484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:20:06.950: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:20:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7753" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":303,"completed":34,"skipped":513,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:20:11.058: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 30 06:20:11.103: INFO: Waiting up to 5m0s for pod "pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9" in namespace "emptydir-4074" to be "Succeeded or Failed"
May 30 06:20:11.109: INFO: Pod "pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523844ms
May 30 06:20:13.260: INFO: Pod "pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157289285s
May 30 06:20:15.414: INFO: Pod "pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.311669296s
STEP: Saw pod success
May 30 06:20:15.414: INFO: Pod "pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9" satisfied condition "Succeeded or Failed"
May 30 06:20:15.675: INFO: Trying to get logs from node my-node pod pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9 container test-container: <nil>
STEP: delete the pod
May 30 06:20:15.730: INFO: Waiting for pod pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9 to disappear
May 30 06:20:15.734: INFO: Pod pod-9d0c3809-fd14-4ac1-bc56-4e17e762b0a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:20:15.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4074" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":35,"skipped":513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:20:15.762: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:20:15.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6029" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":303,"completed":36,"skipped":557,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:20:15.875: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 30 06:20:26.022: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 30 06:20:26.026: INFO: Pod pod-with-poststart-exec-hook still exists
May 30 06:20:28.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 30 06:20:28.031: INFO: Pod pod-with-poststart-exec-hook still exists
May 30 06:20:30.027: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 30 06:20:30.030: INFO: Pod pod-with-poststart-exec-hook still exists
May 30 06:20:32.027: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 30 06:20:32.032: INFO: Pod pod-with-poststart-exec-hook still exists
May 30 06:20:34.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 30 06:20:34.032: INFO: Pod pod-with-poststart-exec-hook still exists
May 30 06:20:36.027: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 30 06:20:36.030: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:20:36.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8480" for this suite.

• [SLOW TEST:20.167 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":303,"completed":37,"skipped":603,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:20:36.042: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
May 30 06:20:42.149: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-53 PodName:pod-sharedvolume-73ba49c7-3076-40f0-9e5f-560066ba917e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:20:42.149: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:20:42.396: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:20:42.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-53" for this suite.

• [SLOW TEST:6.364 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":303,"completed":38,"skipped":612,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:20:42.406: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:20:43.494: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 06:20:45.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:20:47.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789488443, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:20:50.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:21:03.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6365" for this suite.
STEP: Destroying namespace "webhook-6365-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:21.045 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":303,"completed":39,"skipped":619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:21:03.451: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3964
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 30 06:21:03.613: INFO: Found 0 stateful pods, waiting for 3
May 30 06:21:13.621: INFO: Found 1 stateful pods, waiting for 3
May 30 06:21:23.630: INFO: Found 1 stateful pods, waiting for 3
May 30 06:21:33.619: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:21:33.619: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:21:33.619: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
May 30 06:21:43.622: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:21:43.622: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:21:43.622: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:21:43.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-3964 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 06:21:44.046: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 06:21:44.046: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 06:21:44.046: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 30 06:21:54.109: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 30 06:22:04.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-3964 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 06:22:04.511: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 30 06:22:04.511: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 06:22:04.511: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 06:22:14.538: INFO: Waiting for StatefulSet statefulset-3964/ss2 to complete update
May 30 06:22:14.538: INFO: Waiting for Pod statefulset-3964/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:22:14.538: INFO: Waiting for Pod statefulset-3964/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:22:24.546: INFO: Waiting for StatefulSet statefulset-3964/ss2 to complete update
May 30 06:22:24.547: INFO: Waiting for Pod statefulset-3964/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:22:24.547: INFO: Waiting for Pod statefulset-3964/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:22:34.551: INFO: Waiting for StatefulSet statefulset-3964/ss2 to complete update
May 30 06:22:34.551: INFO: Waiting for Pod statefulset-3964/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:22:34.551: INFO: Waiting for Pod statefulset-3964/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:22:44.546: INFO: Waiting for StatefulSet statefulset-3964/ss2 to complete update
May 30 06:22:44.546: INFO: Waiting for Pod statefulset-3964/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
May 30 06:22:54.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-3964 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 06:22:54.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 06:22:54.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 06:22:54.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 06:23:04.960: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 30 06:23:14.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-3964 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 06:23:15.351: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 30 06:23:15.351: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 06:23:15.351: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 06:23:45.410: INFO: Waiting for StatefulSet statefulset-3964/ss2 to complete update
May 30 06:23:45.410: INFO: Waiting for Pod statefulset-3964/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 30 06:23:55.407: INFO: Deleting all statefulset in ns statefulset-3964
May 30 06:23:55.411: INFO: Scaling statefulset ss2 to 0
May 30 06:24:25.431: INFO: Waiting for statefulset status.replicas updated to 0
May 30 06:24:25.435: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:24:25.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3964" for this suite.

• [SLOW TEST:202.101 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":303,"completed":40,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:24:25.553: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:24:25.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6660" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":303,"completed":41,"skipped":694,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:24:25.656: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-db4b7074-da0c-4b5f-ba14-583c80811397
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-db4b7074-da0c-4b5f-ba14-583c80811397
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:25:56.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3841" for this suite.

• [SLOW TEST:90.768 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":42,"skipped":711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:25:56.425: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 in namespace container-probe-3302
May 30 06:26:00.507: INFO: Started pod liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 in namespace container-probe-3302
STEP: checking the pod's current state and verifying that restartCount is present
May 30 06:26:00.510: INFO: Initial restart count of pod liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 is 0
May 30 06:26:12.587: INFO: Restart count of pod container-probe-3302/liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 is now 1 (12.076168325s elapsed)
May 30 06:26:32.690: INFO: Restart count of pod container-probe-3302/liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 is now 2 (32.17910557s elapsed)
May 30 06:26:52.756: INFO: Restart count of pod container-probe-3302/liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 is now 3 (52.245509809s elapsed)
May 30 06:27:12.836: INFO: Restart count of pod container-probe-3302/liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 is now 4 (1m12.32581673s elapsed)
May 30 06:28:15.059: INFO: Restart count of pod container-probe-3302/liveness-bea21cb3-6932-406f-8d97-e7fdbfcba953 is now 5 (2m14.54811236s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:28:15.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3302" for this suite.

• [SLOW TEST:138.697 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":303,"completed":43,"skipped":739,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:28:15.123: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9681, will wait for the garbage collector to delete the pods
May 30 06:28:22.086: INFO: Deleting Job.batch foo took: 112.363795ms
May 30 06:28:22.489: INFO: Terminating Job.batch foo pods took: 402.196523ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:28:56.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9681" for this suite.

• [SLOW TEST:41.183 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":303,"completed":44,"skipped":746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:28:56.308: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 30 06:28:56.405: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3268 /api/v1/namespaces/watch-3268/configmaps/e2e-watch-test-label-changed ed5f20e3-3b1c-45fd-9e3d-65dcdb3f645d 43677 0 2022-05-30 06:28:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-30 06:28:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 06:28:56.432: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3268 /api/v1/namespaces/watch-3268/configmaps/e2e-watch-test-label-changed ed5f20e3-3b1c-45fd-9e3d-65dcdb3f645d 43678 0 2022-05-30 06:28:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-30 06:28:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 06:28:56.433: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3268 /api/v1/namespaces/watch-3268/configmaps/e2e-watch-test-label-changed ed5f20e3-3b1c-45fd-9e3d-65dcdb3f645d 43679 0 2022-05-30 06:28:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-30 06:28:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 30 06:29:06.472: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3268 /api/v1/namespaces/watch-3268/configmaps/e2e-watch-test-label-changed ed5f20e3-3b1c-45fd-9e3d-65dcdb3f645d 43721 0 2022-05-30 06:28:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-30 06:28:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 06:29:06.473: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3268 /api/v1/namespaces/watch-3268/configmaps/e2e-watch-test-label-changed ed5f20e3-3b1c-45fd-9e3d-65dcdb3f645d 43722 0 2022-05-30 06:28:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-30 06:28:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 06:29:06.473: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3268 /api/v1/namespaces/watch-3268/configmaps/e2e-watch-test-label-changed ed5f20e3-3b1c-45fd-9e3d-65dcdb3f645d 43723 0 2022-05-30 06:28:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-30 06:28:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:29:06.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3268" for this suite.

• [SLOW TEST:10.178 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":303,"completed":45,"skipped":769,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:29:06.486: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-9f246425-956d-4f3a-aec5-14d525b61ecf
STEP: Creating a pod to test consume secrets
May 30 06:29:06.575: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9" in namespace "projected-2682" to be "Succeeded or Failed"
May 30 06:29:06.589: INFO: Pod "pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.343445ms
May 30 06:29:08.806: INFO: Pod "pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230954742s
May 30 06:29:10.810: INFO: Pod "pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.235708187s
STEP: Saw pod success
May 30 06:29:10.810: INFO: Pod "pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9" satisfied condition "Succeeded or Failed"
May 30 06:29:10.813: INFO: Trying to get logs from node my-node pod pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 30 06:29:11.157: INFO: Waiting for pod pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9 to disappear
May 30 06:29:11.164: INFO: Pod pod-projected-secrets-13c0d01c-7ce0-40cb-8b1c-0c7bb73381c9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:29:11.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2682" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":46,"skipped":771,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:29:11.198: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 30 06:29:14.360: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:29:14.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7768" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":47,"skipped":782,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:29:14.473: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:29:14.537: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Pending, waiting for it to be Running (with Ready = true)
May 30 06:29:16.616: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Pending, waiting for it to be Running (with Ready = true)
May 30 06:29:18.541: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:20.543: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:22.545: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:24.544: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:26.544: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:28.544: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:30.543: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:32.547: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:34.547: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = false)
May 30 06:29:36.553: INFO: The status of Pod test-webserver-e019dacd-7958-46fe-a0b9-45e7f283facc is Running (Ready = true)
May 30 06:29:36.557: INFO: Container started at 2022-05-30 06:29:16 +0000 UTC, pod became ready at 2022-05-30 06:29:34 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:29:36.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5627" for this suite.

• [SLOW TEST:22.106 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":303,"completed":48,"skipped":792,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:29:36.579: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:29:36.621: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 30 06:29:39.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-3462 --namespace=crd-publish-openapi-3462 create -f -'
May 30 06:29:40.704: INFO: stderr: ""
May 30 06:29:40.704: INFO: stdout: "e2e-test-crd-publish-openapi-4444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 30 06:29:40.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-3462 --namespace=crd-publish-openapi-3462 delete e2e-test-crd-publish-openapi-4444-crds test-cr'
May 30 06:29:40.898: INFO: stderr: ""
May 30 06:29:40.898: INFO: stdout: "e2e-test-crd-publish-openapi-4444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 30 06:29:40.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-3462 --namespace=crd-publish-openapi-3462 apply -f -'
May 30 06:29:41.249: INFO: stderr: ""
May 30 06:29:41.249: INFO: stdout: "e2e-test-crd-publish-openapi-4444-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 30 06:29:41.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-3462 --namespace=crd-publish-openapi-3462 delete e2e-test-crd-publish-openapi-4444-crds test-cr'
May 30 06:29:41.359: INFO: stderr: ""
May 30 06:29:41.359: INFO: stdout: "e2e-test-crd-publish-openapi-4444-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 30 06:29:41.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-3462 explain e2e-test-crd-publish-openapi-4444-crds'
May 30 06:29:41.599: INFO: stderr: ""
May 30 06:29:41.599: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4444-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:29:43.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3462" for this suite.

• [SLOW TEST:6.814 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":303,"completed":49,"skipped":808,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:29:43.394: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 30 06:29:44.289: INFO: Pod name wrapped-volume-race-fa4af8b2-7e96-4971-8f00-91965cb6cca5: Found 0 pods out of 5
May 30 06:29:49.309: INFO: Pod name wrapped-volume-race-fa4af8b2-7e96-4971-8f00-91965cb6cca5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fa4af8b2-7e96-4971-8f00-91965cb6cca5 in namespace emptydir-wrapper-6935, will wait for the garbage collector to delete the pods
May 30 06:30:07.409: INFO: Deleting ReplicationController wrapped-volume-race-fa4af8b2-7e96-4971-8f00-91965cb6cca5 took: 9.650831ms
May 30 06:30:07.809: INFO: Terminating ReplicationController wrapped-volume-race-fa4af8b2-7e96-4971-8f00-91965cb6cca5 pods took: 400.347374ms
STEP: Creating RC which spawns configmap-volume pods
May 30 06:30:25.432: INFO: Pod name wrapped-volume-race-1f0be419-ab17-491e-8dda-9907916d04a0: Found 0 pods out of 5
May 30 06:30:30.443: INFO: Pod name wrapped-volume-race-1f0be419-ab17-491e-8dda-9907916d04a0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1f0be419-ab17-491e-8dda-9907916d04a0 in namespace emptydir-wrapper-6935, will wait for the garbage collector to delete the pods
May 30 06:30:50.535: INFO: Deleting ReplicationController wrapped-volume-race-1f0be419-ab17-491e-8dda-9907916d04a0 took: 7.337093ms
May 30 06:30:51.135: INFO: Terminating ReplicationController wrapped-volume-race-1f0be419-ab17-491e-8dda-9907916d04a0 pods took: 600.399965ms
STEP: Creating RC which spawns configmap-volume pods
May 30 06:31:05.557: INFO: Pod name wrapped-volume-race-3b1b46d9-d599-4b3a-a81f-76d5689960fd: Found 0 pods out of 5
May 30 06:31:10.563: INFO: Pod name wrapped-volume-race-3b1b46d9-d599-4b3a-a81f-76d5689960fd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3b1b46d9-d599-4b3a-a81f-76d5689960fd in namespace emptydir-wrapper-6935, will wait for the garbage collector to delete the pods
May 30 06:31:30.659: INFO: Deleting ReplicationController wrapped-volume-race-3b1b46d9-d599-4b3a-a81f-76d5689960fd took: 11.142241ms
May 30 06:31:31.162: INFO: Terminating ReplicationController wrapped-volume-race-3b1b46d9-d599-4b3a-a81f-76d5689960fd pods took: 502.869141ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:31:45.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6935" for this suite.

• [SLOW TEST:122.621 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":303,"completed":50,"skipped":822,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:31:46.017: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-z9qs
STEP: Creating a pod to test atomic-volume-subpath
May 30 06:31:46.084: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-z9qs" in namespace "subpath-5606" to be "Succeeded or Failed"
May 30 06:31:46.094: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085767ms
May 30 06:31:48.114: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029538698s
May 30 06:31:50.118: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033641954s
May 30 06:31:52.121: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 6.036787362s
May 30 06:31:54.125: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 8.041170319s
May 30 06:31:56.141: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 10.056680528s
May 30 06:31:58.145: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 12.060451964s
May 30 06:32:00.152: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 14.0674397s
May 30 06:32:02.183: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 16.098987217s
May 30 06:32:04.189: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 18.105392173s
May 30 06:32:06.194: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 20.109839244s
May 30 06:32:08.198: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 22.113911466s
May 30 06:32:10.204: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 24.11978921s
May 30 06:32:12.213: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Running", Reason="", readiness=true. Elapsed: 26.128881153s
May 30 06:32:14.220: INFO: Pod "pod-subpath-test-downwardapi-z9qs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.136404436s
STEP: Saw pod success
May 30 06:32:14.221: INFO: Pod "pod-subpath-test-downwardapi-z9qs" satisfied condition "Succeeded or Failed"
May 30 06:32:14.226: INFO: Trying to get logs from node my-node pod pod-subpath-test-downwardapi-z9qs container test-container-subpath-downwardapi-z9qs: <nil>
STEP: delete the pod
May 30 06:32:14.279: INFO: Waiting for pod pod-subpath-test-downwardapi-z9qs to disappear
May 30 06:32:14.288: INFO: Pod pod-subpath-test-downwardapi-z9qs no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-z9qs
May 30 06:32:14.291: INFO: Deleting pod "pod-subpath-test-downwardapi-z9qs" in namespace "subpath-5606"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:32:14.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5606" for this suite.

• [SLOW TEST:28.296 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":303,"completed":51,"skipped":835,"failed":0}
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:32:14.313: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:32:14.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-76" for this suite.
STEP: Destroying namespace "nspatchtest-fec79b4a-c939-4403-86ed-5e07847bda97-106" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":303,"completed":52,"skipped":835,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:32:14.408: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:32:14.468: INFO: Creating deployment "test-recreate-deployment"
May 30 06:32:14.473: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 30 06:32:14.483: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
May 30 06:32:16.534: INFO: Waiting deployment "test-recreate-deployment" to complete
May 30 06:32:16.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489134, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489134, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489134, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489134, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:32:18.543: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 30 06:32:18.571: INFO: Updating deployment test-recreate-deployment
May 30 06:32:18.571: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 30 06:32:19.241: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2586 /apis/apps/v1/namespaces/deployment-2586/deployments/test-recreate-deployment 728bca57-9b11-4ac9-9e52-701537a7af02 45148 2 2022-05-30 06:32:14 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-30 06:32:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-05-30 06:32:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003049918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-30 06:32:18 +0000 UTC,LastTransitionTime:2022-05-30 06:32:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2022-05-30 06:32:18 +0000 UTC,LastTransitionTime:2022-05-30 06:32:14 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 30 06:32:19.245: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-2586 /apis/apps/v1/namespaces/deployment-2586/replicasets/test-recreate-deployment-f79dd4667 078483ca-c5d8-4d20-862e-79d84df79a00 45146 1 2022-05-30 06:32:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 728bca57-9b11-4ac9-9e52-701537a7af02 0xc003049e30 0xc003049e31}] []  [{kube-controller-manager Update apps/v1 2022-05-30 06:32:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"728bca57-9b11-4ac9-9e52-701537a7af02\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003049ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 30 06:32:19.245: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 30 06:32:19.245: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-2586 /apis/apps/v1/namespaces/deployment-2586/replicasets/test-recreate-deployment-c96cf48f f0beccec-e175-42dc-82be-2f6af897a914 45137 2 2022-05-30 06:32:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 728bca57-9b11-4ac9-9e52-701537a7af02 0xc003049d3f 0xc003049d50}] []  [{kube-controller-manager Update apps/v1 2022-05-30 06:32:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"728bca57-9b11-4ac9-9e52-701537a7af02\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003049dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 30 06:32:19.495: INFO: Pod "test-recreate-deployment-f79dd4667-95j2w" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-95j2w test-recreate-deployment-f79dd4667- deployment-2586 /api/v1/namespaces/deployment-2586/pods/test-recreate-deployment-f79dd4667-95j2w 45c8d893-4326-44cd-a383-388a4b22771a 45149 0 2022-05-30 06:32:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 078483ca-c5d8-4d20-862e-79d84df79a00 0xc0007a4560 0xc0007a4561}] []  [{kube-controller-manager Update v1 2022-05-30 06:32:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"078483ca-c5d8-4d20-862e-79d84df79a00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 06:32:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2c9ln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2c9ln,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2c9ln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 06:32:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 06:32:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 06:32:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 06:32:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 06:32:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:32:19.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2586" for this suite.

• [SLOW TEST:5.147 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":53,"skipped":836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:32:19.555: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:32:20.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204" in namespace "downward-api-119" to be "Succeeded or Failed"
May 30 06:32:20.614: INFO: Pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204": Phase="Pending", Reason="", readiness=false. Elapsed: 14.457134ms
May 30 06:32:22.786: INFO: Pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187178419s
May 30 06:32:24.983: INFO: Pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204": Phase="Pending", Reason="", readiness=false. Elapsed: 4.383970143s
May 30 06:32:27.105: INFO: Pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506041413s
May 30 06:32:29.222: INFO: Pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204": Phase="Pending", Reason="", readiness=false. Elapsed: 8.622729974s
May 30 06:32:31.320: INFO: Pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.720426631s
STEP: Saw pod success
May 30 06:32:31.320: INFO: Pod "downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204" satisfied condition "Succeeded or Failed"
May 30 06:32:31.329: INFO: Trying to get logs from node my-node pod downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204 container client-container: <nil>
STEP: delete the pod
May 30 06:32:31.363: INFO: Waiting for pod downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204 to disappear
May 30 06:32:31.383: INFO: Pod downwardapi-volume-8310d0d8-58f2-49ab-8c0d-6bea5ae8b204 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:32:31.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-119" for this suite.

• [SLOW TEST:11.839 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":54,"skipped":871,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:32:31.394: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 30 06:32:39.138: INFO: 10 pods remaining
May 30 06:32:39.138: INFO: 0 pods has nil DeletionTimestamp
May 30 06:32:39.138: INFO: 
May 30 06:32:39.809: INFO: 0 pods remaining
May 30 06:32:39.810: INFO: 0 pods has nil DeletionTimestamp
May 30 06:32:39.810: INFO: 
STEP: Gathering metrics
W0530 06:32:40.778367      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 06:33:43.339: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:33:43.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1169" for this suite.

• [SLOW TEST:71.958 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":303,"completed":55,"skipped":888,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:33:43.352: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:33:58.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3536" for this suite.
STEP: Destroying namespace "nsdeletetest-5810" for this suite.
May 30 06:33:58.553: INFO: Namespace nsdeletetest-5810 was already deleted
STEP: Destroying namespace "nsdeletetest-7634" for this suite.

• [SLOW TEST:15.212 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":303,"completed":56,"skipped":897,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:33:58.564: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:33:58.649: FAIL: Conformance test suite needs a cluster with at least 2 nodes.
Expected
    <int>: 1
to be >
    <int>: 1

Full Stack Trace
k8s.io/kubernetes/test/e2e/apps.glob..func3.9()
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:423 +0x27b
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc001dde480)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc001dde480)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc001dde480, 0x4df04f8)
	/usr/local/go/src/testing/testing.go:1123 +0xef
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1168 +0x2b3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
May 30 06:33:58.714: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5256/daemonsets","resourceVersion":"45624"},"items":null}

May 30 06:33:58.717: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5256/pods","resourceVersion":"45624"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "daemonsets-5256".
STEP: Found 0 events.
May 30 06:33:58.728: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
May 30 06:33:58.729: INFO: 
May 30 06:33:58.749: INFO: 
Logging node info for node my-node
May 30 06:33:58.759: INFO: Node Info: &Node{ObjectMeta:{my-node   /api/v1/nodes/my-node f75ed23b-2f88-4ce1-8189-6f41b657b75d 45352 0 2022-05-27 09:42:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:my-node kubernetes.io/os:linux node-role.kubernetes.io/master:] map[flannel.alpha.coreos.com/backend-data:{"VNI":1,"VtepMAC":"ee:c9:e8:56:92:a4"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.0.2.15 kubeadm.alpha.kubernetes.io/cri-socket:/var/run/dockershim.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2022-05-27 09:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {kubeadm Update v1 2022-05-27 09:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/master":{}}}}} {flanneld Update v1 2022-05-27 09:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{},"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}}} {kube-controller-manager Update v1 2022-05-27 09:44:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}}}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{138856275968 0} {<nil>}  BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{4116410368 0} {<nil>} 4019932Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{124970648165 0} {<nil>} 124970648165 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{4011552768 0} {<nil>} 3917532Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-05-30 01:39:18 +0000 UTC,LastTransitionTime:2022-05-30 01:39:18 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-05-30 06:32:43 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-05-30 06:32:43 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-05-30 06:32:43 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-05-30 06:32:43 +0000 UTC,LastTransitionTime:2022-05-27 09:44:25 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.2.15,},NodeAddress{Type:Hostname,Address:my-node,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:30ab1f02f59e4093b9f48497b6753f85,SystemUUID:a3aa5fdb-d52d-9a42-8fa5-7062181ac666,BootID:67aea70c-c63d-41d9-9999-5d2b2003e8de,KernelVersion:5.13.0-44-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:docker://19.3.15,KubeletVersion:v1.19.16,KubeProxyVersion:v1.19.16,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd sonobuoy/systemd-logs:v0.4],SizeBytes:314096343,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:e1ec90b9a30894aa71eaa6cdf7dcaae6f5d899692460b58973cdc2c26dc72afa k8s.gcr.io/conformance:v1.19.16],SizeBytes:229709026,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver@sha256:2cad6a4cae1713e720e00e1a1c7ef7644777fe111e0b7cbed5f50adb8a3cdf30 k8s.gcr.io/kube-apiserver:v1.19.16],SizeBytes:118899101,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0 k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager@sha256:1d42f7d017f4ceeff46afb9cedd47b2b8140d1f2cc3bfebb57a40c3760acd482 k8s.gcr.io/kube-controller-manager:v1.19.16],SizeBytes:110870941,},ContainerImage{Names:[k8s.gcr.io/kube-proxy@sha256:092f9526686d27964d17be772c42cde086690209cc8aea10c49991456eb879c2 k8s.gcr.io/kube-proxy:v1.19.16],SizeBytes:98938510,},ContainerImage{Names:[rancher/mirrored-flannelcni-flannel@sha256:bf56c4e594948eb5c6b3bc04cdde3477d6de12a85b3bbb67ae3a518142cd8392 rancher/mirrored-flannelcni-flannel:v0.18.0],SizeBytes:62262505,},ContainerImage{Names:[k8s.gcr.io/kube-scheduler@sha256:1a335251eaef4e209d5757da0bf5499ecdce6e65413f7cb92ff4cc633d6fc7dd k8s.gcr.io/kube-scheduler:v1.19.16],SizeBytes:46490013,},ContainerImage{Names:[k8s.gcr.io/coredns@sha256:73ca82b4ce829766d4f1f10947c3a338888f876fbed0540dc849c89ff256e90c k8s.gcr.io/coredns:1.7.0],SizeBytes:45227747,},ContainerImage{Names:[sonobuoy/sonobuoy@sha256:ed5bd072698f80e7fa706a69e4b01eeedbddeadc4821587297a29b727b5b5f19 sonobuoy/sonobuoy:v0.56.6],SizeBytes:44096145,},ContainerImage{Names:[nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0],SizeBytes:8087907,},ContainerImage{Names:[busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 busybox:1.29],SizeBytes:1154361,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:682696,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
May 30 06:33:58.774: INFO: 
Logging kubelet events for node my-node
May 30 06:33:58.779: INFO: 
Logging pods the kubelet thinks is on node my-node
May 30 06:33:58.820: INFO: kube-apiserver-my-node started at 2022-05-30 01:38:05 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container kube-apiserver ready: true, restart count 1
May 30 06:33:58.820: INFO: coredns-f9fd979d6-c9tcq started at 2022-05-27 09:44:35 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container coredns ready: true, restart count 1
May 30 06:33:58.820: INFO: kube-controller-manager-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 06:33:58.820: INFO: sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 30 06:33:58.820: INFO: etcd-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container etcd ready: true, restart count 1
May 30 06:33:58.820: INFO: coredns-f9fd979d6-lrzv9 started at 2022-05-27 09:44:25 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container coredns ready: true, restart count 1
May 30 06:33:58.820: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 started at 2022-05-30 06:08:24 +0000 UTC (0+2 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container e2e ready: true, restart count 0
May 30 06:33:58.820: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 06:33:58.820: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 started at 2022-05-30 06:08:25 +0000 UTC (0+2 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 06:33:58.820: INFO: 	Container systemd-logs ready: true, restart count 0
May 30 06:33:58.820: INFO: kube-scheduler-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 06:33:58.820: INFO: kube-proxy-4pwdh started at 2022-05-27 09:42:33 +0000 UTC (0+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 06:33:58.820: INFO: kube-flannel-ds-lv5nv started at 2022-05-27 09:43:53 +0000 UTC (2+1 container statuses recorded)
May 30 06:33:58.820: INFO: 	Init container install-cni-plugin ready: true, restart count 1
May 30 06:33:58.820: INFO: 	Init container install-cni ready: true, restart count 0
May 30 06:33:58.820: INFO: 	Container kube-flannel ready: true, restart count 1
W0530 06:33:58.825570      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 06:33:59.091: INFO: 
Latency metrics for node my-node
May 30 06:33:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5256" for this suite.

• Failure [0.538 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance] [It]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  May 30 06:33:58.649: Conformance test suite needs a cluster with at least 2 nodes.
  Expected
      <int>: 1
  to be >
      <int>: 1

  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:423
------------------------------
{"msg":"FAILED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":303,"completed":56,"skipped":900,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:33:59.102: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:33:59.772: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 06:34:01.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489239, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489239, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489239, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489239, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:34:04.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:34:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1832" for this suite.
STEP: Destroying namespace "webhook-1832-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.167 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":303,"completed":57,"skipped":935,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
S
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:34:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 30 06:34:06.254: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 30 06:34:06.269: INFO: starting watch
STEP: patching
STEP: updating
May 30 06:34:06.568: INFO: waiting for watch events with expected annotations
May 30 06:34:06.572: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:34:07.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5584" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":303,"completed":58,"skipped":936,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:34:07.857: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0530 06:34:19.020042      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 06:35:21.614: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 30 06:35:21.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-45b2r" in namespace "gc-1025"
May 30 06:35:21.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pprr" in namespace "gc-1025"
May 30 06:35:21.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ncqc" in namespace "gc-1025"
May 30 06:35:21.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-crwkp" in namespace "gc-1025"
May 30 06:35:21.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfm7p" in namespace "gc-1025"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:35:21.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1025" for this suite.

• [SLOW TEST:73.958 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":303,"completed":59,"skipped":946,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:35:21.815: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 30 06:35:23.096: INFO: starting watch
STEP: patching
STEP: updating
May 30 06:35:23.127: INFO: waiting for watch events with expected annotations
May 30 06:35:23.128: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:35:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3003" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":303,"completed":60,"skipped":975,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:35:23.470: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:35:33.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-157" for this suite.

• [SLOW TEST:10.227 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":303,"completed":61,"skipped":1000,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:35:33.699: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:35:34.151: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:35:34.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5852" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":303,"completed":62,"skipped":1000,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:35:34.808: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5363.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5363.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5363.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 06:35:44.927: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:44.934: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:44.939: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:44.944: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:44.962: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:44.973: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:44.980: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:44.984: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:45.003: INFO: Lookups using dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local]

May 30 06:35:50.009: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.012: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.015: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.018: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.027: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.031: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.036: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.039: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:50.045: INFO: Lookups using dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local]

May 30 06:35:55.011: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.019: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.025: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.030: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.045: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.049: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.055: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.066: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:35:55.083: INFO: Lookups using dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local]

May 30 06:36:00.008: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.011: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.014: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.018: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.029: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.033: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.038: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.045: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:00.052: INFO: Lookups using dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local]

May 30 06:36:05.010: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.015: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.020: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.027: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.040: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.043: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.049: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.062: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:05.071: INFO: Lookups using dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local]

May 30 06:36:10.015: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.021: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.026: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.031: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.042: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.047: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.050: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.053: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local from pod dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e: the server could not find the requested resource (get pods dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e)
May 30 06:36:10.073: INFO: Lookups using dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5363.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5363.svc.cluster.local jessie_udp@dns-test-service-2.dns-5363.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5363.svc.cluster.local]

May 30 06:36:15.053: INFO: DNS probes using dns-5363/dns-test-0e4b3773-39b9-4add-9b29-4ae84ea25f0e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:36:15.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5363" for this suite.

• [SLOW TEST:40.411 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":303,"completed":63,"skipped":1000,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:36:15.243: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 30 06:36:15.357: INFO: Waiting up to 5m0s for pod "downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239" in namespace "downward-api-6265" to be "Succeeded or Failed"
May 30 06:36:15.371: INFO: Pod "downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239": Phase="Pending", Reason="", readiness=false. Elapsed: 13.39457ms
May 30 06:36:17.377: INFO: Pod "downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019922772s
May 30 06:36:19.431: INFO: Pod "downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073824943s
May 30 06:36:21.495: INFO: Pod "downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.137671853s
STEP: Saw pod success
May 30 06:36:21.497: INFO: Pod "downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239" satisfied condition "Succeeded or Failed"
May 30 06:36:21.519: INFO: Trying to get logs from node my-node pod downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239 container dapi-container: <nil>
STEP: delete the pod
May 30 06:36:21.551: INFO: Waiting for pod downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239 to disappear
May 30 06:36:21.553: INFO: Pod downward-api-7da6a85b-49d1-4f9f-815c-1e9562a1c239 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:36:21.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6265" for this suite.

• [SLOW TEST:6.327 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":303,"completed":64,"skipped":1024,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:36:21.567: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8644
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
May 30 06:36:21.687: INFO: Found 0 stateful pods, waiting for 3
May 30 06:36:31.761: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:36:31.761: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:36:31.761: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 30 06:36:41.701: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:36:41.701: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:36:41.701: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 30 06:36:41.734: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 30 06:36:51.782: INFO: Updating stateful set ss2
May 30 06:36:51.834: INFO: Waiting for Pod statefulset-8644/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:37:01.842: INFO: Waiting for Pod statefulset-8644/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 30 06:37:12.010: INFO: Found 2 stateful pods, waiting for 3
May 30 06:37:22.019: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:37:22.019: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 06:37:22.019: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 30 06:37:22.050: INFO: Updating stateful set ss2
May 30 06:37:22.076: INFO: Waiting for Pod statefulset-8644/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:37:32.105: INFO: Updating stateful set ss2
May 30 06:37:32.142: INFO: Waiting for StatefulSet statefulset-8644/ss2 to complete update
May 30 06:37:32.142: INFO: Waiting for Pod statefulset-8644/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 30 06:37:42.149: INFO: Waiting for StatefulSet statefulset-8644/ss2 to complete update
May 30 06:37:42.149: INFO: Waiting for Pod statefulset-8644/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 30 06:37:52.156: INFO: Deleting all statefulset in ns statefulset-8644
May 30 06:37:52.159: INFO: Scaling statefulset ss2 to 0
May 30 06:38:32.200: INFO: Waiting for statefulset status.replicas updated to 0
May 30 06:38:32.203: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:38:32.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8644" for this suite.

• [SLOW TEST:130.674 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":303,"completed":65,"skipped":1027,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:38:32.241: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 30 06:38:40.341: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:40.348: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:42.349: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:42.353: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:44.349: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:44.354: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:46.349: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:46.356: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:48.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:48.355: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:50.349: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:50.353: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:52.351: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:52.360: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:54.349: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:54.354: INFO: Pod pod-with-prestop-http-hook still exists
May 30 06:38:56.348: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 30 06:38:56.353: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:38:56.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6705" for this suite.

• [SLOW TEST:24.149 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":303,"completed":66,"skipped":1030,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:38:56.390: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2076
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 30 06:38:56.446: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 30 06:38:56.468: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:38:58.667: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:39:00.471: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:39:02.518: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:39:04.479: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:39:06.473: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:39:08.471: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:39:10.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:39:12.471: INFO: The status of Pod netserver-0 is Running (Ready = true)
STEP: Creating test pods
May 30 06:39:20.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.113:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2076 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:39:20.525: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:39:20.866: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:39:20.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2076" for this suite.

• [SLOW TEST:24.488 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":67,"skipped":1030,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:39:20.886: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:39:20.937: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088" in namespace "downward-api-2495" to be "Succeeded or Failed"
May 30 06:39:20.953: INFO: Pod "downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088": Phase="Pending", Reason="", readiness=false. Elapsed: 16.329552ms
May 30 06:39:22.962: INFO: Pod "downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025663418s
May 30 06:39:24.968: INFO: Pod "downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031101641s
May 30 06:39:26.973: INFO: Pod "downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036244283s
STEP: Saw pod success
May 30 06:39:26.973: INFO: Pod "downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088" satisfied condition "Succeeded or Failed"
May 30 06:39:26.977: INFO: Trying to get logs from node my-node pod downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088 container client-container: <nil>
STEP: delete the pod
May 30 06:39:27.011: INFO: Waiting for pod downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088 to disappear
May 30 06:39:27.021: INFO: Pod downwardapi-volume-b8287ca8-ac61-4c09-aad4-5660e42f1088 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:39:27.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2495" for this suite.

• [SLOW TEST:6.147 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":68,"skipped":1060,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:39:27.034: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:39:27.094: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c" in namespace "downward-api-6633" to be "Succeeded or Failed"
May 30 06:39:27.160: INFO: Pod "downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c": Phase="Pending", Reason="", readiness=false. Elapsed: 64.966627ms
May 30 06:39:29.384: INFO: Pod "downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289557687s
May 30 06:39:31.387: INFO: Pod "downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292757736s
May 30 06:39:33.609: INFO: Pod "downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.514649337s
May 30 06:39:35.615: INFO: Pod "downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.519823025s
STEP: Saw pod success
May 30 06:39:35.615: INFO: Pod "downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c" satisfied condition "Succeeded or Failed"
May 30 06:39:35.619: INFO: Trying to get logs from node my-node pod downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c container client-container: <nil>
STEP: delete the pod
May 30 06:39:35.644: INFO: Waiting for pod downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c to disappear
May 30 06:39:35.647: INFO: Pod downwardapi-volume-a7708b62-03d0-4d56-9497-86f68941e63c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:39:35.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6633" for this suite.

• [SLOW TEST:8.624 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":69,"skipped":1072,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:39:35.658: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 30 06:39:35.707: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:39:46.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4621" for this suite.

• [SLOW TEST:10.811 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":303,"completed":70,"skipped":1104,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:39:46.469: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
May 30 06:39:46.516: INFO: Waiting up to 5m0s for pod "client-containers-0bf2939a-5c14-4048-bfda-217cc194e086" in namespace "containers-238" to be "Succeeded or Failed"
May 30 06:39:46.530: INFO: Pod "client-containers-0bf2939a-5c14-4048-bfda-217cc194e086": Phase="Pending", Reason="", readiness=false. Elapsed: 12.286207ms
May 30 06:39:48.534: INFO: Pod "client-containers-0bf2939a-5c14-4048-bfda-217cc194e086": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016206841s
May 30 06:39:50.546: INFO: Pod "client-containers-0bf2939a-5c14-4048-bfda-217cc194e086": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028787735s
May 30 06:39:52.872: INFO: Pod "client-containers-0bf2939a-5c14-4048-bfda-217cc194e086": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.353891359s
STEP: Saw pod success
May 30 06:39:52.872: INFO: Pod "client-containers-0bf2939a-5c14-4048-bfda-217cc194e086" satisfied condition "Succeeded or Failed"
May 30 06:39:52.877: INFO: Trying to get logs from node my-node pod client-containers-0bf2939a-5c14-4048-bfda-217cc194e086 container test-container: <nil>
STEP: delete the pod
May 30 06:39:53.070: INFO: Waiting for pod client-containers-0bf2939a-5c14-4048-bfda-217cc194e086 to disappear
May 30 06:39:53.075: INFO: Pod client-containers-0bf2939a-5c14-4048-bfda-217cc194e086 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:39:53.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-238" for this suite.

• [SLOW TEST:6.620 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":303,"completed":71,"skipped":1106,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:39:53.096: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 30 06:39:53.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-4259 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
May 30 06:39:55.591: INFO: stderr: ""
May 30 06:39:55.597: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
May 30 06:39:55.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-4259 delete pods e2e-test-httpd-pod'
May 30 06:40:03.460: INFO: stderr: ""
May 30 06:40:03.460: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:40:03.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4259" for this suite.

• [SLOW TEST:10.374 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1509
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":303,"completed":72,"skipped":1139,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:40:03.470: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:40:03.539: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3183e01b-d231-430a-bc76-2bed5926af80" in namespace "security-context-test-8569" to be "Succeeded or Failed"
May 30 06:40:03.542: INFO: Pod "busybox-user-65534-3183e01b-d231-430a-bc76-2bed5926af80": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402811ms
May 30 06:40:05.555: INFO: Pod "busybox-user-65534-3183e01b-d231-430a-bc76-2bed5926af80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01637171s
May 30 06:40:07.560: INFO: Pod "busybox-user-65534-3183e01b-d231-430a-bc76-2bed5926af80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021338554s
May 30 06:40:09.892: INFO: Pod "busybox-user-65534-3183e01b-d231-430a-bc76-2bed5926af80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.353094582s
May 30 06:40:09.892: INFO: Pod "busybox-user-65534-3183e01b-d231-430a-bc76-2bed5926af80" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:40:09.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8569" for this suite.

• [SLOW TEST:6.441 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":73,"skipped":1146,"failed":1,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:40:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 30 06:40:10.179: INFO: Waiting up to 1m0s for all nodes to be ready
May 30 06:41:10.203: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 30 06:41:10.253: INFO: Created pod: pod0-sched-preemption-low-priority
May 30 06:41:10.253: FAIL: We need at least two pods to be created butall nodes are already heavily utilized, so preemption tests cannot be run

Full Stack Trace
k8s.io/kubernetes/test/e2e/scheduling.glob..func5.4()
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:253 +0x101e
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc001dde480)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc001dde480)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc001dde480, 0x4df04f8)
	/usr/local/go/src/testing/testing.go:1123 +0xef
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1168 +0x2b3
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "sched-preemption-4325".
STEP: Found 0 events.
May 30 06:41:10.262: INFO: POD                                 NODE  PHASE    GRACE  CONDITIONS
May 30 06:41:10.262: INFO: pod0-sched-preemption-low-priority        Pending         []
May 30 06:41:10.262: INFO: 
May 30 06:41:10.272: INFO: 
Logging node info for node my-node
May 30 06:41:10.276: INFO: Node Info: &Node{ObjectMeta:{my-node   /api/v1/nodes/my-node f75ed23b-2f88-4ce1-8189-6f41b657b75d 47570 0 2022-05-27 09:42:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:my-node kubernetes.io/os:linux node-role.kubernetes.io/master:] map[flannel.alpha.coreos.com/backend-data:{"VNI":1,"VtepMAC":"ee:c9:e8:56:92:a4"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.0.2.15 kubeadm.alpha.kubernetes.io/cri-socket:/var/run/dockershim.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2022-05-27 09:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {kubeadm Update v1 2022-05-27 09:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/master":{}}}}} {flanneld Update v1 2022-05-27 09:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{},"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}}} {kube-controller-manager Update v1 2022-05-27 09:44:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}}} {e2e.test Update v1 2022-05-30 06:41:10 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:scheduling.k8s.io/foo":{}}}}}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{138856275968 0} {<nil>}  BinarySI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{4116410368 0} {<nil>} 4019932Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Allocatable:ResourceList{cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{124970648165 0} {<nil>} 124970648165 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{4011552768 0} {<nil>} 3917532Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-05-30 01:39:18 +0000 UTC,LastTransitionTime:2022-05-30 01:39:18 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-05-30 06:37:45 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-05-30 06:37:45 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-05-30 06:37:45 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-05-30 06:37:45 +0000 UTC,LastTransitionTime:2022-05-27 09:44:25 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.2.15,},NodeAddress{Type:Hostname,Address:my-node,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:30ab1f02f59e4093b9f48497b6753f85,SystemUUID:a3aa5fdb-d52d-9a42-8fa5-7062181ac666,BootID:67aea70c-c63d-41d9-9999-5d2b2003e8de,KernelVersion:5.13.0-44-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:docker://19.3.15,KubeletVersion:v1.19.16,KubeProxyVersion:v1.19.16,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd sonobuoy/systemd-logs:v0.4],SizeBytes:314096343,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:e1ec90b9a30894aa71eaa6cdf7dcaae6f5d899692460b58973cdc2c26dc72afa k8s.gcr.io/conformance:v1.19.16],SizeBytes:229709026,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver@sha256:2cad6a4cae1713e720e00e1a1c7ef7644777fe111e0b7cbed5f50adb8a3cdf30 k8s.gcr.io/kube-apiserver:v1.19.16],SizeBytes:118899101,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0 k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager@sha256:1d42f7d017f4ceeff46afb9cedd47b2b8140d1f2cc3bfebb57a40c3760acd482 k8s.gcr.io/kube-controller-manager:v1.19.16],SizeBytes:110870941,},ContainerImage{Names:[k8s.gcr.io/kube-proxy@sha256:092f9526686d27964d17be772c42cde086690209cc8aea10c49991456eb879c2 k8s.gcr.io/kube-proxy:v1.19.16],SizeBytes:98938510,},ContainerImage{Names:[rancher/mirrored-flannelcni-flannel@sha256:bf56c4e594948eb5c6b3bc04cdde3477d6de12a85b3bbb67ae3a518142cd8392 rancher/mirrored-flannelcni-flannel:v0.18.0],SizeBytes:62262505,},ContainerImage{Names:[k8s.gcr.io/kube-scheduler@sha256:1a335251eaef4e209d5757da0bf5499ecdce6e65413f7cb92ff4cc633d6fc7dd k8s.gcr.io/kube-scheduler:v1.19.16],SizeBytes:46490013,},ContainerImage{Names:[k8s.gcr.io/coredns@sha256:73ca82b4ce829766d4f1f10947c3a338888f876fbed0540dc849c89ff256e90c k8s.gcr.io/coredns:1.7.0],SizeBytes:45227747,},ContainerImage{Names:[sonobuoy/sonobuoy@sha256:ed5bd072698f80e7fa706a69e4b01eeedbddeadc4821587297a29b727b5b5f19 sonobuoy/sonobuoy:v0.56.6],SizeBytes:44096145,},ContainerImage{Names:[nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0],SizeBytes:8087907,},ContainerImage{Names:[busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 busybox:1.29],SizeBytes:1154361,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:682696,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
May 30 06:41:10.276: INFO: 
Logging kubelet events for node my-node
May 30 06:41:10.281: INFO: 
Logging pods the kubelet thinks is on node my-node
May 30 06:41:10.291: INFO: etcd-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container etcd ready: true, restart count 1
May 30 06:41:10.291: INFO: coredns-f9fd979d6-lrzv9 started at 2022-05-27 09:44:25 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container coredns ready: true, restart count 1
May 30 06:41:10.291: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 started at 2022-05-30 06:08:24 +0000 UTC (0+2 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container e2e ready: true, restart count 0
May 30 06:41:10.291: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 06:41:10.291: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 started at 2022-05-30 06:08:25 +0000 UTC (0+2 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 06:41:10.291: INFO: 	Container systemd-logs ready: true, restart count 0
May 30 06:41:10.291: INFO: kube-scheduler-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 06:41:10.291: INFO: kube-proxy-4pwdh started at 2022-05-27 09:42:33 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 06:41:10.291: INFO: kube-flannel-ds-lv5nv started at 2022-05-27 09:43:53 +0000 UTC (2+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Init container install-cni-plugin ready: true, restart count 1
May 30 06:41:10.291: INFO: 	Init container install-cni ready: true, restart count 0
May 30 06:41:10.291: INFO: 	Container kube-flannel ready: true, restart count 1
May 30 06:41:10.291: INFO: kube-apiserver-my-node started at 2022-05-30 01:38:05 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container kube-apiserver ready: true, restart count 1
May 30 06:41:10.291: INFO: coredns-f9fd979d6-c9tcq started at 2022-05-27 09:44:35 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container coredns ready: true, restart count 1
May 30 06:41:10.291: INFO: kube-controller-manager-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 06:41:10.291: INFO: sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (0+1 container statuses recorded)
May 30 06:41:10.291: INFO: 	Container kube-sonobuoy ready: true, restart count 0
W0530 06:41:10.296599      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 06:41:10.398: INFO: 
Latency metrics for node my-node
May 30 06:41:10.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4325" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• Failure [60.549 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance] [It]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  May 30 06:41:10.253: We need at least two pods to be created butall nodes are already heavily utilized, so preemption tests cannot be run

  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:253
------------------------------
{"msg":"FAILED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":303,"completed":73,"skipped":1159,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:41:10.463: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 30 06:41:18.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 30 06:41:18.797: INFO: Pod pod-with-prestop-exec-hook still exists
May 30 06:41:20.798: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 30 06:41:20.803: INFO: Pod pod-with-prestop-exec-hook still exists
May 30 06:41:22.799: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 30 06:41:22.805: INFO: Pod pod-with-prestop-exec-hook still exists
May 30 06:41:24.799: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 30 06:41:24.803: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:41:24.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7081" for this suite.

• [SLOW TEST:14.359 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":303,"completed":74,"skipped":1185,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:41:24.822: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
May 30 06:41:24.893: INFO: Major version: 1
STEP: Confirm minor version
May 30 06:41:24.893: INFO: cleanMinorVersion: 19
May 30 06:41:24.893: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:41:24.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8209" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":303,"completed":75,"skipped":1207,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:41:24.902: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 30 06:41:24.947: INFO: Waiting up to 5m0s for pod "pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb" in namespace "emptydir-8851" to be "Succeeded or Failed"
May 30 06:41:24.960: INFO: Pod "pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.693293ms
May 30 06:41:26.978: INFO: Pod "pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030341558s
May 30 06:41:28.981: INFO: Pod "pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb": Phase="Running", Reason="", readiness=true. Elapsed: 4.033833857s
May 30 06:41:31.067: INFO: Pod "pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.119793563s
STEP: Saw pod success
May 30 06:41:31.071: INFO: Pod "pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb" satisfied condition "Succeeded or Failed"
May 30 06:41:31.078: INFO: Trying to get logs from node my-node pod pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb container test-container: <nil>
STEP: delete the pod
May 30 06:41:31.471: INFO: Waiting for pod pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb to disappear
May 30 06:41:31.489: INFO: Pod pod-7a5a88c9-440a-4001-ad24-80ac4bfb0ccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:41:31.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8851" for this suite.

• [SLOW TEST:6.601 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":76,"skipped":1220,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:41:31.515: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:41:31.598: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Creating first CR 
May 30 06:41:32.169: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-30T06:41:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-30T06:41:32Z]] name:name1 resourceVersion:47704 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:8f1714e2-ab5b-4fae-ba87-738f4968d9e2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 30 06:41:42.174: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-30T06:41:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-30T06:41:42Z]] name:name2 resourceVersion:47742 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3816371f-1298-472c-bdbe-06ebd4c6b783] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 30 06:41:52.182: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-30T06:41:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-30T06:41:52Z]] name:name1 resourceVersion:47765 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:8f1714e2-ab5b-4fae-ba87-738f4968d9e2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 30 06:42:02.190: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-30T06:41:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-30T06:42:02Z]] name:name2 resourceVersion:47788 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3816371f-1298-472c-bdbe-06ebd4c6b783] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 30 06:42:12.199: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-30T06:41:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-30T06:41:52Z]] name:name1 resourceVersion:47811 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:8f1714e2-ab5b-4fae-ba87-738f4968d9e2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 30 06:42:22.211: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-30T06:41:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-30T06:42:02Z]] name:name2 resourceVersion:47834 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:3816371f-1298-472c-bdbe-06ebd4c6b783] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:42:32.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8170" for this suite.

• [SLOW TEST:61.249 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":303,"completed":77,"skipped":1264,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:42:32.763: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:42:50.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9902" for this suite.

• [SLOW TEST:18.127 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":303,"completed":78,"skipped":1317,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:42:50.890: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-ffadbd2e-a6df-4445-870c-5a3017f3bfde in namespace container-probe-2105
May 30 06:42:57.167: INFO: Started pod liveness-ffadbd2e-a6df-4445-870c-5a3017f3bfde in namespace container-probe-2105
STEP: checking the pod's current state and verifying that restartCount is present
May 30 06:42:57.172: INFO: Initial restart count of pod liveness-ffadbd2e-a6df-4445-870c-5a3017f3bfde is 0
May 30 06:43:19.224: INFO: Restart count of pod container-probe-2105/liveness-ffadbd2e-a6df-4445-870c-5a3017f3bfde is now 1 (22.052201646s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:43:19.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2105" for this suite.

• [SLOW TEST:28.371 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":79,"skipped":1353,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:43:19.261: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:44:19.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8921" for this suite.

• [SLOW TEST:60.074 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":303,"completed":80,"skipped":1354,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:44:19.338: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-522b8ac0-821d-468c-a87e-8143cf7a7c9b
STEP: Creating a pod to test consume configMaps
May 30 06:44:19.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d" in namespace "projected-9582" to be "Succeeded or Failed"
May 30 06:44:19.425: INFO: Pod "pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.418097ms
May 30 06:44:21.582: INFO: Pod "pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182846097s
May 30 06:44:23.587: INFO: Pod "pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.188282822s
STEP: Saw pod success
May 30 06:44:23.587: INFO: Pod "pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d" satisfied condition "Succeeded or Failed"
May 30 06:44:23.590: INFO: Trying to get logs from node my-node pod pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 30 06:44:24.112: INFO: Waiting for pod pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d to disappear
May 30 06:44:24.365: INFO: Pod pod-projected-configmaps-f676264e-9fe6-4293-ada2-475db1a1cd0d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:44:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9582" for this suite.

• [SLOW TEST:5.221 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":81,"skipped":1388,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:44:24.560: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 30 06:44:24.622: INFO: Waiting up to 5m0s for pod "pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14" in namespace "emptydir-7261" to be "Succeeded or Failed"
May 30 06:44:24.632: INFO: Pod "pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14": Phase="Pending", Reason="", readiness=false. Elapsed: 9.673855ms
May 30 06:44:26.636: INFO: Pod "pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013731182s
May 30 06:44:28.902: INFO: Pod "pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28014039s
May 30 06:44:30.918: INFO: Pod "pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.296464831s
May 30 06:44:32.928: INFO: Pod "pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.306312402s
STEP: Saw pod success
May 30 06:44:32.929: INFO: Pod "pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14" satisfied condition "Succeeded or Failed"
May 30 06:44:32.937: INFO: Trying to get logs from node my-node pod pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14 container test-container: <nil>
STEP: delete the pod
May 30 06:44:33.222: INFO: Waiting for pod pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14 to disappear
May 30 06:44:33.225: INFO: Pod pod-77bbcb0d-4414-4b92-9b01-9de95f4f1f14 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:44:33.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7261" for this suite.

• [SLOW TEST:8.683 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":82,"skipped":1397,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:44:33.244: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 30 06:44:33.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 create -f -'
May 30 06:44:33.723: INFO: stderr: ""
May 30 06:44:33.724: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 30 06:44:33.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:44:33.991: INFO: stderr: ""
May 30 06:44:33.991: INFO: stdout: "update-demo-nautilus-96w6t update-demo-nautilus-wt5gd "
May 30 06:44:33.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:44:34.662: INFO: stderr: ""
May 30 06:44:34.662: INFO: stdout: ""
May 30 06:44:34.662: INFO: update-demo-nautilus-96w6t is created but not running
May 30 06:44:39.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:44:39.986: INFO: stderr: ""
May 30 06:44:39.986: INFO: stdout: "update-demo-nautilus-96w6t update-demo-nautilus-wt5gd "
May 30 06:44:39.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:44:40.147: INFO: stderr: ""
May 30 06:44:40.147: INFO: stdout: ""
May 30 06:44:40.147: INFO: update-demo-nautilus-96w6t is created but not running
May 30 06:44:45.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:44:45.289: INFO: stderr: ""
May 30 06:44:45.289: INFO: stdout: "update-demo-nautilus-96w6t update-demo-nautilus-wt5gd "
May 30 06:44:45.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:44:45.406: INFO: stderr: ""
May 30 06:44:45.406: INFO: stdout: "true"
May 30 06:44:45.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 30 06:44:45.527: INFO: stderr: ""
May 30 06:44:45.527: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 30 06:44:45.527: INFO: validating pod update-demo-nautilus-96w6t
May 30 06:44:45.532: INFO: got data: {
  "image": "nautilus.jpg"
}

May 30 06:44:45.532: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 30 06:44:45.532: INFO: update-demo-nautilus-96w6t is verified up and running
May 30 06:44:45.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-wt5gd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:44:45.633: INFO: stderr: ""
May 30 06:44:45.634: INFO: stdout: "true"
May 30 06:44:45.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-wt5gd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 30 06:44:45.753: INFO: stderr: ""
May 30 06:44:45.753: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 30 06:44:45.753: INFO: validating pod update-demo-nautilus-wt5gd
May 30 06:44:45.759: INFO: got data: {
  "image": "nautilus.jpg"
}

May 30 06:44:45.759: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 30 06:44:45.759: INFO: update-demo-nautilus-wt5gd is verified up and running
STEP: scaling down the replication controller
May 30 06:44:45.760: INFO: scanned /root for discovery docs: <nil>
May 30 06:44:45.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 30 06:44:46.939: INFO: stderr: ""
May 30 06:44:46.939: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 30 06:44:46.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:44:47.140: INFO: stderr: ""
May 30 06:44:47.140: INFO: stdout: "update-demo-nautilus-96w6t update-demo-nautilus-wt5gd "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 30 06:44:52.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:44:52.236: INFO: stderr: ""
May 30 06:44:52.236: INFO: stdout: "update-demo-nautilus-96w6t update-demo-nautilus-wt5gd "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 30 06:44:57.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:44:57.359: INFO: stderr: ""
May 30 06:44:57.359: INFO: stdout: "update-demo-nautilus-96w6t "
May 30 06:44:57.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:44:57.484: INFO: stderr: ""
May 30 06:44:57.484: INFO: stdout: "true"
May 30 06:44:57.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 30 06:44:57.631: INFO: stderr: ""
May 30 06:44:57.632: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 30 06:44:57.632: INFO: validating pod update-demo-nautilus-96w6t
May 30 06:44:57.635: INFO: got data: {
  "image": "nautilus.jpg"
}

May 30 06:44:57.635: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 30 06:44:57.635: INFO: update-demo-nautilus-96w6t is verified up and running
STEP: scaling up the replication controller
May 30 06:44:57.637: INFO: scanned /root for discovery docs: <nil>
May 30 06:44:57.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 30 06:44:58.799: INFO: stderr: ""
May 30 06:44:58.799: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 30 06:44:58.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:44:59.123: INFO: stderr: ""
May 30 06:44:59.124: INFO: stdout: "update-demo-nautilus-76dtt update-demo-nautilus-96w6t "
May 30 06:44:59.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-76dtt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:44:59.317: INFO: stderr: ""
May 30 06:44:59.317: INFO: stdout: ""
May 30 06:44:59.317: INFO: update-demo-nautilus-76dtt is created but not running
May 30 06:45:04.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 06:45:04.447: INFO: stderr: ""
May 30 06:45:04.448: INFO: stdout: "update-demo-nautilus-76dtt update-demo-nautilus-96w6t "
May 30 06:45:04.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-76dtt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:45:04.564: INFO: stderr: ""
May 30 06:45:04.564: INFO: stdout: "true"
May 30 06:45:04.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-76dtt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 30 06:45:04.683: INFO: stderr: ""
May 30 06:45:04.683: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 30 06:45:04.683: INFO: validating pod update-demo-nautilus-76dtt
May 30 06:45:04.687: INFO: got data: {
  "image": "nautilus.jpg"
}

May 30 06:45:04.687: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 30 06:45:04.687: INFO: update-demo-nautilus-76dtt is verified up and running
May 30 06:45:04.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 06:45:04.802: INFO: stderr: ""
May 30 06:45:04.802: INFO: stdout: "true"
May 30 06:45:04.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods update-demo-nautilus-96w6t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 30 06:45:04.896: INFO: stderr: ""
May 30 06:45:04.896: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 30 06:45:04.896: INFO: validating pod update-demo-nautilus-96w6t
May 30 06:45:04.900: INFO: got data: {
  "image": "nautilus.jpg"
}

May 30 06:45:04.900: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 30 06:45:04.900: INFO: update-demo-nautilus-96w6t is verified up and running
STEP: using delete to clean up resources
May 30 06:45:04.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 delete --grace-period=0 --force -f -'
May 30 06:45:05.045: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 06:45:05.045: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 30 06:45:05.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get rc,svc -l name=update-demo --no-headers'
May 30 06:45:05.232: INFO: stderr: "No resources found in kubectl-7696 namespace.\n"
May 30 06:45:05.232: INFO: stdout: ""
May 30 06:45:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 30 06:45:05.336: INFO: stderr: ""
May 30 06:45:05.336: INFO: stdout: "update-demo-nautilus-76dtt\nupdate-demo-nautilus-96w6t\n"
May 30 06:45:05.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get rc,svc -l name=update-demo --no-headers'
May 30 06:45:05.949: INFO: stderr: "No resources found in kubectl-7696 namespace.\n"
May 30 06:45:05.949: INFO: stdout: ""
May 30 06:45:05.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7696 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 30 06:45:06.334: INFO: stderr: ""
May 30 06:45:06.334: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:06.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7696" for this suite.

• [SLOW TEST:33.101 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":303,"completed":83,"skipped":1401,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:06.345: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:17.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3355" for this suite.

• [SLOW TEST:11.107 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":303,"completed":84,"skipped":1402,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:17.453: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:45:17.499: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 30 06:45:19.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-9349 --namespace=crd-publish-openapi-9349 create -f -'
May 30 06:45:19.909: INFO: stderr: ""
May 30 06:45:19.909: INFO: stdout: "e2e-test-crd-publish-openapi-3001-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 30 06:45:19.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-9349 --namespace=crd-publish-openapi-9349 delete e2e-test-crd-publish-openapi-3001-crds test-cr'
May 30 06:45:20.018: INFO: stderr: ""
May 30 06:45:20.019: INFO: stdout: "e2e-test-crd-publish-openapi-3001-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 30 06:45:20.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-9349 --namespace=crd-publish-openapi-9349 apply -f -'
May 30 06:45:20.296: INFO: stderr: ""
May 30 06:45:20.296: INFO: stdout: "e2e-test-crd-publish-openapi-3001-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 30 06:45:20.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-9349 --namespace=crd-publish-openapi-9349 delete e2e-test-crd-publish-openapi-3001-crds test-cr'
May 30 06:45:20.417: INFO: stderr: ""
May 30 06:45:20.417: INFO: stdout: "e2e-test-crd-publish-openapi-3001-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 30 06:45:20.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-9349 explain e2e-test-crd-publish-openapi-3001-crds'
May 30 06:45:20.670: INFO: stderr: ""
May 30 06:45:20.670: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3001-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:23.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9349" for this suite.

• [SLOW TEST:5.868 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":303,"completed":85,"skipped":1409,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:23.321: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:23.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7570" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":303,"completed":86,"skipped":1452,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:23.500: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-8444
STEP: creating replication controller nodeport-test in namespace services-8444
I0530 06:45:23.589651      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8444, replica count: 2
I0530 06:45:26.643842      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 06:45:29.645: INFO: Creating new exec pod
I0530 06:45:29.645050      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 06:45:36.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8444 exec execpodjk272 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 30 06:45:37.025: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 30 06:45:37.025: INFO: stdout: ""
May 30 06:45:37.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8444 exec execpodjk272 -- /bin/sh -x -c nc -zv -t -w 2 10.105.152.12 80'
May 30 06:45:37.314: INFO: stderr: "+ nc -zv -t -w 2 10.105.152.12 80\nConnection to 10.105.152.12 80 port [tcp/http] succeeded!\n"
May 30 06:45:37.314: INFO: stdout: ""
May 30 06:45:37.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8444 exec execpodjk272 -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.15 30607'
May 30 06:45:37.603: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.15 30607\nConnection to 10.0.2.15 30607 port [tcp/30607] succeeded!\n"
May 30 06:45:37.604: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:37.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8444" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.121 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":303,"completed":87,"skipped":1461,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:37.623: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-431ba338-54c1-4bc4-8963-3a95d5e8db5c
STEP: Creating a pod to test consume configMaps
May 30 06:45:37.671: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e" in namespace "projected-2873" to be "Succeeded or Failed"
May 30 06:45:37.686: INFO: Pod "pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.691866ms
May 30 06:45:39.958: INFO: Pod "pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286448305s
May 30 06:45:42.219: INFO: Pod "pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.547302894s
STEP: Saw pod success
May 30 06:45:42.219: INFO: Pod "pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e" satisfied condition "Succeeded or Failed"
May 30 06:45:42.225: INFO: Trying to get logs from node my-node pod pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 30 06:45:42.257: INFO: Waiting for pod pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e to disappear
May 30 06:45:42.262: INFO: Pod pod-projected-configmaps-dba700f0-4646-4125-915e-0a69fd5df76e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:42.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2873" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":88,"skipped":1489,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:42.275: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 30 06:45:42.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9020 create -f -'
May 30 06:45:42.788: INFO: stderr: ""
May 30 06:45:42.789: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 30 06:45:43.867: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:43.867: INFO: Found 0 / 1
May 30 06:45:44.867: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:44.867: INFO: Found 0 / 1
May 30 06:45:45.867: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:45.867: INFO: Found 0 / 1
May 30 06:45:47.154: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:47.154: INFO: Found 0 / 1
May 30 06:45:47.873: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:47.873: INFO: Found 0 / 1
May 30 06:45:48.868: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:48.868: INFO: Found 0 / 1
May 30 06:45:49.871: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:49.871: INFO: Found 0 / 1
May 30 06:45:50.867: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:50.867: INFO: Found 1 / 1
May 30 06:45:50.867: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 30 06:45:50.871: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:50.871: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 30 06:45:50.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9020 patch pod agnhost-primary-4659t -p {"metadata":{"annotations":{"x":"y"}}}'
May 30 06:45:50.992: INFO: stderr: ""
May 30 06:45:50.992: INFO: stdout: "pod/agnhost-primary-4659t patched\n"
STEP: checking annotations
May 30 06:45:50.995: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:45:50.995: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:50.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9020" for this suite.

• [SLOW TEST:8.733 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":303,"completed":89,"skipped":1541,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:51.010: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:45:51.696: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 06:45:54.009: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:45:56.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789489951, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:45:59.245: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:45:59.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9632" for this suite.
STEP: Destroying namespace "webhook-9632-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.656 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":303,"completed":90,"skipped":1548,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:45:59.671: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-b3e68394-3dcd-44bb-87ec-407dd5bc3cd7
STEP: Creating a pod to test consume configMaps
May 30 06:45:59.736: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16" in namespace "projected-6552" to be "Succeeded or Failed"
May 30 06:45:59.747: INFO: Pod "pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16": Phase="Pending", Reason="", readiness=false. Elapsed: 10.705611ms
May 30 06:46:01.772: INFO: Pod "pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035595816s
May 30 06:46:03.778: INFO: Pod "pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041489122s
May 30 06:46:05.975: INFO: Pod "pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.23898528s
STEP: Saw pod success
May 30 06:46:05.975: INFO: Pod "pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16" satisfied condition "Succeeded or Failed"
May 30 06:46:05.983: INFO: Trying to get logs from node my-node pod pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 30 06:46:06.020: INFO: Waiting for pod pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16 to disappear
May 30 06:46:06.027: INFO: Pod pod-projected-configmaps-0e67a728-14db-4317-9408-3b1c022edf16 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:06.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6552" for this suite.

• [SLOW TEST:6.363 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":91,"skipped":1551,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-3a4404c1-3603-4ee5-84c7-8041e16ed42d
STEP: Creating a pod to test consume secrets
May 30 06:46:06.088: INFO: Waiting up to 5m0s for pod "pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60" in namespace "secrets-9279" to be "Succeeded or Failed"
May 30 06:46:06.106: INFO: Pod "pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60": Phase="Pending", Reason="", readiness=false. Elapsed: 17.298868ms
May 30 06:46:08.117: INFO: Pod "pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028174075s
May 30 06:46:10.127: INFO: Pod "pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038734834s
May 30 06:46:12.402: INFO: Pod "pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.313076126s
STEP: Saw pod success
May 30 06:46:12.402: INFO: Pod "pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60" satisfied condition "Succeeded or Failed"
May 30 06:46:12.406: INFO: Trying to get logs from node my-node pod pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60 container secret-volume-test: <nil>
STEP: delete the pod
May 30 06:46:12.824: INFO: Waiting for pod pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60 to disappear
May 30 06:46:12.829: INFO: Pod pod-secrets-b87d9339-17f0-4c1a-8ae8-e5835473ae60 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:12.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9279" for this suite.

• [SLOW TEST:6.808 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":92,"skipped":1566,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:12.866: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:12.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4485" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":303,"completed":93,"skipped":1591,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:12.964: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 30 06:46:13.031: INFO: Waiting up to 5m0s for pod "pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156" in namespace "emptydir-3627" to be "Succeeded or Failed"
May 30 06:46:13.033: INFO: Pod "pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095652ms
May 30 06:46:15.041: INFO: Pod "pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009459989s
May 30 06:46:17.044: INFO: Pod "pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013043756s
May 30 06:46:19.330: INFO: Pod "pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.298967558s
STEP: Saw pod success
May 30 06:46:19.333: INFO: Pod "pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156" satisfied condition "Succeeded or Failed"
May 30 06:46:19.482: INFO: Trying to get logs from node my-node pod pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156 container test-container: <nil>
STEP: delete the pod
May 30 06:46:19.571: INFO: Waiting for pod pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156 to disappear
May 30 06:46:19.577: INFO: Pod pod-5d2f2db8-35c4-409c-ad40-4cac06c5d156 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:19.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3627" for this suite.

• [SLOW TEST:6.623 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":94,"skipped":1602,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:19.588: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-65001c74-9fbd-4b46-a41c-2565bb61cb51
STEP: Creating a pod to test consume configMaps
May 30 06:46:19.635: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258" in namespace "projected-2749" to be "Succeeded or Failed"
May 30 06:46:19.646: INFO: Pod "pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258": Phase="Pending", Reason="", readiness=false. Elapsed: 9.852365ms
May 30 06:46:22.046: INFO: Pod "pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409371214s
May 30 06:46:24.053: INFO: Pod "pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.4167068s
STEP: Saw pod success
May 30 06:46:24.053: INFO: Pod "pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258" satisfied condition "Succeeded or Failed"
May 30 06:46:24.056: INFO: Trying to get logs from node my-node pod pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 30 06:46:24.118: INFO: Waiting for pod pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258 to disappear
May 30 06:46:24.125: INFO: Pod pod-projected-configmaps-adeb95d2-b5c9-40fd-a53a-866c5489d258 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:24.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2749" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":95,"skipped":1604,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:46:24.498: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4" in namespace "downward-api-9637" to be "Succeeded or Failed"
May 30 06:46:25.257: INFO: Pod "downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4": Phase="Pending", Reason="", readiness=false. Elapsed: 759.059113ms
May 30 06:46:27.261: INFO: Pod "downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763226146s
May 30 06:46:29.278: INFO: Pod "downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.779549041s
STEP: Saw pod success
May 30 06:46:29.278: INFO: Pod "downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4" satisfied condition "Succeeded or Failed"
May 30 06:46:29.283: INFO: Trying to get logs from node my-node pod downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4 container client-container: <nil>
STEP: delete the pod
May 30 06:46:29.334: INFO: Waiting for pod downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4 to disappear
May 30 06:46:29.354: INFO: Pod downwardapi-volume-c6cc8a4d-db48-4c7a-9e87-7cb3807769a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:29.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9637" for this suite.

• [SLOW TEST:5.233 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":96,"skipped":1616,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:29.377: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:46:29.413: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:35.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2389" for this suite.

• [SLOW TEST:6.637 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":303,"completed":97,"skipped":1646,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:36.014: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
May 30 06:46:36.057: INFO: Waiting up to 5m0s for pod "pod-ba20edc5-2c14-426c-8967-57a5b6873b90" in namespace "emptydir-3079" to be "Succeeded or Failed"
May 30 06:46:36.075: INFO: Pod "pod-ba20edc5-2c14-426c-8967-57a5b6873b90": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892419ms
May 30 06:46:38.253: INFO: Pod "pod-ba20edc5-2c14-426c-8967-57a5b6873b90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184167031s
May 30 06:46:40.258: INFO: Pod "pod-ba20edc5-2c14-426c-8967-57a5b6873b90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188915607s
May 30 06:46:42.264: INFO: Pod "pod-ba20edc5-2c14-426c-8967-57a5b6873b90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.195443138s
STEP: Saw pod success
May 30 06:46:42.264: INFO: Pod "pod-ba20edc5-2c14-426c-8967-57a5b6873b90" satisfied condition "Succeeded or Failed"
May 30 06:46:42.267: INFO: Trying to get logs from node my-node pod pod-ba20edc5-2c14-426c-8967-57a5b6873b90 container test-container: <nil>
STEP: delete the pod
May 30 06:46:42.321: INFO: Waiting for pod pod-ba20edc5-2c14-426c-8967-57a5b6873b90 to disappear
May 30 06:46:42.327: INFO: Pod pod-ba20edc5-2c14-426c-8967-57a5b6873b90 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:42.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3079" for this suite.

• [SLOW TEST:6.324 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":98,"skipped":1658,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:42.338: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:46:42.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 06:46:45.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:46:47.031: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490002, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:46:50.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:46:50.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-687" for this suite.
STEP: Destroying namespace "webhook-687-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.903 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":303,"completed":99,"skipped":1660,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:46:50.264: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:01.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1876" for this suite.

• [SLOW TEST:11.185 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":303,"completed":100,"skipped":1666,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:01.453: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-709" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":303,"completed":101,"skipped":1716,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:01.564: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 30 06:47:01.602: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 30 06:47:01.609: INFO: Waiting for terminating namespaces to be deleted...
May 30 06:47:01.613: INFO: 
Logging pods the apiserver thinks is on node my-node before test
May 30 06:47:01.617: INFO: coredns-f9fd979d6-c9tcq from kube-system started at 2022-05-27 09:44:35 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.617: INFO: 	Container coredns ready: true, restart count 1
May 30 06:47:01.617: INFO: coredns-f9fd979d6-lrzv9 from kube-system started at 2022-05-27 09:44:25 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.617: INFO: 	Container coredns ready: true, restart count 1
May 30 06:47:01.617: INFO: etcd-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.617: INFO: 	Container etcd ready: true, restart count 1
May 30 06:47:01.617: INFO: kube-apiserver-my-node from kube-system started at 2022-05-30 01:38:05 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.617: INFO: 	Container kube-apiserver ready: true, restart count 1
May 30 06:47:01.617: INFO: kube-controller-manager-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.618: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 06:47:01.618: INFO: kube-flannel-ds-lv5nv from kube-system started at 2022-05-27 09:43:53 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.618: INFO: 	Container kube-flannel ready: true, restart count 1
May 30 06:47:01.618: INFO: kube-proxy-4pwdh from kube-system started at 2022-05-27 09:42:33 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.618: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 06:47:01.618: INFO: kube-scheduler-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.618: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 06:47:01.618: INFO: sonobuoy from sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (1 container statuses recorded)
May 30 06:47:01.618: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 30 06:47:01.619: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 from sonobuoy started at 2022-05-30 06:08:24 +0000 UTC (2 container statuses recorded)
May 30 06:47:01.619: INFO: 	Container e2e ready: true, restart count 0
May 30 06:47:01.619: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 06:47:01.619: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 from sonobuoy started at 2022-05-30 06:08:25 +0000 UTC (2 container statuses recorded)
May 30 06:47:01.619: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 06:47:01.619: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16f3cf1f55b460b7], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:02.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2894" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":303,"completed":102,"skipped":1720,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:02.694: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 30 06:47:16.849: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:16.849: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:17.037: INFO: Exec stderr: ""
May 30 06:47:17.037: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:17.037: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:17.198: INFO: Exec stderr: ""
May 30 06:47:17.198: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:17.198: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:17.427: INFO: Exec stderr: ""
May 30 06:47:17.427: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:17.427: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:17.581: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 30 06:47:17.582: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:17.582: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:17.776: INFO: Exec stderr: ""
May 30 06:47:17.776: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:17.776: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:17.924: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 30 06:47:17.924: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:18.096: INFO: Exec stderr: ""
May 30 06:47:18.097: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:18.097: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:18.287: INFO: Exec stderr: ""
May 30 06:47:18.287: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:18.287: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:18.443: INFO: Exec stderr: ""
May 30 06:47:18.444: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:47:18.444: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:47:18.590: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:18.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3957" for this suite.

• [SLOW TEST:15.917 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":103,"skipped":1733,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:18.612: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:47:18.650: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:19.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8548" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":303,"completed":104,"skipped":1759,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:19.782: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:25.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7192" for this suite.
STEP: Destroying namespace "nsdeletetest-4941" for this suite.
May 30 06:47:25.948: INFO: Namespace nsdeletetest-4941 was already deleted
STEP: Destroying namespace "nsdeletetest-680" for this suite.

• [SLOW TEST:6.176 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":303,"completed":105,"skipped":1772,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:25.957: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-2e410506-a98c-462d-b1f0-894ed64ec893
STEP: Creating secret with name s-test-opt-upd-2bb8c389-f993-49e1-8c1b-6b208ec25482
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2e410506-a98c-462d-b1f0-894ed64ec893
STEP: Updating secret s-test-opt-upd-2bb8c389-f993-49e1-8c1b-6b208ec25482
STEP: Creating secret with name s-test-opt-create-8979d1d7-9856-4019-a8f1-d0a27c35fc3b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:38.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1823" for this suite.

• [SLOW TEST:12.274 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":106,"skipped":1778,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:38.232: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:38.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-324" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":303,"completed":107,"skipped":1785,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 30 06:47:47.928: INFO: Successfully updated pod "labelsupdatee277d716-8477-45bd-a696-a5d548838e18"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:50.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-406" for this suite.

• [SLOW TEST:11.992 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":108,"skipped":1803,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:50.399: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-7377d1e0-7ff4-49de-bd9d-75d9cd266429
STEP: Creating a pod to test consume configMaps
May 30 06:47:50.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8" in namespace "configmap-7475" to be "Succeeded or Failed"
May 30 06:47:50.494: INFO: Pod "pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.872668ms
May 30 06:47:52.502: INFO: Pod "pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021157724s
May 30 06:47:54.512: INFO: Pod "pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031182354s
May 30 06:47:56.528: INFO: Pod "pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046978673s
STEP: Saw pod success
May 30 06:47:56.528: INFO: Pod "pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8" satisfied condition "Succeeded or Failed"
May 30 06:47:56.534: INFO: Trying to get logs from node my-node pod pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8 container configmap-volume-test: <nil>
STEP: delete the pod
May 30 06:47:56.854: INFO: Waiting for pod pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8 to disappear
May 30 06:47:57.386: INFO: Pod pod-configmaps-36240f22-f4d9-4eb1-9fe9-043227e75eb8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:47:57.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7475" for this suite.

• [SLOW TEST:7.011 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":109,"skipped":1819,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:47:57.410: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:47:59.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72" in namespace "projected-2006" to be "Succeeded or Failed"
May 30 06:47:59.714: INFO: Pod "downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72": Phase="Pending", Reason="", readiness=false. Elapsed: 214.445043ms
May 30 06:48:01.769: INFO: Pod "downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269473955s
May 30 06:48:03.777: INFO: Pod "downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27709236s
May 30 06:48:06.269: INFO: Pod "downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72": Phase="Pending", Reason="", readiness=false. Elapsed: 6.769323126s
May 30 06:48:08.274: INFO: Pod "downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.773608985s
STEP: Saw pod success
May 30 06:48:08.275: INFO: Pod "downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72" satisfied condition "Succeeded or Failed"
May 30 06:48:08.279: INFO: Trying to get logs from node my-node pod downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72 container client-container: <nil>
STEP: delete the pod
May 30 06:48:08.303: INFO: Waiting for pod downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72 to disappear
May 30 06:48:08.308: INFO: Pod downwardapi-volume-b1e6dfef-e5f4-4ade-8c89-f15c1c145f72 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:48:08.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2006" for this suite.

• [SLOW TEST:10.906 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":303,"completed":110,"skipped":1855,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:48:08.322: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 30 06:48:08.364: INFO: Waiting up to 5m0s for pod "downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903" in namespace "downward-api-5266" to be "Succeeded or Failed"
May 30 06:48:08.416: INFO: Pod "downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903": Phase="Pending", Reason="", readiness=false. Elapsed: 52.131085ms
May 30 06:48:10.420: INFO: Pod "downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05651417s
May 30 06:48:12.426: INFO: Pod "downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062644348s
May 30 06:48:14.440: INFO: Pod "downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076538715s
STEP: Saw pod success
May 30 06:48:14.441: INFO: Pod "downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903" satisfied condition "Succeeded or Failed"
May 30 06:48:14.443: INFO: Trying to get logs from node my-node pod downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903 container dapi-container: <nil>
STEP: delete the pod
May 30 06:48:14.477: INFO: Waiting for pod downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903 to disappear
May 30 06:48:14.479: INFO: Pod downward-api-fb5c2a89-7fb3-4821-ba14-8ae20b131903 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:48:14.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5266" for this suite.

• [SLOW TEST:6.167 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":303,"completed":111,"skipped":1909,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:48:14.489: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:48:14.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e" in namespace "downward-api-935" to be "Succeeded or Failed"
May 30 06:48:14.559: INFO: Pod "downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.160468ms
May 30 06:48:16.562: INFO: Pod "downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027892329s
May 30 06:48:18.590: INFO: Pod "downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055024022s
May 30 06:48:20.596: INFO: Pod "downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061177256s
STEP: Saw pod success
May 30 06:48:20.596: INFO: Pod "downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e" satisfied condition "Succeeded or Failed"
May 30 06:48:20.600: INFO: Trying to get logs from node my-node pod downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e container client-container: <nil>
STEP: delete the pod
May 30 06:48:20.628: INFO: Waiting for pod downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e to disappear
May 30 06:48:20.634: INFO: Pod downwardapi-volume-d1fa60f5-948d-4750-a7e5-2d2b0b58296e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:48:20.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-935" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":112,"skipped":1910,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:48:20.647: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
May 30 06:48:20.689: INFO: Waiting up to 5m0s for pod "pod-302e24e6-759f-4265-9129-8f37f623f6fd" in namespace "emptydir-8479" to be "Succeeded or Failed"
May 30 06:48:20.700: INFO: Pod "pod-302e24e6-759f-4265-9129-8f37f623f6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.625354ms
May 30 06:48:22.717: INFO: Pod "pod-302e24e6-759f-4265-9129-8f37f623f6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027712355s
May 30 06:48:24.722: INFO: Pod "pod-302e24e6-759f-4265-9129-8f37f623f6fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032959812s
STEP: Saw pod success
May 30 06:48:24.722: INFO: Pod "pod-302e24e6-759f-4265-9129-8f37f623f6fd" satisfied condition "Succeeded or Failed"
May 30 06:48:24.726: INFO: Trying to get logs from node my-node pod pod-302e24e6-759f-4265-9129-8f37f623f6fd container test-container: <nil>
STEP: delete the pod
May 30 06:48:24.767: INFO: Waiting for pod pod-302e24e6-759f-4265-9129-8f37f623f6fd to disappear
May 30 06:48:24.772: INFO: Pod pod-302e24e6-759f-4265-9129-8f37f623f6fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:48:24.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8479" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":113,"skipped":1922,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:48:24.785: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-82a74ab7-8040-45e3-bf4e-022bf5663b15 in namespace container-probe-165
May 30 06:48:30.899: INFO: Started pod liveness-82a74ab7-8040-45e3-bf4e-022bf5663b15 in namespace container-probe-165
STEP: checking the pod's current state and verifying that restartCount is present
May 30 06:48:30.902: INFO: Initial restart count of pod liveness-82a74ab7-8040-45e3-bf4e-022bf5663b15 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:52:32.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-165" for this suite.

• [SLOW TEST:247.398 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":303,"completed":114,"skipped":1954,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:52:32.195: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 30 06:52:37.657: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:52:38.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8059" for this suite.

• [SLOW TEST:5.994 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":303,"completed":115,"skipped":1971,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:52:38.189: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 30 06:52:46.392: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 30 06:52:46.400: INFO: Pod pod-with-poststart-http-hook still exists
May 30 06:52:48.401: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 30 06:52:48.405: INFO: Pod pod-with-poststart-http-hook still exists
May 30 06:52:50.401: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 30 06:52:50.408: INFO: Pod pod-with-poststart-http-hook still exists
May 30 06:52:52.402: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 30 06:52:52.406: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:52:52.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9125" for this suite.

• [SLOW TEST:14.229 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":303,"completed":116,"skipped":2002,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:52:52.430: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 30 06:52:52.518: INFO: Pod name pod-release: Found 0 pods out of 1
May 30 06:52:57.522: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:52:58.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-298" for this suite.

• [SLOW TEST:6.495 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":303,"completed":117,"skipped":2053,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:52:58.946: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:53:15.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1350" for this suite.

• [SLOW TEST:16.262 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":303,"completed":118,"skipped":2070,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:53:15.208: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 30 06:53:15.275: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:53:17.962: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:53:27.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6708" for this suite.

• [SLOW TEST:12.097 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":303,"completed":119,"skipped":2072,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:53:27.347: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
May 30 06:53:31.453: INFO: Pod pod-hostip-512c4698-28be-4785-8d21-6146b87603d9 has hostIP: 10.0.2.15
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:53:31.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1762" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":303,"completed":120,"skipped":2165,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:53:31.474: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-twmf
STEP: Creating a pod to test atomic-volume-subpath
May 30 06:53:31.533: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-twmf" in namespace "subpath-6676" to be "Succeeded or Failed"
May 30 06:53:31.548: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.877993ms
May 30 06:53:33.555: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022654871s
May 30 06:53:35.562: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 4.029153815s
May 30 06:53:37.566: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 6.033252533s
May 30 06:53:39.569: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 8.03666634s
May 30 06:53:41.573: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 10.04038632s
May 30 06:53:43.578: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 12.044936395s
May 30 06:53:45.585: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 14.052039338s
May 30 06:53:47.590: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 16.057377069s
May 30 06:53:49.600: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 18.066762342s
May 30 06:53:51.609: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 20.076031103s
May 30 06:53:53.625: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 22.09263867s
May 30 06:53:55.628: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Running", Reason="", readiness=true. Elapsed: 24.095379681s
May 30 06:53:57.634: INFO: Pod "pod-subpath-test-projected-twmf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.101116852s
STEP: Saw pod success
May 30 06:53:57.634: INFO: Pod "pod-subpath-test-projected-twmf" satisfied condition "Succeeded or Failed"
May 30 06:53:57.638: INFO: Trying to get logs from node my-node pod pod-subpath-test-projected-twmf container test-container-subpath-projected-twmf: <nil>
STEP: delete the pod
May 30 06:53:57.665: INFO: Waiting for pod pod-subpath-test-projected-twmf to disappear
May 30 06:53:57.668: INFO: Pod pod-subpath-test-projected-twmf no longer exists
STEP: Deleting pod pod-subpath-test-projected-twmf
May 30 06:53:57.669: INFO: Deleting pod "pod-subpath-test-projected-twmf" in namespace "subpath-6676"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:53:57.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6676" for this suite.

• [SLOW TEST:26.211 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":303,"completed":121,"skipped":2165,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:53:57.684: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-335/configmap-test-47f4b30c-b675-4d56-a5dd-e607fb2099d6
STEP: Creating a pod to test consume configMaps
May 30 06:53:57.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11" in namespace "configmap-335" to be "Succeeded or Failed"
May 30 06:53:57.783: INFO: Pod "pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11": Phase="Pending", Reason="", readiness=false. Elapsed: 50.580191ms
May 30 06:53:59.787: INFO: Pod "pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054107032s
May 30 06:54:01.805: INFO: Pod "pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071990177s
STEP: Saw pod success
May 30 06:54:01.805: INFO: Pod "pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11" satisfied condition "Succeeded or Failed"
May 30 06:54:01.808: INFO: Trying to get logs from node my-node pod pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11 container env-test: <nil>
STEP: delete the pod
May 30 06:54:02.304: INFO: Waiting for pod pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11 to disappear
May 30 06:54:02.480: INFO: Pod pod-configmaps-54e144d3-218d-4524-8d41-83542dd30c11 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:54:02.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-335" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":122,"skipped":2168,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:54:02.500: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 30 06:54:02.786: INFO: Number of nodes with available pods: 0
May 30 06:54:02.786: INFO: Node my-node is running more than one daemon pod
May 30 06:54:04.188: INFO: Number of nodes with available pods: 0
May 30 06:54:04.188: INFO: Node my-node is running more than one daemon pod
May 30 06:54:04.812: INFO: Number of nodes with available pods: 0
May 30 06:54:04.812: INFO: Node my-node is running more than one daemon pod
May 30 06:54:05.829: INFO: Number of nodes with available pods: 0
May 30 06:54:05.829: INFO: Node my-node is running more than one daemon pod
May 30 06:54:06.795: INFO: Number of nodes with available pods: 0
May 30 06:54:06.795: INFO: Node my-node is running more than one daemon pod
May 30 06:54:07.799: INFO: Number of nodes with available pods: 1
May 30 06:54:07.799: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 30 06:54:07.830: INFO: Number of nodes with available pods: 0
May 30 06:54:07.830: INFO: Node my-node is running more than one daemon pod
May 30 06:54:08.839: INFO: Number of nodes with available pods: 0
May 30 06:54:08.839: INFO: Node my-node is running more than one daemon pod
May 30 06:54:10.815: INFO: Number of nodes with available pods: 0
May 30 06:54:10.815: INFO: Node my-node is running more than one daemon pod
May 30 06:54:10.843: INFO: Number of nodes with available pods: 0
May 30 06:54:10.844: INFO: Node my-node is running more than one daemon pod
May 30 06:54:11.838: INFO: Number of nodes with available pods: 0
May 30 06:54:11.838: INFO: Node my-node is running more than one daemon pod
May 30 06:54:12.840: INFO: Number of nodes with available pods: 0
May 30 06:54:12.840: INFO: Node my-node is running more than one daemon pod
May 30 06:54:13.855: INFO: Number of nodes with available pods: 0
May 30 06:54:13.855: INFO: Node my-node is running more than one daemon pod
May 30 06:54:14.840: INFO: Number of nodes with available pods: 0
May 30 06:54:14.840: INFO: Node my-node is running more than one daemon pod
May 30 06:54:15.840: INFO: Number of nodes with available pods: 0
May 30 06:54:15.840: INFO: Node my-node is running more than one daemon pod
May 30 06:54:16.851: INFO: Number of nodes with available pods: 0
May 30 06:54:16.851: INFO: Node my-node is running more than one daemon pod
May 30 06:54:17.836: INFO: Number of nodes with available pods: 1
May 30 06:54:17.837: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6108, will wait for the garbage collector to delete the pods
May 30 06:54:17.901: INFO: Deleting DaemonSet.extensions daemon-set took: 7.350559ms
May 30 06:54:18.302: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.828295ms
May 30 06:54:24.005: INFO: Number of nodes with available pods: 0
May 30 06:54:24.005: INFO: Number of running nodes: 0, number of available pods: 0
May 30 06:54:24.008: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6108/daemonsets","resourceVersion":"50983"},"items":null}

May 30 06:54:24.011: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6108/pods","resourceVersion":"50983"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:54:24.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6108" for this suite.

• [SLOW TEST:21.533 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":303,"completed":123,"skipped":2188,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:54:24.041: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 30 06:54:24.073: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
May 30 06:54:24.688: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 30 06:54:27.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:54:29.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:54:31.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:54:33.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:54:35.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:54:37.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490464, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:54:40.421: INFO: Waited 1.223319678s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:54:41.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3565" for this suite.

• [SLOW TEST:17.086 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":303,"completed":124,"skipped":2195,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:54:41.151: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
May 30 06:54:42.254: INFO: namespace kubectl-8230
May 30 06:54:42.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-8230 create -f -'
May 30 06:54:42.866: INFO: stderr: ""
May 30 06:54:42.867: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 30 06:54:43.871: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:54:43.871: INFO: Found 0 / 1
May 30 06:54:45.184: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:54:45.184: INFO: Found 0 / 1
May 30 06:54:45.870: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:54:45.870: INFO: Found 0 / 1
May 30 06:54:46.966: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:54:46.966: INFO: Found 0 / 1
May 30 06:54:47.870: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:54:47.870: INFO: Found 0 / 1
May 30 06:54:48.876: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:54:48.876: INFO: Found 1 / 1
May 30 06:54:48.876: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 30 06:54:48.881: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 06:54:48.881: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 30 06:54:48.881: INFO: wait on agnhost-primary startup in kubectl-8230 
May 30 06:54:48.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-8230 logs agnhost-primary-25xqc agnhost-primary'
May 30 06:54:48.998: INFO: stderr: ""
May 30 06:54:48.999: INFO: stdout: "Paused\n"
STEP: exposing RC
May 30 06:54:49.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-8230 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 30 06:54:49.155: INFO: stderr: ""
May 30 06:54:49.155: INFO: stdout: "service/rm2 exposed\n"
May 30 06:54:49.168: INFO: Service rm2 in namespace kubectl-8230 found.
STEP: exposing service
May 30 06:54:51.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-8230 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 30 06:54:51.348: INFO: stderr: ""
May 30 06:54:51.348: INFO: stdout: "service/rm3 exposed\n"
May 30 06:54:51.351: INFO: Service rm3 in namespace kubectl-8230 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:54:53.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8230" for this suite.

• [SLOW TEST:12.228 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":303,"completed":125,"skipped":2211,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:54:53.380: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-351
STEP: creating service affinity-nodeport in namespace services-351
STEP: creating replication controller affinity-nodeport in namespace services-351
I0530 06:54:53.581745      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-351, replica count: 3
I0530 06:54:56.633966      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:54:59.635076      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 06:55:02.638372      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 06:55:02.667: INFO: Creating new exec pod
May 30 06:55:07.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-351 exec execpod-affinity46c84 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 30 06:55:08.124: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 30 06:55:08.124: INFO: stdout: ""
May 30 06:55:08.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-351 exec execpod-affinity46c84 -- /bin/sh -x -c nc -zv -t -w 2 10.99.25.123 80'
May 30 06:55:08.419: INFO: stderr: "+ nc -zv -t -w 2 10.99.25.123 80\nConnection to 10.99.25.123 80 port [tcp/http] succeeded!\n"
May 30 06:55:08.419: INFO: stdout: ""
May 30 06:55:08.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-351 exec execpod-affinity46c84 -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.15 30220'
May 30 06:55:08.740: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.15 30220\nConnection to 10.0.2.15 30220 port [tcp/30220] succeeded!\n"
May 30 06:55:08.740: INFO: stdout: ""
May 30 06:55:08.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-351 exec execpod-affinity46c84 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.2.15:30220/ ; done'
May 30 06:55:09.243: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30220/\n"
May 30 06:55:09.245: INFO: stdout: "\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4\naffinity-nodeport-nbtp4"
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Received response from host: affinity-nodeport-nbtp4
May 30 06:55:09.245: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-351, will wait for the garbage collector to delete the pods
May 30 06:55:09.357: INFO: Deleting ReplicationController affinity-nodeport took: 21.185138ms
May 30 06:55:09.468: INFO: Terminating ReplicationController affinity-nodeport pods took: 110.64017ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:55:25.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-351" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:32.181 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":126,"skipped":2219,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:55:25.561: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:55:25.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d" in namespace "projected-8229" to be "Succeeded or Failed"
May 30 06:55:25.643: INFO: Pod "downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.531507ms
May 30 06:55:27.660: INFO: Pod "downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040997961s
May 30 06:55:29.799: INFO: Pod "downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.17921174s
May 30 06:55:31.804: INFO: Pod "downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.184692258s
STEP: Saw pod success
May 30 06:55:31.804: INFO: Pod "downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d" satisfied condition "Succeeded or Failed"
May 30 06:55:31.808: INFO: Trying to get logs from node my-node pod downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d container client-container: <nil>
STEP: delete the pod
May 30 06:55:31.843: INFO: Waiting for pod downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d to disappear
May 30 06:55:31.856: INFO: Pod downwardapi-volume-d615f6db-b601-4975-8503-a1fcedc59a6d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:55:31.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8229" for this suite.

• [SLOW TEST:6.315 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":303,"completed":127,"skipped":2263,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:55:31.877: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 30 06:55:32.046: INFO: Number of nodes with available pods: 0
May 30 06:55:32.047: INFO: Node my-node is running more than one daemon pod
May 30 06:55:33.053: INFO: Number of nodes with available pods: 0
May 30 06:55:33.053: INFO: Node my-node is running more than one daemon pod
May 30 06:55:34.098: INFO: Number of nodes with available pods: 0
May 30 06:55:34.098: INFO: Node my-node is running more than one daemon pod
May 30 06:55:35.064: INFO: Number of nodes with available pods: 0
May 30 06:55:35.064: INFO: Node my-node is running more than one daemon pod
May 30 06:55:36.055: INFO: Number of nodes with available pods: 0
May 30 06:55:36.055: INFO: Node my-node is running more than one daemon pod
May 30 06:55:37.054: INFO: Number of nodes with available pods: 1
May 30 06:55:37.055: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 30 06:55:37.121: INFO: Number of nodes with available pods: 0
May 30 06:55:37.121: INFO: Node my-node is running more than one daemon pod
May 30 06:55:38.173: INFO: Number of nodes with available pods: 0
May 30 06:55:38.183: INFO: Node my-node is running more than one daemon pod
May 30 06:55:39.139: INFO: Number of nodes with available pods: 0
May 30 06:55:39.140: INFO: Node my-node is running more than one daemon pod
May 30 06:55:40.169: INFO: Number of nodes with available pods: 0
May 30 06:55:40.169: INFO: Node my-node is running more than one daemon pod
May 30 06:55:41.130: INFO: Number of nodes with available pods: 0
May 30 06:55:41.130: INFO: Node my-node is running more than one daemon pod
May 30 06:55:42.318: INFO: Number of nodes with available pods: 0
May 30 06:55:42.319: INFO: Node my-node is running more than one daemon pod
May 30 06:55:43.209: INFO: Number of nodes with available pods: 0
May 30 06:55:43.209: INFO: Node my-node is running more than one daemon pod
May 30 06:55:44.159: INFO: Number of nodes with available pods: 0
May 30 06:55:44.159: INFO: Node my-node is running more than one daemon pod
May 30 06:55:45.132: INFO: Number of nodes with available pods: 1
May 30 06:55:45.132: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5491, will wait for the garbage collector to delete the pods
May 30 06:55:45.203: INFO: Deleting DaemonSet.extensions daemon-set took: 7.930088ms
May 30 06:55:45.304: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.237019ms
May 30 06:55:55.409: INFO: Number of nodes with available pods: 0
May 30 06:55:55.410: INFO: Number of running nodes: 0, number of available pods: 0
May 30 06:55:55.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5491/daemonsets","resourceVersion":"51529"},"items":null}

May 30 06:55:55.416: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5491/pods","resourceVersion":"51529"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:55:55.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5491" for this suite.

• [SLOW TEST:23.556 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":303,"completed":128,"skipped":2269,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:55:55.456: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:56:11.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-94" for this suite.

• [SLOW TEST:16.258 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":303,"completed":129,"skipped":2339,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:56:11.713: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:56:11.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194" in namespace "downward-api-752" to be "Succeeded or Failed"
May 30 06:56:11.769: INFO: Pod "downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194": Phase="Pending", Reason="", readiness=false. Elapsed: 9.422043ms
May 30 06:56:13.774: INFO: Pod "downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014762761s
May 30 06:56:15.779: INFO: Pod "downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019505579s
May 30 06:56:18.031: INFO: Pod "downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.271521486s
STEP: Saw pod success
May 30 06:56:18.031: INFO: Pod "downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194" satisfied condition "Succeeded or Failed"
May 30 06:56:18.037: INFO: Trying to get logs from node my-node pod downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194 container client-container: <nil>
STEP: delete the pod
May 30 06:56:18.121: INFO: Waiting for pod downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194 to disappear
May 30 06:56:18.140: INFO: Pod downwardapi-volume-f603766b-1880-4beb-8ce0-451e849b0194 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:56:18.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-752" for this suite.

• [SLOW TEST:6.444 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":130,"skipped":2355,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:56:18.178: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 06:56:18.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437" in namespace "downward-api-9622" to be "Succeeded or Failed"
May 30 06:56:18.381: INFO: Pod "downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437": Phase="Pending", Reason="", readiness=false. Elapsed: 19.332317ms
May 30 06:56:20.530: INFO: Pod "downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.168553973s
May 30 06:56:22.534: INFO: Pod "downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172366292s
May 30 06:56:24.550: INFO: Pod "downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.187835348s
STEP: Saw pod success
May 30 06:56:24.550: INFO: Pod "downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437" satisfied condition "Succeeded or Failed"
May 30 06:56:24.554: INFO: Trying to get logs from node my-node pod downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437 container client-container: <nil>
STEP: delete the pod
May 30 06:56:24.667: INFO: Waiting for pod downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437 to disappear
May 30 06:56:24.680: INFO: Pod downwardapi-volume-7fbe20c0-7218-4e98-9280-3d786df94437 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:56:24.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9622" for this suite.

• [SLOW TEST:6.517 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":131,"skipped":2365,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:56:24.696: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:56:25.587: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 06:56:27.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:56:29.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490585, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:56:32.626: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 06:56:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8596-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:56:33.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1345" for this suite.
STEP: Destroying namespace "webhook-1345-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.348 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":303,"completed":132,"skipped":2387,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:56:34.058: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2767
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 30 06:56:34.100: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 30 06:56:34.128: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:56:36.231: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:56:38.448: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 06:56:40.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:56:42.141: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:56:44.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:56:46.133: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:56:48.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:56:50.135: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:56:52.135: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 06:56:54.137: INFO: The status of Pod netserver-0 is Running (Ready = true)
STEP: Creating test pods
May 30 06:57:00.164: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.180:8080/dial?request=hostname&protocol=http&host=10.244.0.179&port=8080&tries=1'] Namespace:pod-network-test-2767 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 06:57:00.164: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 06:57:00.341: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:57:00.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2767" for this suite.

• [SLOW TEST:26.297 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":303,"completed":133,"skipped":2421,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:57:00.355: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
May 30 06:57:00.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 30 06:57:01.152: INFO: stderr: ""
May 30 06:57:01.152: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
May 30 06:57:01.152: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 30 06:57:01.152: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1224" to be "running and ready, or succeeded"
May 30 06:57:01.181: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 29.28644ms
May 30 06:57:03.186: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033641526s
May 30 06:57:05.190: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03816423s
May 30 06:57:07.533: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 6.381238127s
May 30 06:57:07.533: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 30 06:57:07.533: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 30 06:57:07.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 logs logs-generator logs-generator'
May 30 06:57:07.920: INFO: stderr: ""
May 30 06:57:07.920: INFO: stdout: "I0530 06:57:04.649782       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/smx 387\nI0530 06:57:04.850888       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/6flc 504\nI0530 06:57:05.051762       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/9vn 550\nI0530 06:57:05.250832       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/n2k 533\nI0530 06:57:05.450637       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/bvnx 399\nI0530 06:57:05.650291       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/x6kw 545\nI0530 06:57:05.851398       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/d9mb 366\nI0530 06:57:06.050192       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/n4w 344\nI0530 06:57:06.250063       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/44cb 205\nI0530 06:57:06.450392       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/jjgm 522\nI0530 06:57:06.650284       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/99k 436\nI0530 06:57:06.850692       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/mf49 317\nI0530 06:57:07.050108       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/4th 272\nI0530 06:57:07.250313       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/r4h 218\nI0530 06:57:07.450227       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/7jl 562\nI0530 06:57:07.650275       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/qcz 221\nI0530 06:57:07.850668       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/mk9 555\n"
STEP: limiting log lines
May 30 06:57:07.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 logs logs-generator logs-generator --tail=1'
May 30 06:57:08.178: INFO: stderr: ""
May 30 06:57:08.178: INFO: stdout: "I0530 06:57:08.056382       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/bbfm 528\n"
May 30 06:57:08.178: INFO: got output "I0530 06:57:08.056382       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/bbfm 528\n"
STEP: limiting log bytes
May 30 06:57:08.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 logs logs-generator logs-generator --limit-bytes=1'
May 30 06:57:08.471: INFO: stderr: ""
May 30 06:57:08.471: INFO: stdout: "I"
May 30 06:57:08.471: INFO: got output "I"
STEP: exposing timestamps
May 30 06:57:08.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 logs logs-generator logs-generator --tail=1 --timestamps'
May 30 06:57:08.618: INFO: stderr: ""
May 30 06:57:08.618: INFO: stdout: "2022-05-30T06:57:08.450853488Z I0530 06:57:08.450123       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/zfv 313\n"
May 30 06:57:08.618: INFO: got output "2022-05-30T06:57:08.450853488Z I0530 06:57:08.450123       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/zfv 313\n"
STEP: restricting to a time range
May 30 06:57:11.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 logs logs-generator logs-generator --since=1s'
May 30 06:57:11.265: INFO: stderr: ""
May 30 06:57:11.265: INFO: stdout: "I0530 06:57:10.451272       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/twf4 230\nI0530 06:57:10.651387       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/m9v 339\nI0530 06:57:10.850091       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/p2bk 367\nI0530 06:57:11.050211       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/q2b 342\nI0530 06:57:11.250438       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/zrq 428\n"
May 30 06:57:11.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 logs logs-generator logs-generator --since=24h'
May 30 06:57:11.393: INFO: stderr: ""
May 30 06:57:11.393: INFO: stdout: "I0530 06:57:04.649782       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/smx 387\nI0530 06:57:04.850888       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/6flc 504\nI0530 06:57:05.051762       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/9vn 550\nI0530 06:57:05.250832       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/n2k 533\nI0530 06:57:05.450637       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/bvnx 399\nI0530 06:57:05.650291       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/x6kw 545\nI0530 06:57:05.851398       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/d9mb 366\nI0530 06:57:06.050192       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/n4w 344\nI0530 06:57:06.250063       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/44cb 205\nI0530 06:57:06.450392       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/jjgm 522\nI0530 06:57:06.650284       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/99k 436\nI0530 06:57:06.850692       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/mf49 317\nI0530 06:57:07.050108       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/4th 272\nI0530 06:57:07.250313       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/r4h 218\nI0530 06:57:07.450227       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/7jl 562\nI0530 06:57:07.650275       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/qcz 221\nI0530 06:57:07.850668       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/mk9 555\nI0530 06:57:08.056382       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/bbfm 528\nI0530 06:57:08.250579       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/54tt 506\nI0530 06:57:08.450123       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/zfv 313\nI0530 06:57:08.650002       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/n7k 531\nI0530 06:57:09.750183       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/z8v 573\nI0530 06:57:09.850120       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/w78 360\nI0530 06:57:10.050527       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/s4jn 231\nI0530 06:57:10.251178       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/9j8z 597\nI0530 06:57:10.451272       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/twf4 230\nI0530 06:57:10.651387       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/m9v 339\nI0530 06:57:10.850091       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/p2bk 367\nI0530 06:57:11.050211       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/q2b 342\nI0530 06:57:11.250438       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/zrq 428\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
May 30 06:57:11.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1224 delete pod logs-generator'
May 30 06:57:15.490: INFO: stderr: ""
May 30 06:57:15.490: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:57:15.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1224" for this suite.

• [SLOW TEST:15.149 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":303,"completed":134,"skipped":2426,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:57:15.504: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
May 30 06:57:15.601: INFO: Waiting up to 5m0s for pod "var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b" in namespace "var-expansion-7950" to be "Succeeded or Failed"
May 30 06:57:15.606: INFO: Pod "var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66979ms
May 30 06:57:17.627: INFO: Pod "var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025568719s
May 30 06:57:19.632: INFO: Pod "var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030275388s
May 30 06:57:21.636: INFO: Pod "var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034860247s
STEP: Saw pod success
May 30 06:57:21.637: INFO: Pod "var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b" satisfied condition "Succeeded or Failed"
May 30 06:57:21.640: INFO: Trying to get logs from node my-node pod var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b container dapi-container: <nil>
STEP: delete the pod
May 30 06:57:21.669: INFO: Waiting for pod var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b to disappear
May 30 06:57:21.673: INFO: Pod var-expansion-0b78a78b-df20-4c35-9b20-c2b00113eb9b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:57:21.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7950" for this suite.

• [SLOW TEST:6.181 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":303,"completed":135,"skipped":2446,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:57:21.692: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 06:57:22.101: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 06:57:24.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 06:57:26.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789490642, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 06:57:29.178: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:57:39.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9594" for this suite.
STEP: Destroying namespace "webhook-9594-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":303,"completed":136,"skipped":2462,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:57:39.987: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-356/secret-test-fabe3a35-f8fe-4d79-9750-83d95f989495
STEP: Creating a pod to test consume secrets
May 30 06:57:40.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92" in namespace "secrets-356" to be "Succeeded or Failed"
May 30 06:57:40.068: INFO: Pod "pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92": Phase="Pending", Reason="", readiness=false. Elapsed: 17.627863ms
May 30 06:57:42.919: INFO: Pod "pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868351594s
May 30 06:57:45.158: INFO: Pod "pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92": Phase="Pending", Reason="", readiness=false. Elapsed: 5.108039624s
May 30 06:57:47.355: INFO: Pod "pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.3048314s
STEP: Saw pod success
May 30 06:57:47.355: INFO: Pod "pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92" satisfied condition "Succeeded or Failed"
May 30 06:57:47.449: INFO: Trying to get logs from node my-node pod pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92 container env-test: <nil>
STEP: delete the pod
May 30 06:57:47.606: INFO: Waiting for pod pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92 to disappear
May 30 06:57:47.609: INFO: Pod pod-configmaps-3e8cab96-e914-40f8-b4cd-4ff714219c92 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:57:47.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-356" for this suite.

• [SLOW TEST:7.635 seconds]
[sig-api-machinery] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":303,"completed":137,"skipped":2462,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:57:47.622: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 30 06:57:47.683: INFO: Waiting up to 5m0s for pod "pod-f4535076-0575-416e-9bfa-031a204c87a8" in namespace "emptydir-7514" to be "Succeeded or Failed"
May 30 06:57:47.696: INFO: Pod "pod-f4535076-0575-416e-9bfa-031a204c87a8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.043139ms
May 30 06:57:49.720: INFO: Pod "pod-f4535076-0575-416e-9bfa-031a204c87a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036455826s
May 30 06:57:51.728: INFO: Pod "pod-f4535076-0575-416e-9bfa-031a204c87a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.044268949s
May 30 06:57:53.731: INFO: Pod "pod-f4535076-0575-416e-9bfa-031a204c87a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047413036s
STEP: Saw pod success
May 30 06:57:53.731: INFO: Pod "pod-f4535076-0575-416e-9bfa-031a204c87a8" satisfied condition "Succeeded or Failed"
May 30 06:57:53.734: INFO: Trying to get logs from node my-node pod pod-f4535076-0575-416e-9bfa-031a204c87a8 container test-container: <nil>
STEP: delete the pod
May 30 06:57:53.764: INFO: Waiting for pod pod-f4535076-0575-416e-9bfa-031a204c87a8 to disappear
May 30 06:57:53.767: INFO: Pod pod-f4535076-0575-416e-9bfa-031a204c87a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:57:53.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7514" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":138,"skipped":2467,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:57:53.785: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:57:57.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4019" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":139,"skipped":2495,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:57:57.923: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-db8f62bb-4020-4f81-8d25-284e0662a2df
STEP: Creating a pod to test consume secrets
May 30 06:57:58.005: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02" in namespace "projected-1446" to be "Succeeded or Failed"
May 30 06:57:58.017: INFO: Pod "pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02": Phase="Pending", Reason="", readiness=false. Elapsed: 11.1901ms
May 30 06:58:00.323: INFO: Pod "pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.317787213s
May 30 06:58:02.330: INFO: Pod "pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.325013003s
STEP: Saw pod success
May 30 06:58:02.331: INFO: Pod "pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02" satisfied condition "Succeeded or Failed"
May 30 06:58:02.338: INFO: Trying to get logs from node my-node pod pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 30 06:58:02.425: INFO: Waiting for pod pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02 to disappear
May 30 06:58:02.429: INFO: Pod pod-projected-secrets-ffb6d2b6-c1e5-4d11-90c0-1d38e4f05b02 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:58:02.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1446" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":140,"skipped":2497,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:58:02.440: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-7e3efdb0-8150-4341-98b7-7dd746d2c6fd in namespace container-probe-4448
May 30 06:58:08.499: INFO: Started pod busybox-7e3efdb0-8150-4341-98b7-7dd746d2c6fd in namespace container-probe-4448
STEP: checking the pod's current state and verifying that restartCount is present
May 30 06:58:08.502: INFO: Initial restart count of pod busybox-7e3efdb0-8150-4341-98b7-7dd746d2c6fd is 0
May 30 06:59:04.748: INFO: Restart count of pod container-probe-4448/busybox-7e3efdb0-8150-4341-98b7-7dd746d2c6fd is now 1 (56.245925725s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 06:59:04.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4448" for this suite.

• [SLOW TEST:62.349 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":303,"completed":141,"skipped":2510,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 06:59:04.789: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0530 06:59:45.304348      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 07:00:47.333: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 30 07:00:47.333: INFO: Deleting pod "simpletest.rc-c2fpc" in namespace "gc-517"
May 30 07:00:47.363: INFO: Deleting pod "simpletest.rc-hsh7f" in namespace "gc-517"
May 30 07:00:47.409: INFO: Deleting pod "simpletest.rc-j5wwd" in namespace "gc-517"
May 30 07:00:47.451: INFO: Deleting pod "simpletest.rc-jz8gf" in namespace "gc-517"
May 30 07:00:47.487: INFO: Deleting pod "simpletest.rc-kwplj" in namespace "gc-517"
May 30 07:00:47.507: INFO: Deleting pod "simpletest.rc-szdjj" in namespace "gc-517"
May 30 07:00:47.553: INFO: Deleting pod "simpletest.rc-w7zqz" in namespace "gc-517"
May 30 07:00:47.572: INFO: Deleting pod "simpletest.rc-xcn54" in namespace "gc-517"
May 30 07:00:47.668: INFO: Deleting pod "simpletest.rc-z46mx" in namespace "gc-517"
May 30 07:00:47.712: INFO: Deleting pod "simpletest.rc-zv7tt" in namespace "gc-517"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:00:47.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-517" for this suite.

• [SLOW TEST:103.031 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":303,"completed":142,"skipped":2517,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:00:47.822: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:00:48.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9383" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":303,"completed":143,"skipped":2521,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:00:48.079: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 30 07:00:48.144: INFO: Waiting up to 5m0s for pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976" in namespace "downward-api-4506" to be "Succeeded or Failed"
May 30 07:00:48.167: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 23.524537ms
May 30 07:00:50.437: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293833977s
May 30 07:00:52.669: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525399577s
May 30 07:00:54.778: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 6.634255219s
May 30 07:00:56.980: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 8.836157896s
May 30 07:00:59.153: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 11.009816019s
May 30 07:01:01.695: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 13.551832111s
May 30 07:01:04.051: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 15.907158387s
May 30 07:01:06.120: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Pending", Reason="", readiness=false. Elapsed: 17.975930954s
May 30 07:01:08.124: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 19.980026266s
STEP: Saw pod success
May 30 07:01:08.124: INFO: Pod "downward-api-aeebb65c-f058-47e3-9587-26d55e44d976" satisfied condition "Succeeded or Failed"
May 30 07:01:08.127: INFO: Trying to get logs from node my-node pod downward-api-aeebb65c-f058-47e3-9587-26d55e44d976 container dapi-container: <nil>
STEP: delete the pod
May 30 07:01:08.168: INFO: Waiting for pod downward-api-aeebb65c-f058-47e3-9587-26d55e44d976 to disappear
May 30 07:01:08.172: INFO: Pod downward-api-aeebb65c-f058-47e3-9587-26d55e44d976 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:01:08.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4506" for this suite.

• [SLOW TEST:20.113 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":303,"completed":144,"skipped":2524,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:01:08.193: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:01:08.626: INFO: Checking APIGroup: apiregistration.k8s.io
May 30 07:01:08.627: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 30 07:01:08.627: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.627: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 30 07:01:08.627: INFO: Checking APIGroup: extensions
May 30 07:01:08.629: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 30 07:01:08.629: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 30 07:01:08.630: INFO: extensions/v1beta1 matches extensions/v1beta1
May 30 07:01:08.630: INFO: Checking APIGroup: apps
May 30 07:01:08.631: INFO: PreferredVersion.GroupVersion: apps/v1
May 30 07:01:08.631: INFO: Versions found [{apps/v1 v1}]
May 30 07:01:08.631: INFO: apps/v1 matches apps/v1
May 30 07:01:08.631: INFO: Checking APIGroup: events.k8s.io
May 30 07:01:08.632: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 30 07:01:08.632: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.632: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 30 07:01:08.632: INFO: Checking APIGroup: authentication.k8s.io
May 30 07:01:08.634: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 30 07:01:08.634: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.634: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 30 07:01:08.634: INFO: Checking APIGroup: authorization.k8s.io
May 30 07:01:08.635: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 30 07:01:08.635: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.635: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 30 07:01:08.635: INFO: Checking APIGroup: autoscaling
May 30 07:01:08.636: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 30 07:01:08.636: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 30 07:01:08.636: INFO: autoscaling/v1 matches autoscaling/v1
May 30 07:01:08.636: INFO: Checking APIGroup: batch
May 30 07:01:08.637: INFO: PreferredVersion.GroupVersion: batch/v1
May 30 07:01:08.637: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 30 07:01:08.637: INFO: batch/v1 matches batch/v1
May 30 07:01:08.637: INFO: Checking APIGroup: certificates.k8s.io
May 30 07:01:08.641: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 30 07:01:08.641: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.641: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 30 07:01:08.641: INFO: Checking APIGroup: networking.k8s.io
May 30 07:01:08.643: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 30 07:01:08.643: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.643: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 30 07:01:08.643: INFO: Checking APIGroup: policy
May 30 07:01:08.644: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 30 07:01:08.644: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 30 07:01:08.644: INFO: policy/v1beta1 matches policy/v1beta1
May 30 07:01:08.644: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 30 07:01:08.645: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 30 07:01:08.645: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.645: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 30 07:01:08.645: INFO: Checking APIGroup: storage.k8s.io
May 30 07:01:08.647: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 30 07:01:08.647: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.647: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 30 07:01:08.647: INFO: Checking APIGroup: admissionregistration.k8s.io
May 30 07:01:08.650: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 30 07:01:08.650: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.650: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 30 07:01:08.650: INFO: Checking APIGroup: apiextensions.k8s.io
May 30 07:01:08.651: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 30 07:01:08.651: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.651: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 30 07:01:08.651: INFO: Checking APIGroup: scheduling.k8s.io
May 30 07:01:08.653: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 30 07:01:08.653: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.653: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 30 07:01:08.653: INFO: Checking APIGroup: coordination.k8s.io
May 30 07:01:08.654: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 30 07:01:08.654: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.654: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 30 07:01:08.654: INFO: Checking APIGroup: node.k8s.io
May 30 07:01:08.656: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
May 30 07:01:08.656: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.656: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
May 30 07:01:08.656: INFO: Checking APIGroup: discovery.k8s.io
May 30 07:01:08.657: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 30 07:01:08.657: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 30 07:01:08.657: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:01:08.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7443" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":303,"completed":145,"skipped":2557,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:01:08.666: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 30 07:01:08.704: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 30 07:01:08.711: INFO: Waiting for terminating namespaces to be deleted...
May 30 07:01:08.713: INFO: 
Logging pods the apiserver thinks is on node my-node before test
May 30 07:01:08.724: INFO: coredns-f9fd979d6-c9tcq from kube-system started at 2022-05-27 09:44:35 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container coredns ready: true, restart count 1
May 30 07:01:08.739: INFO: coredns-f9fd979d6-lrzv9 from kube-system started at 2022-05-27 09:44:25 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container coredns ready: true, restart count 1
May 30 07:01:08.739: INFO: etcd-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container etcd ready: true, restart count 1
May 30 07:01:08.739: INFO: kube-apiserver-my-node from kube-system started at 2022-05-30 01:38:05 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container kube-apiserver ready: true, restart count 1
May 30 07:01:08.739: INFO: kube-controller-manager-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 07:01:08.739: INFO: kube-flannel-ds-lv5nv from kube-system started at 2022-05-27 09:43:53 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container kube-flannel ready: true, restart count 1
May 30 07:01:08.739: INFO: kube-proxy-4pwdh from kube-system started at 2022-05-27 09:42:33 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 07:01:08.739: INFO: kube-scheduler-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 07:01:08.739: INFO: sonobuoy from sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (1 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 30 07:01:08.739: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 from sonobuoy started at 2022-05-30 06:08:24 +0000 UTC (2 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container e2e ready: true, restart count 0
May 30 07:01:08.739: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:01:08.739: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 from sonobuoy started at 2022-05-30 06:08:25 +0000 UTC (2 container statuses recorded)
May 30 07:01:08.739: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:01:08.739: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e6d507a8-9098-4ee6-aed9-822666f7faf8 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-e6d507a8-9098-4ee6-aed9-822666f7faf8 off the node my-node
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e6d507a8-9098-4ee6-aed9-822666f7faf8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:01:37.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5586" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:28.426 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":303,"completed":146,"skipped":2565,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:01:37.092: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-117e4344-3c48-4585-a150-865368d690ec
STEP: Creating a pod to test consume secrets
May 30 07:01:37.154: INFO: Waiting up to 5m0s for pod "pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5" in namespace "secrets-5517" to be "Succeeded or Failed"
May 30 07:01:37.192: INFO: Pod "pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.580585ms
May 30 07:01:39.198: INFO: Pod "pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043830174s
May 30 07:01:41.313: INFO: Pod "pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.158619124s
May 30 07:01:43.675: INFO: Pod "pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.5202382s
STEP: Saw pod success
May 30 07:01:43.675: INFO: Pod "pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5" satisfied condition "Succeeded or Failed"
May 30 07:01:43.679: INFO: Trying to get logs from node my-node pod pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5 container secret-volume-test: <nil>
STEP: delete the pod
May 30 07:01:45.116: INFO: Waiting for pod pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5 to disappear
May 30 07:01:45.131: INFO: Pod pod-secrets-db2417b6-9cc7-4a8d-8e7b-4d354678fdf5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:01:45.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5517" for this suite.

• [SLOW TEST:8.107 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":303,"completed":147,"skipped":2589,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:01:45.199: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:01:45.584: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 30 07:01:45.628: INFO: Number of nodes with available pods: 0
May 30 07:01:45.628: INFO: Node my-node is running more than one daemon pod
May 30 07:01:46.873: INFO: Number of nodes with available pods: 0
May 30 07:01:46.874: INFO: Node my-node is running more than one daemon pod
May 30 07:01:48.220: INFO: Number of nodes with available pods: 0
May 30 07:01:48.221: INFO: Node my-node is running more than one daemon pod
May 30 07:01:49.483: INFO: Number of nodes with available pods: 0
May 30 07:01:49.484: INFO: Node my-node is running more than one daemon pod
May 30 07:01:49.982: INFO: Number of nodes with available pods: 0
May 30 07:01:49.982: INFO: Node my-node is running more than one daemon pod
May 30 07:01:50.945: INFO: Number of nodes with available pods: 0
May 30 07:01:50.945: INFO: Node my-node is running more than one daemon pod
May 30 07:01:51.645: INFO: Number of nodes with available pods: 0
May 30 07:01:51.646: INFO: Node my-node is running more than one daemon pod
May 30 07:01:52.807: INFO: Number of nodes with available pods: 0
May 30 07:01:52.807: INFO: Node my-node is running more than one daemon pod
May 30 07:01:54.189: INFO: Number of nodes with available pods: 0
May 30 07:01:54.189: INFO: Node my-node is running more than one daemon pod
May 30 07:01:55.510: INFO: Number of nodes with available pods: 0
May 30 07:01:55.510: INFO: Node my-node is running more than one daemon pod
May 30 07:01:55.792: INFO: Number of nodes with available pods: 0
May 30 07:01:55.792: INFO: Node my-node is running more than one daemon pod
May 30 07:01:56.642: INFO: Number of nodes with available pods: 1
May 30 07:01:56.643: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 30 07:01:56.730: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:01:57.741: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:01:58.750: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:01:59.740: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:02:00.741: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:02:01.749: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:02:01.749: INFO: Pod daemon-set-lnlp5 is not available
May 30 07:02:02.743: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:02:02.745: INFO: Pod daemon-set-lnlp5 is not available
May 30 07:02:03.740: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:02:03.740: INFO: Pod daemon-set-lnlp5 is not available
May 30 07:02:04.747: INFO: Wrong image for pod: daemon-set-lnlp5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
May 30 07:02:04.747: INFO: Pod daemon-set-lnlp5 is not available
May 30 07:02:05.757: INFO: Pod daemon-set-mprfw is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 30 07:02:05.768: INFO: Number of nodes with available pods: 0
May 30 07:02:05.768: INFO: Node my-node is running more than one daemon pod
May 30 07:02:06.863: INFO: Number of nodes with available pods: 0
May 30 07:02:06.863: INFO: Node my-node is running more than one daemon pod
May 30 07:02:07.991: INFO: Number of nodes with available pods: 0
May 30 07:02:07.991: INFO: Node my-node is running more than one daemon pod
May 30 07:02:09.035: INFO: Number of nodes with available pods: 0
May 30 07:02:09.035: INFO: Node my-node is running more than one daemon pod
May 30 07:02:09.777: INFO: Number of nodes with available pods: 0
May 30 07:02:09.777: INFO: Node my-node is running more than one daemon pod
May 30 07:02:10.777: INFO: Number of nodes with available pods: 1
May 30 07:02:10.777: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6088, will wait for the garbage collector to delete the pods
May 30 07:02:10.884: INFO: Deleting DaemonSet.extensions daemon-set took: 36.696324ms
May 30 07:02:11.285: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.983542ms
May 30 07:02:16.290: INFO: Number of nodes with available pods: 0
May 30 07:02:16.290: INFO: Number of running nodes: 0, number of available pods: 0
May 30 07:02:16.295: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6088/daemonsets","resourceVersion":"53255"},"items":null}

May 30 07:02:16.301: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6088/pods","resourceVersion":"53255"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:02:16.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6088" for this suite.

• [SLOW TEST:31.120 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":303,"completed":148,"skipped":2594,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:02:16.319: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:02:16.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9180" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":303,"completed":149,"skipped":2597,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:02:16.711: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:02:22.819: INFO: Waiting up to 5m0s for pod "client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d" in namespace "pods-7396" to be "Succeeded or Failed"
May 30 07:02:22.880: INFO: Pod "client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d": Phase="Pending", Reason="", readiness=false. Elapsed: 60.546093ms
May 30 07:02:24.885: INFO: Pod "client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06604577s
May 30 07:02:26.895: INFO: Pod "client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075567042s
May 30 07:02:29.304: INFO: Pod "client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.484708023s
STEP: Saw pod success
May 30 07:02:29.304: INFO: Pod "client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d" satisfied condition "Succeeded or Failed"
May 30 07:02:29.310: INFO: Trying to get logs from node my-node pod client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d container env3cont: <nil>
STEP: delete the pod
May 30 07:02:29.459: INFO: Waiting for pod client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d to disappear
May 30 07:02:29.462: INFO: Pod client-envvars-468ff599-2aef-471c-97ff-89f00a30b83d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:02:29.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7396" for this suite.

• [SLOW TEST:12.759 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":303,"completed":150,"skipped":2608,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:02:29.472: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
May 30 07:04:30.065: INFO: Successfully updated pod "var-expansion-5d9b5238-fe7c-414d-aee9-3e97448b60e8"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 30 07:04:32.088: INFO: Deleting pod "var-expansion-5d9b5238-fe7c-414d-aee9-3e97448b60e8" in namespace "var-expansion-9377"
May 30 07:04:32.097: INFO: Wait up to 5m0s for pod "var-expansion-5d9b5238-fe7c-414d-aee9-3e97448b60e8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:05:08.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9377" for this suite.

• [SLOW TEST:158.648 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":303,"completed":151,"skipped":2617,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:05:08.120: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
May 30 07:05:08.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-4052 create -f -'
May 30 07:05:08.939: INFO: stderr: ""
May 30 07:05:08.939: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 30 07:05:08.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-4052 diff -f -'
May 30 07:05:12.448: INFO: rc: 1
May 30 07:05:12.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-4052 delete -f -'
May 30 07:05:12.809: INFO: stderr: ""
May 30 07:05:12.810: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:05:12.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4052" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":303,"completed":152,"skipped":2628,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:05:12.828: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 07:05:13.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a" in namespace "projected-8531" to be "Succeeded or Failed"
May 30 07:05:13.479: INFO: Pod "downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 39.574917ms
May 30 07:05:15.903: INFO: Pod "downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463958112s
May 30 07:05:18.188: INFO: Pod "downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748563648s
May 30 07:05:20.194: INFO: Pod "downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.754719777s
May 30 07:05:22.206: INFO: Pod "downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.766345384s
STEP: Saw pod success
May 30 07:05:22.206: INFO: Pod "downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a" satisfied condition "Succeeded or Failed"
May 30 07:05:22.209: INFO: Trying to get logs from node my-node pod downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a container client-container: <nil>
STEP: delete the pod
May 30 07:05:22.246: INFO: Waiting for pod downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a to disappear
May 30 07:05:22.249: INFO: Pod downwardapi-volume-13537c22-a229-45d8-aa3b-3d5020a73e4a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:05:22.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8531" for this suite.

• [SLOW TEST:9.443 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":153,"skipped":2647,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:05:22.295: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2122.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2122.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 07:05:32.402: INFO: DNS probes using dns-test-a710e004-ae9f-49fe-a1d9-60153acc53aa succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2122.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2122.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 07:05:42.640: INFO: File wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:42.645: INFO: File jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:42.645: INFO: Lookups using dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 failed for: [wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local]

May 30 07:05:47.650: INFO: File wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:47.655: INFO: File jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:47.655: INFO: Lookups using dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 failed for: [wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local]

May 30 07:05:52.653: INFO: File wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:52.656: INFO: File jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:52.656: INFO: Lookups using dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 failed for: [wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local]

May 30 07:05:57.651: INFO: File wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:57.655: INFO: File jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local from pod  dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 30 07:05:57.655: INFO: Lookups using dns-2122/dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 failed for: [wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local]

May 30 07:06:02.654: INFO: DNS probes using dns-test-2d46bbe4-fc8b-4f2e-901c-a622c1452059 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2122.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2122.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2122.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2122.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 07:06:12.812: INFO: DNS probes using dns-test-102255e7-1b3c-48d5-a5b1-6dbee5b4f174 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:06:12.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2122" for this suite.

• [SLOW TEST:50.627 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":303,"completed":154,"skipped":2673,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:06:12.922: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 07:06:14.376: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 07:06:16.382: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:06:18.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:06:20.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491174, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:06:23.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 30 07:06:27.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=webhook-900 attach --namespace=webhook-900 to-be-attached-pod -i -c=container1'
May 30 07:06:27.612: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:06:27.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-900" for this suite.
STEP: Destroying namespace "webhook-900-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.778 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":303,"completed":155,"skipped":2676,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:06:27.700: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-546
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-546
STEP: creating replication controller externalsvc in namespace services-546
I0530 07:06:27.812708      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-546, replica count: 2
I0530 07:06:30.867157      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:06:33.875134      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:06:36.877046      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:06:39.881985      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 30 07:06:39.957: INFO: Creating new exec pod
May 30 07:06:45.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-546 exec execpodjhbmd -- /bin/sh -x -c nslookup nodeport-service.services-546.svc.cluster.local'
May 30 07:06:46.363: INFO: stderr: "+ nslookup nodeport-service.services-546.svc.cluster.local\n"
May 30 07:06:46.363: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-546.svc.cluster.local\tcanonical name = externalsvc.services-546.svc.cluster.local.\nName:\texternalsvc.services-546.svc.cluster.local\nAddress: 10.110.246.88\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-546, will wait for the garbage collector to delete the pods
May 30 07:06:46.431: INFO: Deleting ReplicationController externalsvc took: 13.839952ms
May 30 07:06:46.532: INFO: Terminating ReplicationController externalsvc pods took: 101.343637ms
May 30 07:07:05.478: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:07:05.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-546" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:37.916 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":303,"completed":156,"skipped":2688,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:07:05.616: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3746
STEP: creating service affinity-nodeport-transition in namespace services-3746
STEP: creating replication controller affinity-nodeport-transition in namespace services-3746
I0530 07:07:05.679978      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3746, replica count: 3
I0530 07:07:08.735886      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:07:11.736071      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:07:14.738587      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:07:17.739353      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 07:07:17.753: INFO: Creating new exec pod
May 30 07:07:22.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-3746 exec execpod-affinitykhxxt -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 30 07:07:23.686: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 30 07:07:23.686: INFO: stdout: ""
May 30 07:07:23.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-3746 exec execpod-affinitykhxxt -- /bin/sh -x -c nc -zv -t -w 2 10.105.14.65 80'
May 30 07:07:24.081: INFO: stderr: "+ nc -zv -t -w 2 10.105.14.65 80\nConnection to 10.105.14.65 80 port [tcp/http] succeeded!\n"
May 30 07:07:24.081: INFO: stdout: ""
May 30 07:07:24.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-3746 exec execpod-affinitykhxxt -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.15 30191'
May 30 07:07:24.393: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.15 30191\nConnection to 10.0.2.15 30191 port [tcp/30191] succeeded!\n"
May 30 07:07:24.393: INFO: stdout: ""
May 30 07:07:24.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-3746 exec execpod-affinitykhxxt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.2.15:30191/ ; done'
May 30 07:07:24.862: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n"
May 30 07:07:24.862: INFO: stdout: "\naffinity-nodeport-transition-ssfbh\naffinity-nodeport-transition-ssfbh\naffinity-nodeport-transition-mmwnb\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-ssfbh\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-mmwnb\naffinity-nodeport-transition-ssfbh\naffinity-nodeport-transition-mmwnb\naffinity-nodeport-transition-mmwnb\naffinity-nodeport-transition-mmwnb\naffinity-nodeport-transition-ssfbh\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-ssfbh\naffinity-nodeport-transition-mmwnb"
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-ssfbh
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-ssfbh
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-mmwnb
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-ssfbh
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-mmwnb
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-ssfbh
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-mmwnb
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-mmwnb
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-mmwnb
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-ssfbh
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-ssfbh
May 30 07:07:24.862: INFO: Received response from host: affinity-nodeport-transition-mmwnb
May 30 07:07:24.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-3746 exec execpod-affinitykhxxt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.2.15:30191/ ; done'
May 30 07:07:25.334: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30191/\n"
May 30 07:07:25.334: INFO: stdout: "\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87\naffinity-nodeport-transition-dsz87"
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Received response from host: affinity-nodeport-transition-dsz87
May 30 07:07:25.334: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3746, will wait for the garbage collector to delete the pods
May 30 07:07:25.417: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.938514ms
May 30 07:07:26.118: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 700.588263ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:07:45.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3746" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:39.962 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":157,"skipped":2702,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:07:45.579: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-8323
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8323 to expose endpoints map[]
May 30 07:07:45.734: INFO: successfully validated that service multi-endpoint-test in namespace services-8323 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8323
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8323 to expose endpoints map[pod1:[100]]
May 30 07:07:49.792: INFO: successfully validated that service multi-endpoint-test in namespace services-8323 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8323
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8323 to expose endpoints map[pod1:[100] pod2:[101]]
May 30 07:07:53.845: INFO: Unexpected endpoints: found map[700d4c59-616f-43d2-962a-efcd09f0a46c:[100]], expected map[pod1:[100] pod2:[101]], will retry
May 30 07:07:54.850: INFO: successfully validated that service multi-endpoint-test in namespace services-8323 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-8323
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8323 to expose endpoints map[pod2:[101]]
May 30 07:07:55.019: INFO: successfully validated that service multi-endpoint-test in namespace services-8323 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8323
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8323 to expose endpoints map[]
May 30 07:07:55.628: INFO: successfully validated that service multi-endpoint-test in namespace services-8323 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:07:55.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8323" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.205 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":303,"completed":158,"skipped":2705,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:07:55.788: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-5601/configmap-test-d644b475-9fbc-41a0-a9d6-299bb0ff81ab
STEP: Creating a pod to test consume configMaps
May 30 07:07:55.875: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c" in namespace "configmap-5601" to be "Succeeded or Failed"
May 30 07:07:55.891: INFO: Pod "pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.318466ms
May 30 07:07:57.974: INFO: Pod "pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096425265s
May 30 07:08:00.182: INFO: Pod "pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.304626029s
May 30 07:08:02.311: INFO: Pod "pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.433492981s
May 30 07:08:04.317: INFO: Pod "pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.439866183s
STEP: Saw pod success
May 30 07:08:04.317: INFO: Pod "pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c" satisfied condition "Succeeded or Failed"
May 30 07:08:04.319: INFO: Trying to get logs from node my-node pod pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c container env-test: <nil>
STEP: delete the pod
May 30 07:08:05.059: INFO: Waiting for pod pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c to disappear
May 30 07:08:05.095: INFO: Pod pod-configmaps-b2a4fe24-356b-438b-b610-09837e667e8c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:08:05.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5601" for this suite.

• [SLOW TEST:9.330 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":303,"completed":159,"skipped":2715,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:08:05.120: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6526 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6526;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6526 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6526;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6526.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6526.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6526.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6526.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6526.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6526.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6526.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6526.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6526.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 74.147.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.147.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.147.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.147.74_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6526 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6526;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6526 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6526;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6526.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6526.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6526.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6526.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6526.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6526.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6526.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6526.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6526.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6526.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 74.147.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.147.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.147.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.147.74_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 07:08:13.317: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.322: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.326: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.331: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.335: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.341: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.347: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.351: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.386: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.390: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.394: INFO: Unable to read jessie_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.398: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.404: INFO: Unable to read jessie_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.408: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:13.438: INFO: Lookups using dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6526 wheezy_tcp@dns-test-service.dns-6526 wheezy_udp@dns-test-service.dns-6526.svc wheezy_tcp@dns-test-service.dns-6526.svc wheezy_udp@_http._tcp.dns-test-service.dns-6526.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6526.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6526 jessie_tcp@dns-test-service.dns-6526 jessie_udp@dns-test-service.dns-6526.svc jessie_tcp@dns-test-service.dns-6526.svc]

May 30 07:08:18.442: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.448: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.452: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.456: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.459: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.462: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.516: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.520: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.523: INFO: Unable to read jessie_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.529: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.533: INFO: Unable to read jessie_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.537: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:18.566: INFO: Lookups using dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6526 wheezy_tcp@dns-test-service.dns-6526 wheezy_udp@dns-test-service.dns-6526.svc wheezy_tcp@dns-test-service.dns-6526.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6526 jessie_tcp@dns-test-service.dns-6526 jessie_udp@dns-test-service.dns-6526.svc jessie_tcp@dns-test-service.dns-6526.svc]

May 30 07:08:23.448: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.479: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.489: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.495: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.500: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.535: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.540: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.544: INFO: Unable to read jessie_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.549: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.553: INFO: Unable to read jessie_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.558: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:23.592: INFO: Lookups using dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6526 wheezy_tcp@dns-test-service.dns-6526 wheezy_udp@dns-test-service.dns-6526.svc wheezy_tcp@dns-test-service.dns-6526.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6526 jessie_tcp@dns-test-service.dns-6526 jessie_udp@dns-test-service.dns-6526.svc jessie_tcp@dns-test-service.dns-6526.svc]

May 30 07:08:28.442: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.455: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.462: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.467: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.513: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.518: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.522: INFO: Unable to read jessie_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.526: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.532: INFO: Unable to read jessie_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.543: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:28.585: INFO: Lookups using dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6526 wheezy_tcp@dns-test-service.dns-6526 wheezy_udp@dns-test-service.dns-6526.svc wheezy_tcp@dns-test-service.dns-6526.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6526 jessie_tcp@dns-test-service.dns-6526 jessie_udp@dns-test-service.dns-6526.svc jessie_tcp@dns-test-service.dns-6526.svc]

May 30 07:08:33.448: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.457: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.463: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.471: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.475: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.540: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.548: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.554: INFO: Unable to read jessie_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.556: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.563: INFO: Unable to read jessie_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.568: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:33.666: INFO: Lookups using dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6526 wheezy_tcp@dns-test-service.dns-6526 wheezy_udp@dns-test-service.dns-6526.svc wheezy_tcp@dns-test-service.dns-6526.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6526 jessie_tcp@dns-test-service.dns-6526 jessie_udp@dns-test-service.dns-6526.svc jessie_tcp@dns-test-service.dns-6526.svc]

May 30 07:08:38.444: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.454: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.458: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.464: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.488: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.544: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.553: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.560: INFO: Unable to read jessie_udp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526 from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.580: INFO: Unable to read jessie_udp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.588: INFO: Unable to read jessie_tcp@dns-test-service.dns-6526.svc from pod dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2: the server could not find the requested resource (get pods dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2)
May 30 07:08:38.648: INFO: Lookups using dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6526 wheezy_tcp@dns-test-service.dns-6526 wheezy_udp@dns-test-service.dns-6526.svc wheezy_tcp@dns-test-service.dns-6526.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6526 jessie_tcp@dns-test-service.dns-6526 jessie_udp@dns-test-service.dns-6526.svc jessie_tcp@dns-test-service.dns-6526.svc]

May 30 07:08:43.583: INFO: DNS probes using dns-6526/dns-test-b21c8711-8f3d-4214-bf3c-86d9ceb253b2 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:08:43.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6526" for this suite.

• [SLOW TEST:38.785 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":303,"completed":160,"skipped":2740,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:08:43.908: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 30 07:08:44.689: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 30 07:08:44.842: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 30 07:08:44.930: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 30 07:08:45.005: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 30 07:08:45.005: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 30 07:08:45.017: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 30 07:08:45.017: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 30 07:08:52.131: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:08:52.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4660" for this suite.

• [SLOW TEST:8.355 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":303,"completed":161,"skipped":2786,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:08:52.263: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:08:52.305: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:08:53.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1705" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":303,"completed":162,"skipped":2788,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:08:53.662: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:08:53.700: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:09:01.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9914" for this suite.

• [SLOW TEST:7.522 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":303,"completed":163,"skipped":2806,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:09:01.186: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 30 07:09:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:09:15.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3472" for this suite.

• [SLOW TEST:14.648 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":303,"completed":164,"skipped":2825,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:09:15.836: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 07:09:16.212: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2" in namespace "projected-5925" to be "Succeeded or Failed"
May 30 07:09:16.242: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.538971ms
May 30 07:09:18.318: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102067914s
May 30 07:09:20.387: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171116455s
May 30 07:09:22.418: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.201922444s
May 30 07:09:24.423: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2": Phase="Running", Reason="", readiness=true. Elapsed: 8.207091014s
May 30 07:09:26.427: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2": Phase="Running", Reason="", readiness=true. Elapsed: 10.211391742s
May 30 07:09:28.574: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.358460826s
STEP: Saw pod success
May 30 07:09:28.574: INFO: Pod "downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2" satisfied condition "Succeeded or Failed"
May 30 07:09:28.580: INFO: Trying to get logs from node my-node pod downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2 container client-container: <nil>
STEP: delete the pod
May 30 07:09:28.862: INFO: Waiting for pod downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2 to disappear
May 30 07:09:28.878: INFO: Pod downwardapi-volume-9af2c2dd-0aee-4a14-bec0-c2c06e2d35e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:09:28.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5925" for this suite.

• [SLOW TEST:13.204 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":303,"completed":165,"skipped":2839,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:09:29.136: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0530 07:09:33.261348      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 07:10:35.731: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:10:35.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-121" for this suite.

• [SLOW TEST:66.635 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":303,"completed":166,"skipped":2910,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:10:35.772: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-25018f7d-e549-4fb9-86f5-5b7772110e3f
STEP: Creating a pod to test consume secrets
May 30 07:10:36.025: INFO: Waiting up to 5m0s for pod "pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f" in namespace "secrets-8485" to be "Succeeded or Failed"
May 30 07:10:36.028: INFO: Pod "pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.051348ms
May 30 07:10:38.090: INFO: Pod "pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065241009s
May 30 07:10:40.094: INFO: Pod "pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069660099s
May 30 07:10:42.122: INFO: Pod "pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096911637s
STEP: Saw pod success
May 30 07:10:42.122: INFO: Pod "pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f" satisfied condition "Succeeded or Failed"
May 30 07:10:42.139: INFO: Trying to get logs from node my-node pod pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f container secret-volume-test: <nil>
STEP: delete the pod
May 30 07:10:44.061: INFO: Waiting for pod pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f to disappear
May 30 07:10:44.067: INFO: Pod pod-secrets-4716ea0b-395e-4bc8-9334-c7057c1da37f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:10:44.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8485" for this suite.
STEP: Destroying namespace "secret-namespace-3508" for this suite.

• [SLOW TEST:8.342 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":303,"completed":167,"skipped":2918,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:10:44.114: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-61190f57-3cd5-487b-bbe6-c6450c25706d
STEP: Creating a pod to test consume secrets
May 30 07:10:44.202: INFO: Waiting up to 5m0s for pod "pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a" in namespace "secrets-7347" to be "Succeeded or Failed"
May 30 07:10:44.245: INFO: Pod "pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a": Phase="Pending", Reason="", readiness=false. Elapsed: 43.405696ms
May 30 07:10:46.467: INFO: Pod "pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264660674s
May 30 07:10:48.685: INFO: Pod "pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.483276917s
May 30 07:10:50.724: INFO: Pod "pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.522326178s
May 30 07:10:52.729: INFO: Pod "pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.527267145s
STEP: Saw pod success
May 30 07:10:52.729: INFO: Pod "pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a" satisfied condition "Succeeded or Failed"
May 30 07:10:52.732: INFO: Trying to get logs from node my-node pod pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a container secret-volume-test: <nil>
STEP: delete the pod
May 30 07:10:52.832: INFO: Waiting for pod pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a to disappear
May 30 07:10:52.835: INFO: Pod pod-secrets-4e0c7768-2a35-498f-99fd-acd3fb30476a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:10:52.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7347" for this suite.

• [SLOW TEST:8.802 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":168,"skipped":2934,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:10:52.917: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
May 30 07:10:53.030: INFO: Waiting up to 5m0s for pod "var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786" in namespace "var-expansion-7701" to be "Succeeded or Failed"
May 30 07:10:53.047: INFO: Pod "var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786": Phase="Pending", Reason="", readiness=false. Elapsed: 16.093965ms
May 30 07:10:55.133: INFO: Pod "var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102161759s
May 30 07:10:57.139: INFO: Pod "var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108343783s
May 30 07:10:59.284: INFO: Pod "var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.253453546s
STEP: Saw pod success
May 30 07:10:59.285: INFO: Pod "var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786" satisfied condition "Succeeded or Failed"
May 30 07:10:59.327: INFO: Trying to get logs from node my-node pod var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786 container dapi-container: <nil>
STEP: delete the pod
May 30 07:10:59.401: INFO: Waiting for pod var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786 to disappear
May 30 07:10:59.406: INFO: Pod var-expansion-cf3fb364-3cf2-4b2f-87d5-e983281c2786 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:10:59.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7701" for this suite.

• [SLOW TEST:6.505 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":303,"completed":169,"skipped":2940,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:10:59.422: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9690
May 30 07:11:05.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-9690 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 30 07:11:09.089: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 30 07:11:09.089: INFO: stdout: "iptables"
May 30 07:11:09.089: INFO: proxyMode: iptables
May 30 07:11:09.102: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:11:09.355: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:11:11.362: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:11:11.366: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:11:13.362: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:11:13.368: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9690
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9690
I0530 07:11:13.643479      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9690, replica count: 3
I0530 07:11:16.765623      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:11:19.940987      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:11:22.941229      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:11:25.941488      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:11:28.948155      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 07:11:29.049: INFO: Creating new exec pod
May 30 07:11:36.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-9690 exec execpod-affinity4tkhg -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 30 07:11:39.420: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 30 07:11:39.421: INFO: stdout: ""
May 30 07:11:39.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-9690 exec execpod-affinity4tkhg -- /bin/sh -x -c nc -zv -t -w 2 10.102.157.148 80'
May 30 07:11:39.971: INFO: stderr: "+ nc -zv -t -w 2 10.102.157.148 80\nConnection to 10.102.157.148 80 port [tcp/http] succeeded!\n"
May 30 07:11:39.971: INFO: stdout: ""
May 30 07:11:39.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-9690 exec execpod-affinity4tkhg -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.15 30472'
May 30 07:11:40.308: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.15 30472\nConnection to 10.0.2.15 30472 port [tcp/30472] succeeded!\n"
May 30 07:11:40.308: INFO: stdout: ""
May 30 07:11:40.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-9690 exec execpod-affinity4tkhg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.2.15:30472/ ; done'
May 30 07:11:40.919: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n"
May 30 07:11:40.919: INFO: stdout: "\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f\naffinity-nodeport-timeout-dbd9f"
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Received response from host: affinity-nodeport-timeout-dbd9f
May 30 07:11:40.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-9690 exec execpod-affinity4tkhg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.2.15:30472/'
May 30 07:11:41.267: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n"
May 30 07:11:41.268: INFO: stdout: "affinity-nodeport-timeout-dbd9f"
May 30 07:11:56.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-9690 exec execpod-affinity4tkhg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.2.15:30472/'
May 30 07:11:56.587: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.2.15:30472/\n"
May 30 07:11:56.588: INFO: stdout: "affinity-nodeport-timeout-9q6d9"
May 30 07:11:56.589: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9690, will wait for the garbage collector to delete the pods
May 30 07:11:56.674: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 15.421327ms
May 30 07:11:57.475: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 800.322562ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:12:15.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9690" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:76.332 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":303,"completed":170,"skipped":2954,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:12:15.812: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:12:16.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-420" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":303,"completed":171,"skipped":2982,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:12:16.301: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:12:16.417: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31" in namespace "security-context-test-5481" to be "Succeeded or Failed"
May 30 07:12:16.437: INFO: Pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31": Phase="Pending", Reason="", readiness=false. Elapsed: 19.135744ms
May 30 07:12:18.574: INFO: Pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.156972392s
May 30 07:12:20.585: INFO: Pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167198515s
May 30 07:12:23.497: INFO: Pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31": Phase="Pending", Reason="", readiness=false. Elapsed: 7.079230031s
May 30 07:12:25.632: INFO: Pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31": Phase="Pending", Reason="", readiness=false. Elapsed: 9.214594395s
May 30 07:12:27.638: INFO: Pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.220356643s
May 30 07:12:27.638: INFO: Pod "busybox-readonly-false-338219d7-6e2d-454e-ab77-bd5969fa5a31" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:12:27.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5481" for this suite.

• [SLOW TEST:11.377 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":303,"completed":172,"skipped":2996,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:12:28.832: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:12:30.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-6112 create -f -'
May 30 07:12:31.425: INFO: stderr: ""
May 30 07:12:31.425: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 30 07:12:31.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-6112 create -f -'
May 30 07:12:32.066: INFO: stderr: ""
May 30 07:12:32.066: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 30 07:12:33.125: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:33.130: INFO: Found 0 / 1
May 30 07:12:34.242: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:34.242: INFO: Found 0 / 1
May 30 07:12:35.322: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:35.324: INFO: Found 0 / 1
May 30 07:12:36.143: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:36.143: INFO: Found 0 / 1
May 30 07:12:37.125: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:37.125: INFO: Found 0 / 1
May 30 07:12:38.123: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:38.123: INFO: Found 0 / 1
May 30 07:12:39.125: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:39.126: INFO: Found 0 / 1
May 30 07:12:40.132: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:40.133: INFO: Found 0 / 1
May 30 07:12:41.138: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:41.139: INFO: Found 1 / 1
May 30 07:12:41.139: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 30 07:12:41.146: INFO: Selector matched 1 pods for map[app:agnhost]
May 30 07:12:41.146: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 30 07:12:41.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-6112 describe pod agnhost-primary-7vqk9'
May 30 07:12:41.331: INFO: stderr: ""
May 30 07:12:41.331: INFO: stdout: "Name:         agnhost-primary-7vqk9\nNamespace:    kubectl-6112\nPriority:     0\nNode:         my-node/10.0.2.15\nStart Time:   Mon, 30 May 2022 07:12:31 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.0.239\nIPs:\n  IP:           10.244.0.239\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://cfeecf5dbf57ac56f1367cc2a1fdb969b2956e32470a276b91cf240afe0fab78\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 30 May 2022 07:12:40 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4pf96 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-4pf96:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-4pf96\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason       Age   From               Message\n  ----     ------       ----  ----               -------\n  Normal   Scheduled    10s   default-scheduler  Successfully assigned kubectl-6112/agnhost-primary-7vqk9 to my-node\n  Warning  FailedMount  9s    kubelet            MountVolume.SetUp failed for volume \"default-token-4pf96\" : failed to sync secret cache: timed out waiting for the condition\n  Normal   Pulled       4s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal   Created      1s    kubelet            Created container agnhost-primary\n  Normal   Started      1s    kubelet            Started container agnhost-primary\n"
May 30 07:12:41.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-6112 describe rc agnhost-primary'
May 30 07:12:41.562: INFO: stderr: ""
May 30 07:12:41.562: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6112\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  10s   replication-controller  Created pod: agnhost-primary-7vqk9\n"
May 30 07:12:41.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-6112 describe service agnhost-primary'
May 30 07:12:41.708: INFO: stderr: ""
May 30 07:12:41.708: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6112\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.101.127.181\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.0.239:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 30 07:12:41.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-6112 describe node my-node'
May 30 07:12:41.941: INFO: stderr: ""
May 30 07:12:41.941: INFO: stdout: "Name:               my-node\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=my-node\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"ee:c9:e8:56:92:a4\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.2.15\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 27 May 2022 09:42:12 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  my-node\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 30 May 2022 07:12:33 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 30 May 2022 01:39:18 +0000   Mon, 30 May 2022 01:39:18 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 30 May 2022 07:09:47 +0000   Fri, 27 May 2022 09:42:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 30 May 2022 07:09:47 +0000   Fri, 27 May 2022 09:42:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 30 May 2022 07:09:47 +0000   Fri, 27 May 2022 09:42:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 30 May 2022 07:09:47 +0000   Fri, 27 May 2022 09:44:25 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.2.15\n  Hostname:    my-node\nCapacity:\n  cpu:                    3\n  ephemeral-storage:      135601832Ki\n  hugepages-2Mi:          0\n  memory:                 4019932Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    3\n  ephemeral-storage:      124970648165\n  hugepages-2Mi:          0\n  memory:                 3917532Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 30ab1f02f59e4093b9f48497b6753f85\n  System UUID:                a3aa5fdb-d52d-9a42-8fa5-7062181ac666\n  Boot ID:                    67aea70c-c63d-41d9-9999-5d2b2003e8de\n  Kernel Version:             5.13.0-44-generic\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.15\n  Kubelet Version:            v1.19.16\n  Kube-Proxy Version:         v1.19.16\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-f9fd979d6-c9tcq                                    100m (3%)     0 (0%)      70Mi (1%)        170Mi (4%)     2d21h\n  kube-system                 coredns-f9fd979d6-lrzv9                                    100m (3%)     0 (0%)      70Mi (1%)        170Mi (4%)     2d21h\n  kube-system                 etcd-my-node                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d21h\n  kube-system                 kube-apiserver-my-node                                     250m (8%)     0 (0%)      0 (0%)           0 (0%)         2d21h\n  kube-system                 kube-controller-manager-my-node                            200m (6%)     0 (0%)      0 (0%)           0 (0%)         2d21h\n  kube-system                 kube-flannel-ds-lv5nv                                      100m (3%)     100m (3%)   50Mi (1%)        50Mi (1%)      2d21h\n  kube-system                 kube-proxy-4pwdh                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d21h\n  kube-system                 kube-scheduler-my-node                                     100m (3%)     0 (0%)      0 (0%)           0 (0%)         2d21h\n  kubectl-6112                agnhost-primary-7vqk9                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         10s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                    sonobuoy-e2e-job-8e0fe1b369e84dc8                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798    0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    850m (28%)  100m (3%)\n  memory                 190Mi (4%)  390Mi (10%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:                  <none>\n"
May 30 07:12:41.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-6112 describe namespace kubectl-6112'
May 30 07:12:42.085: INFO: stderr: ""
May 30 07:12:42.086: INFO: stdout: "Name:         kubectl-6112\nLabels:       e2e-framework=kubectl\n              e2e-run=105f8b04-2c20-4bce-a6e6-8087de21f14f\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:12:42.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6112" for this suite.

• [SLOW TEST:13.266 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":303,"completed":173,"skipped":3065,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:12:42.145: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9263
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9263
STEP: Creating statefulset with conflicting port in namespace statefulset-9263
STEP: Waiting until pod test-pod will start running in namespace statefulset-9263
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9263
May 30 07:12:50.969: INFO: Observed stateful pod in namespace: statefulset-9263, name: ss-0, uid: f4b1785c-b15d-47c2-a887-d643e53d39fc, status phase: Pending. Waiting for statefulset controller to delete.
May 30 07:12:51.023: INFO: Observed stateful pod in namespace: statefulset-9263, name: ss-0, uid: f4b1785c-b15d-47c2-a887-d643e53d39fc, status phase: Failed. Waiting for statefulset controller to delete.
May 30 07:12:51.040: INFO: Observed stateful pod in namespace: statefulset-9263, name: ss-0, uid: f4b1785c-b15d-47c2-a887-d643e53d39fc, status phase: Failed. Waiting for statefulset controller to delete.
May 30 07:12:51.067: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9263
STEP: Removing pod with conflicting port in namespace statefulset-9263
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9263 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 30 07:13:03.813: INFO: Deleting all statefulset in ns statefulset-9263
May 30 07:13:03.920: INFO: Scaling statefulset ss to 0
May 30 07:13:24.086: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:13:24.107: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:13:24.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9263" for this suite.

• [SLOW TEST:42.105 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":303,"completed":174,"skipped":3080,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:13:24.263: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 07:13:25.158: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 30 07:13:27.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:13:29.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491605, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:13:32.885: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:13:32.889: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1423-crds.webhook.example.com via the AdmissionRegistration API
May 30 07:13:34.369: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:13:35.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7707" for this suite.
STEP: Destroying namespace "webhook-7707-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.663 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":303,"completed":175,"skipped":3165,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:13:35.926: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d8bfa458-2f6f-48df-bc02-2e58d63f1cc1
STEP: Creating a pod to test consume secrets
May 30 07:13:36.078: INFO: Waiting up to 5m0s for pod "pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac" in namespace "secrets-1788" to be "Succeeded or Failed"
May 30 07:13:36.096: INFO: Pod "pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 18.106163ms
May 30 07:13:38.720: INFO: Pod "pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641503961s
May 30 07:13:40.728: INFO: Pod "pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.649387035s
May 30 07:13:42.898: INFO: Pod "pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819469399s
May 30 07:13:44.912: INFO: Pod "pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.833650896s
STEP: Saw pod success
May 30 07:13:44.912: INFO: Pod "pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac" satisfied condition "Succeeded or Failed"
May 30 07:13:44.915: INFO: Trying to get logs from node my-node pod pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac container secret-volume-test: <nil>
STEP: delete the pod
May 30 07:13:45.917: INFO: Waiting for pod pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac to disappear
May 30 07:13:45.939: INFO: Pod pod-secrets-38bd5b0f-8c94-4f23-80ff-045f56dbe4ac no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:13:45.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1788" for this suite.

• [SLOW TEST:10.031 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":176,"skipped":3179,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:13:45.958: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 07:13:46.340: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 30 07:13:48.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:13:50.505: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789491626, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:13:53.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:13:53.539: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:13:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6496" for this suite.
STEP: Destroying namespace "webhook-6496-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.100 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":303,"completed":177,"skipped":3184,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:13:55.060: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4827
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-4827
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4827
May 30 07:13:55.171: INFO: Found 0 stateful pods, waiting for 1
May 30 07:14:05.177: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 30 07:14:05.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:14:05.831: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:14:05.831: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:14:05.831: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:14:05.843: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 30 07:14:15.858: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:14:15.858: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:14:15.964: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:15.964: INFO: ss-0  my-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:15.964: INFO: ss-1           Pending         []
May 30 07:14:15.964: INFO: 
May 30 07:14:15.964: INFO: StatefulSet ss has not reached scale 3, at 2
May 30 07:14:17.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.936403832s
May 30 07:14:18.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.190039594s
May 30 07:14:19.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.154311534s
May 30 07:14:21.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.146045626s
May 30 07:14:22.302: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.605256462s
May 30 07:14:23.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.599739883s
May 30 07:14:24.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.587026584s
May 30 07:14:25.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 581.83286ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4827
May 30 07:14:26.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:14:26.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 30 07:14:26.990: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 07:14:26.990: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 07:14:26.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:14:27.457: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 30 07:14:27.457: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 07:14:27.457: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 07:14:27.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:14:28.010: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 30 07:14:28.010: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 07:14:28.010: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 07:14:28.017: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 07:14:28.017: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 07:14:28.017: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 30 07:14:28.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:14:28.353: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:14:28.353: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:14:28.353: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:14:28.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:14:28.613: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:14:28.613: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:14:28.613: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:14:28.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:14:28.956: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:14:28.956: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:14:28.956: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:14:28.957: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:14:28.962: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 30 07:14:38.970: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:14:38.970: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:14:38.970: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:14:39.058: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:39.058: INFO: ss-0  my-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:39.058: INFO: ss-1  my-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:39.058: INFO: ss-2  my-node  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:39.058: INFO: 
May 30 07:14:39.058: INFO: StatefulSet ss has not reached scale 0, at 3
May 30 07:14:40.349: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:40.349: INFO: ss-0  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:40.349: INFO: ss-1  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:40.349: INFO: ss-2  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:40.349: INFO: 
May 30 07:14:40.349: INFO: StatefulSet ss has not reached scale 0, at 3
May 30 07:14:41.513: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:41.513: INFO: ss-0  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:41.513: INFO: ss-1  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:41.513: INFO: ss-2  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:41.513: INFO: 
May 30 07:14:41.513: INFO: StatefulSet ss has not reached scale 0, at 3
May 30 07:14:42.689: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:42.689: INFO: ss-0  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:42.690: INFO: ss-1  my-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:42.690: INFO: ss-2  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:42.690: INFO: 
May 30 07:14:42.690: INFO: StatefulSet ss has not reached scale 0, at 3
May 30 07:14:44.146: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:44.146: INFO: ss-0  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:44.146: INFO: ss-1  my-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:44.146: INFO: ss-2  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:44.146: INFO: 
May 30 07:14:44.146: INFO: StatefulSet ss has not reached scale 0, at 3
May 30 07:14:45.151: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:45.151: INFO: ss-0  my-node  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:45.151: INFO: ss-1  my-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:45.151: INFO: ss-2  my-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:15 +0000 UTC  }]
May 30 07:14:45.151: INFO: 
May 30 07:14:45.151: INFO: StatefulSet ss has not reached scale 0, at 3
May 30 07:14:46.157: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:46.157: INFO: ss-0  my-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:46.157: INFO: 
May 30 07:14:46.157: INFO: StatefulSet ss has not reached scale 0, at 1
May 30 07:14:47.162: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:47.162: INFO: ss-0  my-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:47.162: INFO: 
May 30 07:14:47.162: INFO: StatefulSet ss has not reached scale 0, at 1
May 30 07:14:48.166: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
May 30 07:14:48.166: INFO: ss-0  my-node  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:14:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-30 07:13:55 +0000 UTC  }]
May 30 07:14:48.167: INFO: 
May 30 07:14:48.167: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4827
May 30 07:14:49.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:14:49.343: INFO: rc: 1
May 30 07:14:49.343: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 30 07:14:59.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:14:59.511: INFO: rc: 1
May 30 07:14:59.511: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:15:09.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:15:09.638: INFO: rc: 1
May 30 07:15:09.638: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:15:19.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:15:19.749: INFO: rc: 1
May 30 07:15:19.750: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:15:29.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:15:29.859: INFO: rc: 1
May 30 07:15:29.859: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:15:39.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:15:39.979: INFO: rc: 1
May 30 07:15:39.979: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:15:49.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:15:50.100: INFO: rc: 1
May 30 07:15:50.100: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:16:00.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:16:00.214: INFO: rc: 1
May 30 07:16:00.214: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:16:10.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:16:10.340: INFO: rc: 1
May 30 07:16:10.340: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:16:20.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:16:20.469: INFO: rc: 1
May 30 07:16:20.469: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:16:30.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:16:30.615: INFO: rc: 1
May 30 07:16:30.615: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:16:40.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:16:40.718: INFO: rc: 1
May 30 07:16:40.718: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:16:50.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:16:50.859: INFO: rc: 1
May 30 07:16:50.859: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:17:00.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:17:01.001: INFO: rc: 1
May 30 07:17:01.001: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:17:11.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:17:11.248: INFO: rc: 1
May 30 07:17:11.248: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:17:21.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:17:21.403: INFO: rc: 1
May 30 07:17:21.403: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:17:31.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:17:32.043: INFO: rc: 1
May 30 07:17:32.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:17:42.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:17:42.178: INFO: rc: 1
May 30 07:17:42.178: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:17:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:17:52.291: INFO: rc: 1
May 30 07:17:52.291: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:18:02.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:18:02.429: INFO: rc: 1
May 30 07:18:02.429: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:18:12.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:18:12.577: INFO: rc: 1
May 30 07:18:12.577: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:18:22.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:18:22.687: INFO: rc: 1
May 30 07:18:22.687: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:18:32.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:18:32.842: INFO: rc: 1
May 30 07:18:32.842: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:18:42.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:18:42.961: INFO: rc: 1
May 30 07:18:42.961: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:18:52.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:18:53.077: INFO: rc: 1
May 30 07:18:53.077: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:19:03.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:19:03.250: INFO: rc: 1
May 30 07:19:03.250: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:19:13.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:19:13.375: INFO: rc: 1
May 30 07:19:13.376: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:19:23.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:19:23.516: INFO: rc: 1
May 30 07:19:23.516: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:19:33.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:19:33.639: INFO: rc: 1
May 30 07:19:33.639: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:19:43.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:19:43.767: INFO: rc: 1
May 30 07:19:43.767: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 30 07:19:53.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-4827 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:19:53.886: INFO: rc: 1
May 30 07:19:53.886: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
May 30 07:19:53.886: INFO: Scaling statefulset ss to 0
May 30 07:19:53.910: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 30 07:19:53.918: INFO: Deleting all statefulset in ns statefulset-4827
May 30 07:19:53.926: INFO: Scaling statefulset ss to 0
May 30 07:19:53.938: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:19:53.941: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:19:53.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4827" for this suite.

• [SLOW TEST:358.908 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":303,"completed":178,"skipped":3217,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:19:54.008: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
May 30 07:19:54.276: INFO: Waiting up to 5m0s for pod "pod-068e6291-a93f-4d97-9693-00f50327e571" in namespace "emptydir-4200" to be "Succeeded or Failed"
May 30 07:19:54.290: INFO: Pod "pod-068e6291-a93f-4d97-9693-00f50327e571": Phase="Pending", Reason="", readiness=false. Elapsed: 13.52978ms
May 30 07:19:56.300: INFO: Pod "pod-068e6291-a93f-4d97-9693-00f50327e571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023877823s
May 30 07:19:58.306: INFO: Pod "pod-068e6291-a93f-4d97-9693-00f50327e571": Phase="Running", Reason="", readiness=true. Elapsed: 4.029913364s
May 30 07:20:00.311: INFO: Pod "pod-068e6291-a93f-4d97-9693-00f50327e571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035199522s
STEP: Saw pod success
May 30 07:20:00.314: INFO: Pod "pod-068e6291-a93f-4d97-9693-00f50327e571" satisfied condition "Succeeded or Failed"
May 30 07:20:00.321: INFO: Trying to get logs from node my-node pod pod-068e6291-a93f-4d97-9693-00f50327e571 container test-container: <nil>
STEP: delete the pod
May 30 07:20:00.404: INFO: Waiting for pod pod-068e6291-a93f-4d97-9693-00f50327e571 to disappear
May 30 07:20:00.410: INFO: Pod pod-068e6291-a93f-4d97-9693-00f50327e571 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:20:00.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4200" for this suite.

• [SLOW TEST:6.417 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":179,"skipped":3223,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:20:00.424: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-95d0fab4-f80f-43e7-94ec-d7babffd938b
STEP: Creating a pod to test consume secrets
May 30 07:20:00.663: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a" in namespace "projected-468" to be "Succeeded or Failed"
May 30 07:20:00.681: INFO: Pod "pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.244994ms
May 30 07:20:02.734: INFO: Pod "pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071706131s
May 30 07:20:04.739: INFO: Pod "pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076475636s
May 30 07:20:06.742: INFO: Pod "pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079624737s
STEP: Saw pod success
May 30 07:20:06.743: INFO: Pod "pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a" satisfied condition "Succeeded or Failed"
May 30 07:20:06.745: INFO: Trying to get logs from node my-node pod pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 30 07:20:06.774: INFO: Waiting for pod pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a to disappear
May 30 07:20:06.779: INFO: Pod pod-projected-secrets-e7f63827-eb19-4108-8e67-d973909b523a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:20:06.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-468" for this suite.

• [SLOW TEST:6.366 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":180,"skipped":3228,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:20:06.792: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
May 30 07:20:06.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 create -f -'
May 30 07:20:07.269: INFO: stderr: ""
May 30 07:20:07.269: INFO: stdout: "pod/pause created\n"
May 30 07:20:07.269: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 30 07:20:07.275: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3067" to be "running and ready"
May 30 07:20:07.343: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 68.328768ms
May 30 07:20:09.359: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084048775s
May 30 07:20:11.798: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.522476009s
May 30 07:20:11.865: INFO: Pod "pause" satisfied condition "running and ready"
May 30 07:20:11.867: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
May 30 07:20:11.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 label pods pause testing-label=testing-label-value'
May 30 07:20:12.139: INFO: stderr: ""
May 30 07:20:12.139: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 30 07:20:12.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 get pod pause -L testing-label'
May 30 07:20:12.459: INFO: stderr: ""
May 30 07:20:12.459: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 30 07:20:12.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 label pods pause testing-label-'
May 30 07:20:12.582: INFO: stderr: ""
May 30 07:20:12.582: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 30 07:20:12.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 get pod pause -L testing-label'
May 30 07:20:12.680: INFO: stderr: ""
May 30 07:20:12.680: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
May 30 07:20:12.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 delete --grace-period=0 --force -f -'
May 30 07:20:12.801: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:20:12.801: INFO: stdout: "pod \"pause\" force deleted\n"
May 30 07:20:12.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 get rc,svc -l name=pause --no-headers'
May 30 07:20:13.002: INFO: stderr: "No resources found in kubectl-3067 namespace.\n"
May 30 07:20:13.002: INFO: stdout: ""
May 30 07:20:13.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-3067 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 30 07:20:13.771: INFO: stderr: ""
May 30 07:20:13.771: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:20:13.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3067" for this suite.

• [SLOW TEST:6.993 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1305
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":303,"completed":181,"skipped":3265,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:20:13.784: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333
May 30 07:20:14.216: INFO: Pod name my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333: Found 0 pods out of 1
May 30 07:20:19.224: INFO: Pod name my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333: Found 1 pods out of 1
May 30 07:20:19.224: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333" are running
May 30 07:20:19.229: INFO: Pod "my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333-nzz8t" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 07:20:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 07:20:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 07:20:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-30 07:20:14 +0000 UTC Reason: Message:}])
May 30 07:20:19.326: INFO: Trying to dial the pod
May 30 07:20:24.401: INFO: Controller my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333: Got expected result from replica 1 [my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333-nzz8t]: "my-hostname-basic-e89f6a21-22c1-4942-9480-849bb1292333-nzz8t", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:20:24.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5310" for this suite.

• [SLOW TEST:10.627 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":303,"completed":182,"skipped":3269,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:20:24.412: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:20:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:20:29.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-537" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":303,"completed":183,"skipped":3274,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:20:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 30 07:20:29.077: INFO: Waiting up to 5m0s for pod "pod-f4055437-c397-42c7-ac23-355c1ab77774" in namespace "emptydir-2637" to be "Succeeded or Failed"
May 30 07:20:29.094: INFO: Pod "pod-f4055437-c397-42c7-ac23-355c1ab77774": Phase="Pending", Reason="", readiness=false. Elapsed: 16.600248ms
May 30 07:20:31.206: INFO: Pod "pod-f4055437-c397-42c7-ac23-355c1ab77774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12892742s
May 30 07:20:33.226: INFO: Pod "pod-f4055437-c397-42c7-ac23-355c1ab77774": Phase="Pending", Reason="", readiness=false. Elapsed: 4.149294596s
May 30 07:20:35.837: INFO: Pod "pod-f4055437-c397-42c7-ac23-355c1ab77774": Phase="Pending", Reason="", readiness=false. Elapsed: 6.759946906s
May 30 07:20:37.845: INFO: Pod "pod-f4055437-c397-42c7-ac23-355c1ab77774": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.767393789s
STEP: Saw pod success
May 30 07:20:37.846: INFO: Pod "pod-f4055437-c397-42c7-ac23-355c1ab77774" satisfied condition "Succeeded or Failed"
May 30 07:20:37.858: INFO: Trying to get logs from node my-node pod pod-f4055437-c397-42c7-ac23-355c1ab77774 container test-container: <nil>
STEP: delete the pod
May 30 07:20:38.456: INFO: Waiting for pod pod-f4055437-c397-42c7-ac23-355c1ab77774 to disappear
May 30 07:20:38.466: INFO: Pod pod-f4055437-c397-42c7-ac23-355c1ab77774 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:20:38.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2637" for this suite.

• [SLOW TEST:9.468 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":184,"skipped":3287,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:20:38.496: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:20:43.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8913" for this suite.

• [SLOW TEST:5.137 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":303,"completed":185,"skipped":3306,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:20:43.633: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8718
May 30 07:20:47.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 30 07:20:48.231: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 30 07:20:48.231: INFO: stdout: "iptables"
May 30 07:20:48.231: INFO: proxyMode: iptables
May 30 07:20:48.240: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:20:48.246: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:20:50.250: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:20:50.253: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:20:52.247: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:20:52.254: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:20:54.248: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:20:54.252: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:20:56.246: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:20:56.253: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:20:58.247: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:20:58.253: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:21:00.247: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:21:00.251: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:21:02.246: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:21:02.251: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:21:04.247: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:21:04.259: INFO: Pod kube-proxy-mode-detector still exists
May 30 07:21:06.249: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 30 07:21:06.648: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-8718
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8718
I0530 07:21:06.836611      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8718, replica count: 3
I0530 07:21:09.890907      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:21:12.913706      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:21:15.915376      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 07:21:15.923: INFO: Creating new exec pod
May 30 07:21:20.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec execpod-affinityk4fx4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 30 07:21:21.446: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 30 07:21:21.446: INFO: stdout: ""
May 30 07:21:21.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec execpod-affinityk4fx4 -- /bin/sh -x -c nc -zv -t -w 2 10.101.197.161 80'
May 30 07:21:21.738: INFO: stderr: "+ nc -zv -t -w 2 10.101.197.161 80\nConnection to 10.101.197.161 80 port [tcp/http] succeeded!\n"
May 30 07:21:21.738: INFO: stdout: ""
May 30 07:21:21.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec execpod-affinityk4fx4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.197.161:80/ ; done'
May 30 07:21:22.230: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n"
May 30 07:21:22.230: INFO: stdout: "\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6\naffinity-clusterip-timeout-bslg6"
May 30 07:21:22.230: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.230: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.230: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Received response from host: affinity-clusterip-timeout-bslg6
May 30 07:21:22.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec execpod-affinityk4fx4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.101.197.161:80/'
May 30 07:21:23.090: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n"
May 30 07:21:23.090: INFO: stdout: "affinity-clusterip-timeout-bslg6"
May 30 07:21:38.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec execpod-affinityk4fx4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.101.197.161:80/'
May 30 07:21:38.421: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n"
May 30 07:21:38.421: INFO: stdout: "affinity-clusterip-timeout-bslg6"
May 30 07:21:53.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec execpod-affinityk4fx4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.101.197.161:80/'
May 30 07:21:53.717: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n"
May 30 07:21:53.717: INFO: stdout: "affinity-clusterip-timeout-bslg6"
May 30 07:22:08.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-8718 exec execpod-affinityk4fx4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.101.197.161:80/'
May 30 07:22:08.992: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.101.197.161:80/\n"
May 30 07:22:08.992: INFO: stdout: "affinity-clusterip-timeout-54zv8"
May 30 07:22:08.992: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8718, will wait for the garbage collector to delete the pods
May 30 07:22:09.171: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 16.646277ms
May 30 07:22:09.772: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.773448ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:22:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8718" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:101.947 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":186,"skipped":3316,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:22:25.582: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:22:25.675: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c" in namespace "security-context-test-7599" to be "Succeeded or Failed"
May 30 07:22:25.693: INFO: Pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.088856ms
May 30 07:22:28.211: INFO: Pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534434029s
May 30 07:22:30.218: INFO: Pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.541363582s
May 30 07:22:32.225: INFO: Pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54810162s
May 30 07:22:34.276: INFO: Pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.598778369s
May 30 07:22:36.584: INFO: Pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.907275572s
May 30 07:22:36.584: INFO: Pod "alpine-nnp-false-966ed8a0-b143-4ddd-bed4-e81583b8472c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:22:36.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7599" for this suite.

• [SLOW TEST:11.256 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":187,"skipped":3319,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:22:36.843: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6662.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6662.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 07:22:47.126: INFO: DNS probes using dns-6662/dns-test-54397a7b-1958-49fe-8b7f-7ba54e919b78 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:22:47.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6662" for this suite.

• [SLOW TEST:10.351 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":303,"completed":188,"skipped":3327,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:22:47.194: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 30 07:22:55.634: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:22:56.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-978" for this suite.

• [SLOW TEST:9.305 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":189,"skipped":3352,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:22:56.501: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:24:56.676: INFO: Deleting pod "var-expansion-dd080256-06f8-4225-a557-fba519aaaa84" in namespace "var-expansion-3046"
May 30 07:24:56.684: INFO: Wait up to 5m0s for pod "var-expansion-dd080256-06f8-4225-a557-fba519aaaa84" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:25:00.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3046" for this suite.

• [SLOW TEST:124.219 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":303,"completed":190,"skipped":3375,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:25:00.734: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 30 07:25:07.358: INFO: Successfully updated pod "pod-update-1bce3d94-4edf-4bb0-86e0-fd6efcc265d2"
STEP: verifying the updated pod is in kubernetes
May 30 07:25:07.366: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:25:07.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2113" for this suite.

• [SLOW TEST:6.686 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":303,"completed":191,"skipped":3384,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:25:07.419: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:25:07.560: INFO: Creating deployment "webserver-deployment"
May 30 07:25:07.576: INFO: Waiting for observed generation 1
May 30 07:25:10.052: INFO: Waiting for all required pods to come up
May 30 07:25:10.090: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 30 07:25:28.277: INFO: Waiting for deployment "webserver-deployment" to complete
May 30 07:25:28.283: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 30 07:25:28.790: INFO: Updating deployment webserver-deployment
May 30 07:25:28.791: INFO: Waiting for observed generation 2
May 30 07:25:30.927: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 30 07:25:30.931: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 30 07:25:30.934: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 30 07:25:30.944: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 30 07:25:30.944: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 30 07:25:30.946: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 30 07:25:30.951: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 30 07:25:30.951: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 30 07:25:30.958: INFO: Updating deployment webserver-deployment
May 30 07:25:30.958: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 30 07:25:31.148: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 30 07:25:31.698: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 30 07:25:32.979: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-522 /apis/apps/v1/namespaces/deployment-522/deployments/webserver-deployment 1b8994b4-5f8f-4d0f-b159-9fce88cd306d 58820 3 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-05-30 07:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00377dab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:21,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-30 07:25:31 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-05-30 07:25:32 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 30 07:25:33.516: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-522 /apis/apps/v1/namespaces/deployment-522/replicasets/webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 58815 3 2022-05-30 07:25:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1b8994b4-5f8f-4d0f-b159-9fce88cd306d 0xc000e357f7 0xc000e357f8}] []  [{kube-controller-manager Update apps/v1 2022-05-30 07:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b8994b4-5f8f-4d0f-b159-9fce88cd306d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e35878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 30 07:25:33.516: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 30 07:25:33.516: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-522 /apis/apps/v1/namespaces/deployment-522/replicasets/webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 58816 3 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1b8994b4-5f8f-4d0f-b159-9fce88cd306d 0xc000e358d7 0xc000e358d8}] []  [{kube-controller-manager Update apps/v1 2022-05-30 07:25:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b8994b4-5f8f-4d0f-b159-9fce88cd306d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e35948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 30 07:25:34.065: INFO: Pod "webserver-deployment-795d758f88-4lbzp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4lbzp webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-4lbzp 1009452e-240c-4252-9b12-635ae8797292 58781 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86127 0xc002e86128}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.065: INFO: Pod "webserver-deployment-795d758f88-6f8h7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6f8h7 webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-6f8h7 b6de1353-64f5-4fa1-959c-ac0bf7a83a8e 58801 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86270 0xc002e86271}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.066: INFO: Pod "webserver-deployment-795d758f88-76f7b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-76f7b webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-76f7b 87da676c-a203-48a4-9f9a-ff47575d9d5c 58799 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e863b0 0xc002e863b1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.066: INFO: Pod "webserver-deployment-795d758f88-8fxqt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8fxqt webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-8fxqt 4d37d7bd-5cee-467e-9d19-9d9e2c0f6501 58829 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e864f0 0xc002e864f1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.067: INFO: Pod "webserver-deployment-795d758f88-8pbls" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8pbls webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-8pbls 0b3c0b1d-d136-4771-b86c-df0e6ab67a2e 58726 0 2022-05-30 07:25:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86690 0xc002e86691}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.068: INFO: Pod "webserver-deployment-795d758f88-9h6hh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9h6hh webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-9h6hh 14edbb4a-9eff-4045-94bc-949b08650d60 58811 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86830 0xc002e86831}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.068: INFO: Pod "webserver-deployment-795d758f88-9ps4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9ps4k webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-9ps4k a0e15aeb-34cf-41b6-b84b-7782dcec4e99 58737 0 2022-05-30 07:25:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86970 0xc002e86971}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.069: INFO: Pod "webserver-deployment-795d758f88-jsm9z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jsm9z webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-jsm9z 65b2fcd3-0d77-4eca-8276-efa02b10c123 58756 0 2022-05-30 07:25:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86b10 0xc002e86b11}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.069: INFO: Pod "webserver-deployment-795d758f88-ksplb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ksplb webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-ksplb f93f6a96-4b68-4923-9c6d-621ee4468383 58804 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86ce0 0xc002e86ce1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.069: INFO: Pod "webserver-deployment-795d758f88-npg2l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-npg2l webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-npg2l fb76830a-4d7f-47a6-8fcb-47933aec8def 58803 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86e20 0xc002e86e21}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.070: INFO: Pod "webserver-deployment-795d758f88-t5j45" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t5j45 webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-t5j45 2c78d1f6-f327-4c24-8efa-93d0458e5b62 58790 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e86f60 0xc002e86f61}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.070: INFO: Pod "webserver-deployment-795d758f88-thfwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-thfwj webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-thfwj e47d6489-bf5c-4cc4-9140-3ea368e22a8e 58755 0 2022-05-30 07:25:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e870b0 0xc002e870b1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.070: INFO: Pod "webserver-deployment-795d758f88-zdxmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zdxmx webserver-deployment-795d758f88- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-795d758f88-zdxmx f66110f0-2db1-45f1-88f7-1f6ea02dc322 58753 0 2022-05-30 07:25:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e3e2a94f-41bb-4be0-afd0-b93c98389aed 0xc002e87250 0xc002e87251}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3e2a94f-41bb-4be0-afd0-b93c98389aed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.071: INFO: Pod "webserver-deployment-dd94f59b7-25ndk" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-25ndk webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-25ndk 5c4ac7f6-0cd0-4cb6-9c2e-bd12bc5638ab 58674 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e873f0 0xc002e873f1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.21,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e28ac83316d32a8e3edb43b33cfe4f0342407ce26d738623898ed3bf2a7eb632,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.071: INFO: Pod "webserver-deployment-dd94f59b7-4n7cl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4n7cl webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-4n7cl 0eb37555-11cb-4a17-b045-cc3c8ee9b41b 58639 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e87590 0xc002e87591}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.15,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://064c92fe686ee1f70a8391fda76a2fd3b8c728e5fc14d99d181a15ed63e8cd10,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.071: INFO: Pod "webserver-deployment-dd94f59b7-5r67m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5r67m webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-5r67m 59ae1f55-9ebd-46b5-a17e-6a5da08da9d2 58662 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e87750 0xc002e87751}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.16,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b45d91e164da74286e2eda3ca00529b82fe3b2d9315ff7bc4ae2c9cd1f5eccaa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.072: INFO: Pod "webserver-deployment-dd94f59b7-7zmdb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7zmdb webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-7zmdb 19b03fc7-dbf5-46a7-910d-c17169e4faf7 58806 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e878f0 0xc002e878f1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.072: INFO: Pod "webserver-deployment-dd94f59b7-8xfpb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8xfpb webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-8xfpb f752e9c7-997b-426d-b7b5-b781a0b915ae 58668 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e87a20 0xc002e87a21}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.23,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://01bed7de9885ff8ed652150c71901638cb51cbbe5e8a61169c0657b45a6deef9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.072: INFO: Pod "webserver-deployment-dd94f59b7-9s6b2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9s6b2 webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-9s6b2 f5037bdc-2daf-4489-a708-f3a33439f507 58779 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e87bc0 0xc002e87bc1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.072: INFO: Pod "webserver-deployment-dd94f59b7-bltbx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bltbx webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-bltbx 52fa89fd-d942-4080-bdc1-67e88d5e253c 58813 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e87cf0 0xc002e87cf1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.072: INFO: Pod "webserver-deployment-dd94f59b7-cnwnm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cnwnm webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-cnwnm 700027f8-3bbc-468e-a1ed-18b9786c2f5b 58812 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e87e20 0xc002e87e21}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.072: INFO: Pod "webserver-deployment-dd94f59b7-dbq6p" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dbq6p webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-dbq6p f8a131e0-69de-40fe-926d-91383ede2863 58677 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc002e87f50 0xc002e87f51}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.19,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://949a430d84aa7ee8ad803c8cf212631f5af92614677e78e8848205b0dba225ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.072: INFO: Pod "webserver-deployment-dd94f59b7-gkb2c" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gkb2c webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-gkb2c 2d3ff610-a531-4f31-a5df-1f2021327f64 58671 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dc100 0xc0034dc101}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.17,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ff65d4817e726515c4ce3071fd01b8d63ac8e50fef2c55e41a69acfeb6fbf421,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.073: INFO: Pod "webserver-deployment-dd94f59b7-gw4x8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gw4x8 webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-gw4x8 446c2c33-c5b5-47b0-91b9-bb8407671f6b 58810 0 2022-05-30 07:25:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dc2d0 0xc0034dc2d1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.073: INFO: Pod "webserver-deployment-dd94f59b7-knwgz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-knwgz webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-knwgz 5fa2dfff-9ad4-4688-9abf-50104b381d8d 58782 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dc6d0 0xc0034dc6d1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.073: INFO: Pod "webserver-deployment-dd94f59b7-ljqnc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ljqnc webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-ljqnc 5ba884b8-25f8-4807-81ef-959a1b9f5e6c 58778 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dc800 0xc0034dc801}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.073: INFO: Pod "webserver-deployment-dd94f59b7-mnjbt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mnjbt webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-mnjbt e835e0f8-f2c6-42cf-a95c-850edfdf4e97 58824 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dcaf0 0xc0034dcaf1}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:,StartTime:2022-05-30 07:25:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.073: INFO: Pod "webserver-deployment-dd94f59b7-n4lsb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n4lsb webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-n4lsb fbb0ec56-cfbc-475b-9382-60ddbeebda5e 58805 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dce20 0xc0034dce21}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.074: INFO: Pod "webserver-deployment-dd94f59b7-wczfv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wczfv webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-wczfv 74badf93-42b3-4bc3-9768-65bf4576515e 58800 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dd050 0xc0034dd051}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.074: INFO: Pod "webserver-deployment-dd94f59b7-xdls2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xdls2 webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-xdls2 5c191225-bf28-4242-9674-f70512eacfc4 58692 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dd320 0xc0034dd321}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.18,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://55f678209dfb75b44db0cebc974992b64845017556bee571308da3dee3d6a44f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.074: INFO: Pod "webserver-deployment-dd94f59b7-z22nl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z22nl webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-z22nl 6541b31d-6527-41a3-8e9b-bb0164d2b555 58680 0 2022-05-30 07:25:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0034dd670 0xc0034dd671}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:25:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.22,StartTime:2022-05-30 07:25:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:25:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9c45a6d3d2e3a08e9b6bed205e5e727e5747fbf2e745c32e2657bdf7bd51d13f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.074: INFO: Pod "webserver-deployment-dd94f59b7-zqmfx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zqmfx webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-zqmfx 01a8d881-bdeb-4f5a-b92f-950e53534ebf 58780 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0030fe150 0xc0030fe151}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:25:34.074: INFO: Pod "webserver-deployment-dd94f59b7-zzpjb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zzpjb webserver-deployment-dd94f59b7- deployment-522 /api/v1/namespaces/deployment-522/pods/webserver-deployment-dd94f59b7-zzpjb 59453ffe-d156-4408-b0ec-6007c03fbd69 58777 0 2022-05-30 07:25:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 25bc26db-9a5e-4cf9-a797-19ef703fc961 0xc0030fe290 0xc0030fe291}] []  [{kube-controller-manager Update v1 2022-05-30 07:25:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25bc26db-9a5e-4cf9-a797-19ef703fc961\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p4fs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p4fs4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p4fs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:25:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:25:34.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-522" for this suite.

• [SLOW TEST:26.944 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":303,"completed":192,"skipped":3412,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:25:34.372: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:26:47.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5284" for this suite.

• [SLOW TEST:73.243 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":303,"completed":193,"skipped":3442,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:26:47.722: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
May 30 07:26:48.679: INFO: created pod pod-service-account-defaultsa
May 30 07:26:48.684: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 30 07:26:48.691: INFO: created pod pod-service-account-mountsa
May 30 07:26:48.692: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 30 07:26:48.709: INFO: created pod pod-service-account-nomountsa
May 30 07:26:48.710: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 30 07:26:48.717: INFO: created pod pod-service-account-defaultsa-mountspec
May 30 07:26:48.717: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 30 07:26:48.845: INFO: created pod pod-service-account-mountsa-mountspec
May 30 07:26:48.846: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 30 07:26:48.994: INFO: created pod pod-service-account-nomountsa-mountspec
May 30 07:26:48.994: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 30 07:26:49.022: INFO: created pod pod-service-account-defaultsa-nomountspec
May 30 07:26:49.022: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 30 07:26:49.053: INFO: created pod pod-service-account-mountsa-nomountspec
May 30 07:26:49.053: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 30 07:26:49.282: INFO: created pod pod-service-account-nomountsa-nomountspec
May 30 07:26:49.282: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:26:49.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4966" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":303,"completed":194,"skipped":3453,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:26:49.725: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 07:26:55.549: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 30 07:26:59.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:02.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:03.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:05.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:08.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:09.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:11.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:13.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:27:15.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492417, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492416, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:27:18.281: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:27:19.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2944" for this suite.
STEP: Destroying namespace "webhook-2944-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:29.495 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":303,"completed":195,"skipped":3456,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:27:19.237: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 30 07:27:19.302: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 30 07:27:19.338: INFO: Waiting for terminating namespaces to be deleted...
May 30 07:27:19.357: INFO: 
Logging pods the apiserver thinks is on node my-node before test
May 30 07:27:19.447: INFO: coredns-f9fd979d6-c9tcq from kube-system started at 2022-05-27 09:44:35 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.447: INFO: 	Container coredns ready: true, restart count 1
May 30 07:27:19.448: INFO: coredns-f9fd979d6-lrzv9 from kube-system started at 2022-05-27 09:44:25 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.449: INFO: 	Container coredns ready: true, restart count 1
May 30 07:27:19.449: INFO: etcd-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container etcd ready: true, restart count 1
May 30 07:27:19.450: INFO: kube-apiserver-my-node from kube-system started at 2022-05-30 01:38:05 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container kube-apiserver ready: true, restart count 1
May 30 07:27:19.450: INFO: kube-controller-manager-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 07:27:19.450: INFO: kube-flannel-ds-lv5nv from kube-system started at 2022-05-27 09:43:53 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container kube-flannel ready: true, restart count 1
May 30 07:27:19.450: INFO: kube-proxy-4pwdh from kube-system started at 2022-05-27 09:42:33 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 07:27:19.450: INFO: kube-scheduler-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 07:27:19.450: INFO: busybox-scheduling-3c363a6d-5610-4db8-a9a1-6bcbbc7368f7 from kubelet-test-5284 started at 2022-05-30 07:25:37 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container busybox-scheduling-3c363a6d-5610-4db8-a9a1-6bcbbc7368f7 ready: true, restart count 0
May 30 07:27:19.450: INFO: sonobuoy from sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 30 07:27:19.450: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 from sonobuoy started at 2022-05-30 06:08:24 +0000 UTC (2 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container e2e ready: true, restart count 0
May 30 07:27:19.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:27:19.450: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 from sonobuoy started at 2022-05-30 06:08:25 +0000 UTC (2 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:27:19.450: INFO: 	Container systemd-logs ready: true, restart count 0
May 30 07:27:19.450: INFO: sample-webhook-deployment-cbccbf6bb-xnwrf from webhook-2944 started at 2022-05-30 07:26:57 +0000 UTC (1 container statuses recorded)
May 30 07:27:19.450: INFO: 	Container sample-webhook ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node my-node
May 30 07:27:19.804: INFO: Pod coredns-f9fd979d6-c9tcq requesting resource cpu=100m on Node my-node
May 30 07:27:19.805: INFO: Pod coredns-f9fd979d6-lrzv9 requesting resource cpu=100m on Node my-node
May 30 07:27:19.806: INFO: Pod etcd-my-node requesting resource cpu=0m on Node my-node
May 30 07:27:19.806: INFO: Pod kube-apiserver-my-node requesting resource cpu=250m on Node my-node
May 30 07:27:19.807: INFO: Pod kube-controller-manager-my-node requesting resource cpu=200m on Node my-node
May 30 07:27:19.807: INFO: Pod kube-flannel-ds-lv5nv requesting resource cpu=100m on Node my-node
May 30 07:27:19.807: INFO: Pod kube-proxy-4pwdh requesting resource cpu=0m on Node my-node
May 30 07:27:19.807: INFO: Pod kube-scheduler-my-node requesting resource cpu=100m on Node my-node
May 30 07:27:19.807: INFO: Pod busybox-scheduling-3c363a6d-5610-4db8-a9a1-6bcbbc7368f7 requesting resource cpu=0m on Node my-node
May 30 07:27:19.807: INFO: Pod sonobuoy requesting resource cpu=0m on Node my-node
May 30 07:27:19.807: INFO: Pod sonobuoy-e2e-job-8e0fe1b369e84dc8 requesting resource cpu=0m on Node my-node
May 30 07:27:19.807: INFO: Pod sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 requesting resource cpu=0m on Node my-node
May 30 07:27:19.807: INFO: Pod sample-webhook-deployment-cbccbf6bb-xnwrf requesting resource cpu=0m on Node my-node
STEP: Starting Pods to consume most of the cluster CPU.
May 30 07:27:19.807: INFO: Creating a pod which consumes cpu=1505m on Node my-node
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d67b5845-f116-4213-8194-76761cbd9c63.16f3d15264c96ff8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4308/filler-pod-d67b5845-f116-4213-8194-76761cbd9c63 to my-node]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d67b5845-f116-4213-8194-76761cbd9c63.16f3d1533ec80e88], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d67b5845-f116-4213-8194-76761cbd9c63.16f3d1538d12115f], Reason = [Created], Message = [Created container filler-pod-d67b5845-f116-4213-8194-76761cbd9c63]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d67b5845-f116-4213-8194-76761cbd9c63.16f3d153a0015869], Reason = [Started], Message = [Started container filler-pod-d67b5845-f116-4213-8194-76761cbd9c63]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16f3d153ccdb4109], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu.]
STEP: removing the label node off the node my-node
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:27:27.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4308" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.477 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":303,"completed":196,"skipped":3535,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:27:27.714: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-ee57b71c-6353-4e41-8ed0-5003d4a6b1b0
STEP: Creating a pod to test consume secrets
May 30 07:27:27.832: INFO: Waiting up to 5m0s for pod "pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e" in namespace "secrets-5" to be "Succeeded or Failed"
May 30 07:27:27.840: INFO: Pod "pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516643ms
May 30 07:27:29.956: INFO: Pod "pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123132838s
May 30 07:27:32.113: INFO: Pod "pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280037427s
May 30 07:27:34.493: INFO: Pod "pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.660405979s
May 30 07:27:37.087: INFO: Pod "pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.254904079s
STEP: Saw pod success
May 30 07:27:37.087: INFO: Pod "pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e" satisfied condition "Succeeded or Failed"
May 30 07:27:37.329: INFO: Trying to get logs from node my-node pod pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e container secret-volume-test: <nil>
STEP: delete the pod
May 30 07:27:37.625: INFO: Waiting for pod pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e to disappear
May 30 07:27:37.636: INFO: Pod pod-secrets-4bf6f67c-3793-4b0e-92b8-230d6443823e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:27:37.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5" for this suite.

• [SLOW TEST:9.946 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":197,"skipped":3562,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:27:37.660: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
May 30 07:27:37.887: INFO: created test-pod-1
May 30 07:27:37.897: INFO: created test-pod-2
May 30 07:27:37.920: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:27:38.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4593" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":303,"completed":198,"skipped":3576,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:27:38.402: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 30 07:27:39.136: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 30 07:27:39.437: INFO: Waiting for terminating namespaces to be deleted...
May 30 07:27:39.443: INFO: 
Logging pods the apiserver thinks is on node my-node before test
May 30 07:27:40.055: INFO: coredns-f9fd979d6-c9tcq from kube-system started at 2022-05-27 09:44:35 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container coredns ready: true, restart count 1
May 30 07:27:40.055: INFO: coredns-f9fd979d6-lrzv9 from kube-system started at 2022-05-27 09:44:25 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container coredns ready: true, restart count 1
May 30 07:27:40.055: INFO: etcd-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container etcd ready: true, restart count 1
May 30 07:27:40.055: INFO: kube-apiserver-my-node from kube-system started at 2022-05-30 01:38:05 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container kube-apiserver ready: true, restart count 1
May 30 07:27:40.055: INFO: kube-controller-manager-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 07:27:40.055: INFO: kube-flannel-ds-lv5nv from kube-system started at 2022-05-27 09:43:53 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container kube-flannel ready: true, restart count 1
May 30 07:27:40.055: INFO: kube-proxy-4pwdh from kube-system started at 2022-05-27 09:42:33 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 07:27:40.055: INFO: kube-scheduler-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.055: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 07:27:40.057: INFO: sonobuoy from sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (1 container statuses recorded)
May 30 07:27:40.057: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 30 07:27:40.057: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 from sonobuoy started at 2022-05-30 06:08:24 +0000 UTC (2 container statuses recorded)
May 30 07:27:40.057: INFO: 	Container e2e ready: true, restart count 0
May 30 07:27:40.057: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:27:40.057: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 from sonobuoy started at 2022-05-30 06:08:25 +0000 UTC (2 container statuses recorded)
May 30 07:27:40.057: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:27:40.057: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5707e598-cbfb-4d1c-b072-0d1ba275d218 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-5707e598-cbfb-4d1c-b072-0d1ba275d218 off the node my-node
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5707e598-cbfb-4d1c-b072-0d1ba275d218
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:33:00.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9534" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:322.292 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":303,"completed":199,"skipped":3579,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:33:00.736: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1915
STEP: creating service affinity-clusterip-transition in namespace services-1915
STEP: creating replication controller affinity-clusterip-transition in namespace services-1915
I0530 07:33:01.041544      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1915, replica count: 3
I0530 07:33:04.107000      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:33:07.121979      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:33:10.143831      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:33:13.144590      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 07:33:13.154: INFO: Creating new exec pod
May 30 07:33:18.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-1915 exec execpod-affinityg4jn8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 30 07:33:20.655: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 30 07:33:20.655: INFO: stdout: ""
May 30 07:33:20.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-1915 exec execpod-affinityg4jn8 -- /bin/sh -x -c nc -zv -t -w 2 10.101.206.232 80'
May 30 07:33:21.027: INFO: stderr: "+ nc -zv -t -w 2 10.101.206.232 80\nConnection to 10.101.206.232 80 port [tcp/http] succeeded!\n"
May 30 07:33:21.027: INFO: stdout: ""
May 30 07:33:21.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-1915 exec execpod-affinityg4jn8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.206.232:80/ ; done'
May 30 07:33:21.668: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n"
May 30 07:33:21.668: INFO: stdout: "\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zz84x\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-j2jgg\naffinity-clusterip-transition-j2jgg\naffinity-clusterip-transition-zz84x\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zz84x\naffinity-clusterip-transition-j2jgg\naffinity-clusterip-transition-zz84x\naffinity-clusterip-transition-zz84x\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-j2jgg\naffinity-clusterip-transition-j2jgg"
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zz84x
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-j2jgg
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-j2jgg
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zz84x
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zz84x
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-j2jgg
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zz84x
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zz84x
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-j2jgg
May 30 07:33:21.669: INFO: Received response from host: affinity-clusterip-transition-j2jgg
May 30 07:33:21.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-1915 exec execpod-affinityg4jn8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.206.232:80/ ; done'
May 30 07:33:22.110: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.206.232:80/\n"
May 30 07:33:22.110: INFO: stdout: "\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w\naffinity-clusterip-transition-zsj9w"
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Received response from host: affinity-clusterip-transition-zsj9w
May 30 07:33:22.110: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1915, will wait for the garbage collector to delete the pods
May 30 07:33:22.216: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.868814ms
May 30 07:33:23.317: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.100999257s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:33:35.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1915" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:34.836 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":200,"skipped":3649,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:33:35.576: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6242
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 30 07:33:35.698: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 30 07:33:35.741: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:33:37.748: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:33:39.747: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:33:41.746: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 07:33:43.745: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 07:33:45.745: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 07:33:47.745: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 07:33:49.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 30 07:33:51.748: INFO: The status of Pod netserver-0 is Running (Ready = true)
STEP: Creating test pods
May 30 07:33:59.840: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6242 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 07:33:59.840: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 07:34:01.320: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:34:01.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6242" for this suite.

• [SLOW TEST:25.772 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":201,"skipped":3684,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:34:01.419: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-4f8c719e-89e6-46f2-a411-036abb4e2d6e
STEP: Creating a pod to test consume configMaps
May 30 07:34:01.572: INFO: Waiting up to 5m0s for pod "pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6" in namespace "configmap-2928" to be "Succeeded or Failed"
May 30 07:34:01.576: INFO: Pod "pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.397257ms
May 30 07:34:04.142: INFO: Pod "pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570867653s
May 30 07:34:06.146: INFO: Pod "pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.574948864s
STEP: Saw pod success
May 30 07:34:06.147: INFO: Pod "pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6" satisfied condition "Succeeded or Failed"
May 30 07:34:06.166: INFO: Trying to get logs from node my-node pod pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6 container configmap-volume-test: <nil>
STEP: delete the pod
May 30 07:34:06.214: INFO: Waiting for pod pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6 to disappear
May 30 07:34:06.222: INFO: Pod pod-configmaps-372bc715-7830-449d-8740-a4c77dc428f6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:34:06.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2928" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":202,"skipped":3718,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:34:06.246: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
May 30 07:34:06.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-7497 cluster-info'
May 30 07:34:06.680: INFO: stderr: ""
May 30 07:34:06.680: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:34:06.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7497" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":303,"completed":203,"skipped":3721,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:34:07.581: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:34:07.783: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 30 07:34:12.996: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 30 07:34:17.392: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 30 07:34:19.396: INFO: Creating deployment "test-rollover-deployment"
May 30 07:34:19.417: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 30 07:34:21.428: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 30 07:34:21.451: INFO: Ensure that both replica sets have 1 created replica
May 30 07:34:21.488: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 30 07:34:21.503: INFO: Updating deployment test-rollover-deployment
May 30 07:34:21.505: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 30 07:34:23.631: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 30 07:34:23.701: INFO: Make sure deployment "test-rollover-deployment" is complete
May 30 07:34:23.720: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:23.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492861, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:25.730: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:25.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492861, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:27.727: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:27.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492861, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:29.728: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:29.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492867, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:31.834: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:31.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492867, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:33.732: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:33.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492867, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:35.765: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:35.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492867, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:37.731: INFO: all replica sets need to contain the pod-template-hash label
May 30 07:34:37.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492867, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789492859, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:34:39.734: INFO: 
May 30 07:34:39.734: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 30 07:34:40.257: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4129 /apis/apps/v1/namespaces/deployment-4129/deployments/test-rollover-deployment 37a5f889-83fa-4c2a-85c1-f6296dd75fa5 60797 2 2022-05-30 07:34:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-30 07:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-05-30 07:34:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00377c288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-30 07:34:19 +0000 UTC,LastTransitionTime:2022-05-30 07:34:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2022-05-30 07:34:38 +0000 UTC,LastTransitionTime:2022-05-30 07:34:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 30 07:34:40.382: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-4129 /apis/apps/v1/namespaces/deployment-4129/replicasets/test-rollover-deployment-5797c7764 baed66cd-5974-4d01-8f71-36f6dc5bc23b 60786 2 2022-05-30 07:34:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 37a5f889-83fa-4c2a-85c1-f6296dd75fa5 0xc00377c7e0 0xc00377c7e1}] []  [{kube-controller-manager Update apps/v1 2022-05-30 07:34:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37a5f889-83fa-4c2a-85c1-f6296dd75fa5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00377c858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 30 07:34:40.382: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 30 07:34:40.382: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4129 /apis/apps/v1/namespaces/deployment-4129/replicasets/test-rollover-controller 315d7c95-3d34-4e90-9159-7d4553ac1a83 60796 2 2022-05-30 07:34:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 37a5f889-83fa-4c2a-85c1-f6296dd75fa5 0xc00377c6b7 0xc00377c6b8}] []  [{e2e.test Update apps/v1 2022-05-30 07:34:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-05-30 07:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37a5f889-83fa-4c2a-85c1-f6296dd75fa5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00377c768 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 30 07:34:40.382: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4129 /apis/apps/v1/namespaces/deployment-4129/replicasets/test-rollover-deployment-78bc8b888c 4124aa50-c1d1-44d7-830b-cad7bf151e2c 60736 2 2022-05-30 07:34:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 37a5f889-83fa-4c2a-85c1-f6296dd75fa5 0xc00377c8c7 0xc00377c8c8}] []  [{kube-controller-manager Update apps/v1 2022-05-30 07:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37a5f889-83fa-4c2a-85c1-f6296dd75fa5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00377c958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 30 07:34:40.461: INFO: Pod "test-rollover-deployment-5797c7764-qh24z" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-qh24z test-rollover-deployment-5797c7764- deployment-4129 /api/v1/namespaces/deployment-4129/pods/test-rollover-deployment-5797c7764-qh24z e7078050-00ef-44c4-9987-83cb8372b4a0 60760 0 2022-05-30 07:34:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 baed66cd-5974-4d01-8f71-36f6dc5bc23b 0xc00377cf00 0xc00377cf01}] []  [{kube-controller-manager Update v1 2022-05-30 07:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"baed66cd-5974-4d01-8f71-36f6dc5bc23b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:34:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n69d9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n69d9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n69d9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:34:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.60,StartTime:2022-05-30 07:34:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://61afc6ad4db3f3cb35faabf4d6a800c22e2fad1de540ddbdd80f448742b7e691,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:34:40.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4129" for this suite.

• [SLOW TEST:32.902 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":303,"completed":204,"skipped":3744,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:34:40.482: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-39
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-39
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-39
May 30 07:34:40.806: INFO: Found 0 stateful pods, waiting for 1
May 30 07:34:50.816: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 30 07:34:50.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:34:51.215: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:34:51.215: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:34:51.215: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:34:51.226: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 30 07:35:01.230: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:35:01.230: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:35:01.331: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999969s
May 30 07:35:02.336: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983671879s
May 30 07:35:03.342: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978616297s
May 30 07:35:04.348: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973084233s
May 30 07:35:05.352: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.967099064s
May 30 07:35:06.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.962048162s
May 30 07:35:07.366: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.955438463s
May 30 07:35:08.378: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.944753471s
May 30 07:35:09.390: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.935963171s
May 30 07:35:10.404: INFO: Verifying statefulset ss doesn't scale past 1 for another 923.676564ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-39
May 30 07:35:11.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:35:12.264: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 30 07:35:12.264: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 07:35:12.264: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 07:35:12.324: INFO: Found 1 stateful pods, waiting for 3
May 30 07:35:22.346: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 07:35:22.346: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 07:35:22.347: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
May 30 07:35:32.329: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 30 07:35:32.329: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 30 07:35:32.329: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 30 07:35:32.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:35:32.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:35:32.629: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:35:32.629: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:35:32.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:35:32.960: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:35:32.960: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:35:32.960: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:35:32.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 30 07:35:33.513: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 30 07:35:33.513: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 30 07:35:33.513: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 30 07:35:33.513: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:35:33.516: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 30 07:35:43.533: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:35:43.533: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:35:43.533: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 30 07:35:43.551: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999657s
May 30 07:35:44.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991630584s
May 30 07:35:45.579: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97579036s
May 30 07:35:46.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.964906025s
May 30 07:35:47.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.959545704s
May 30 07:35:48.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952536637s
May 30 07:35:49.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948107669s
May 30 07:35:50.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944665116s
May 30 07:35:51.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936636841s
May 30 07:35:52.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.00985ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-39
May 30 07:35:53.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:35:54.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 30 07:35:54.023: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 07:35:54.023: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 07:35:54.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:35:54.674: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 30 07:35:54.674: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 07:35:54.674: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 07:35:54.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=statefulset-39 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 30 07:35:55.004: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 30 07:35:55.004: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 30 07:35:55.004: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 30 07:35:55.004: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 30 07:36:35.059: INFO: Deleting all statefulset in ns statefulset-39
May 30 07:36:35.064: INFO: Scaling statefulset ss to 0
May 30 07:36:35.079: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:36:35.083: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:36:35.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-39" for this suite.

• [SLOW TEST:114.751 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":303,"completed":205,"skipped":3753,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:36:35.252: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-60bf8ca8-2918-41d2-bf35-49dbc752532e in namespace container-probe-1363
May 30 07:36:41.327: INFO: Started pod test-webserver-60bf8ca8-2918-41d2-bf35-49dbc752532e in namespace container-probe-1363
STEP: checking the pod's current state and verifying that restartCount is present
May 30 07:36:41.330: INFO: Initial restart count of pod test-webserver-60bf8ca8-2918-41d2-bf35-49dbc752532e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:40:42.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1363" for this suite.

• [SLOW TEST:247.419 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":303,"completed":206,"skipped":3804,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:40:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 30 07:40:52.021: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:40:53.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9589" for this suite.

• [SLOW TEST:10.431 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":303,"completed":207,"skipped":3805,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:40:53.507: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 30 07:40:54.117: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 30 07:40:54.159: INFO: Waiting for terminating namespaces to be deleted...
May 30 07:40:54.287: INFO: 
Logging pods the apiserver thinks is on node my-node before test
May 30 07:40:54.513: INFO: coredns-f9fd979d6-c9tcq from kube-system started at 2022-05-27 09:44:35 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container coredns ready: true, restart count 1
May 30 07:40:54.685: INFO: coredns-f9fd979d6-lrzv9 from kube-system started at 2022-05-27 09:44:25 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container coredns ready: true, restart count 1
May 30 07:40:54.685: INFO: etcd-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container etcd ready: true, restart count 1
May 30 07:40:54.685: INFO: kube-apiserver-my-node from kube-system started at 2022-05-30 01:38:05 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container kube-apiserver ready: true, restart count 1
May 30 07:40:54.685: INFO: kube-controller-manager-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 07:40:54.685: INFO: kube-flannel-ds-lv5nv from kube-system started at 2022-05-27 09:43:53 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container kube-flannel ready: true, restart count 1
May 30 07:40:54.685: INFO: kube-proxy-4pwdh from kube-system started at 2022-05-27 09:42:33 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 07:40:54.685: INFO: kube-scheduler-my-node from kube-system started at 2022-05-27 09:42:24 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.685: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 07:40:54.685: INFO: pod-adoption-release from replicaset-9589 started at 2022-05-30 07:40:42 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.686: INFO: 	Container pod-adoption-release ready: true, restart count 0
May 30 07:40:54.686: INFO: pod-adoption-release-hkw2r from replicaset-9589 started at 2022-05-30 07:40:52 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.686: INFO: 	Container pod-adoption-release ready: false, restart count 0
May 30 07:40:54.686: INFO: sonobuoy from sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (1 container statuses recorded)
May 30 07:40:54.686: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 30 07:40:54.686: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 from sonobuoy started at 2022-05-30 06:08:24 +0000 UTC (2 container statuses recorded)
May 30 07:40:54.686: INFO: 	Container e2e ready: true, restart count 0
May 30 07:40:54.686: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:40:54.686: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 from sonobuoy started at 2022-05-30 06:08:25 +0000 UTC (2 container statuses recorded)
May 30 07:40:54.686: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:40:54.686: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-47271d7d-b0d8-4464-b89d-d0cc2144b21a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-47271d7d-b0d8-4464-b89d-d0cc2144b21a off the node my-node
STEP: verifying the node doesn't have the label kubernetes.io/e2e-47271d7d-b0d8-4464-b89d-d0cc2144b21a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:41:13.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6403" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:19.960 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":303,"completed":208,"skipped":3821,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:41:13.473: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-efb49442-7f01-4c15-ac3a-aaeeb862e0d7
STEP: Creating configMap with name cm-test-opt-upd-dda745da-c73d-4075-a4a5-42410a200ab1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-efb49442-7f01-4c15-ac3a-aaeeb862e0d7
STEP: Updating configmap cm-test-opt-upd-dda745da-c73d-4075-a4a5-42410a200ab1
STEP: Creating configMap with name cm-test-opt-create-f8e2931a-311a-4a45-a0d9-09ba318ea6a7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:42:56.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9473" for this suite.

• [SLOW TEST:103.511 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":209,"skipped":3833,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:42:56.989: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 30 07:43:04.002: INFO: Successfully updated pod "annotationupdate2156325b-f87f-4a1d-8c81-48abed5afffc"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:43:06.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4610" for this suite.

• [SLOW TEST:9.210 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":210,"skipped":3833,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:43:06.198: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 30 07:43:06.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1480 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 30 07:43:06.743: INFO: stderr: ""
May 30 07:43:06.743: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 30 07:43:11.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1480 get pod e2e-test-httpd-pod -o json'
May 30 07:43:12.350: INFO: stderr: ""
May 30 07:43:12.350: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-05-30T07:43:06Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-05-30T07:43:06Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.0.71\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2022-05-30T07:43:10Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1480\",\n        \"resourceVersion\": \"62339\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1480/pods/e2e-test-httpd-pod\",\n        \"uid\": \"a85e0018-6727-4628-8a58-f7b1b6f89cf0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8n7qb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"my-node\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8n7qb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8n7qb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-30T07:43:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-30T07:43:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-30T07:43:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-30T07:43:06Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://74f008e54ca93c7d2c8a15a1a617449b238a3dc965f29c72947061ab9226c52f\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-05-30T07:43:10Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.2.15\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.0.71\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.0.71\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-05-30T07:43:06Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 30 07:43:12.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1480 replace -f -'
May 30 07:43:14.133: INFO: stderr: ""
May 30 07:43:14.133: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
May 30 07:43:14.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1480 delete pods e2e-test-httpd-pod'
May 30 07:43:21.283: INFO: stderr: ""
May 30 07:43:21.283: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:43:21.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1480" for this suite.

• [SLOW TEST:15.099 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":303,"completed":211,"skipped":3837,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:43:21.389: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
May 30 07:43:21.535: INFO: created test-podtemplate-1
May 30 07:43:21.546: INFO: created test-podtemplate-2
May 30 07:43:21.555: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 30 07:43:21.559: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 30 07:43:21.583: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:43:21.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2886" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":303,"completed":212,"skipped":3895,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:43:21.607: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 30 07:43:21.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1509 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 30 07:43:22.431: INFO: stderr: ""
May 30 07:43:22.431: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 30 07:43:22.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1509 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
May 30 07:43:22.807: INFO: stderr: ""
May 30 07:43:22.807: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
May 30 07:43:22.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-1509 delete pods e2e-test-httpd-pod'
May 30 07:43:25.548: INFO: stderr: ""
May 30 07:43:25.550: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:43:25.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1509" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":303,"completed":213,"skipped":3912,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:43:25.721: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 30 07:43:26.811: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 07:43:31.020: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:43:41.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6889" for this suite.

• [SLOW TEST:16.099 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":303,"completed":214,"skipped":3921,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:43:41.822: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7488
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7488
STEP: creating replication controller externalsvc in namespace services-7488
I0530 07:43:42.112604      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7488, replica count: 2
I0530 07:43:45.164791      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:43:48.203101      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 30 07:43:48.440: INFO: Creating new exec pod
May 30 07:43:54.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-7488 exec execpodl7jch -- /bin/sh -x -c nslookup clusterip-service.services-7488.svc.cluster.local'
May 30 07:43:55.460: INFO: stderr: "+ nslookup clusterip-service.services-7488.svc.cluster.local\n"
May 30 07:43:55.460: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7488.svc.cluster.local\tcanonical name = externalsvc.services-7488.svc.cluster.local.\nName:\texternalsvc.services-7488.svc.cluster.local\nAddress: 10.102.113.35\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7488, will wait for the garbage collector to delete the pods
May 30 07:43:55.549: INFO: Deleting ReplicationController externalsvc took: 21.345512ms
May 30 07:43:56.050: INFO: Terminating ReplicationController externalsvc pods took: 501.150288ms
May 30 07:44:05.518: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:44:05.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7488" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:23.729 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":303,"completed":215,"skipped":3936,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:44:05.551: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 30 07:44:05.672: INFO: Waiting up to 5m0s for pod "downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f" in namespace "downward-api-9108" to be "Succeeded or Failed"
May 30 07:44:05.688: INFO: Pod "downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.85382ms
May 30 07:44:07.695: INFO: Pod "downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023438739s
May 30 07:44:09.790: INFO: Pod "downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118583376s
May 30 07:44:11.802: INFO: Pod "downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.130732229s
STEP: Saw pod success
May 30 07:44:11.802: INFO: Pod "downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f" satisfied condition "Succeeded or Failed"
May 30 07:44:11.811: INFO: Trying to get logs from node my-node pod downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f container dapi-container: <nil>
STEP: delete the pod
May 30 07:44:12.518: INFO: Waiting for pod downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f to disappear
May 30 07:44:12.752: INFO: Pod downward-api-f755ced7-ace5-4afc-95c9-482a12f2e91f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:44:12.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9108" for this suite.

• [SLOW TEST:7.229 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":303,"completed":216,"skipped":3943,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:44:12.782: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-a5d33367-d9cd-48c1-930f-dec9aefa8953
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:44:12.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3399" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":303,"completed":217,"skipped":3946,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:44:12.949: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-gfxv
STEP: Creating a pod to test atomic-volume-subpath
May 30 07:44:13.299: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gfxv" in namespace "subpath-6231" to be "Succeeded or Failed"
May 30 07:44:13.488: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Pending", Reason="", readiness=false. Elapsed: 187.966586ms
May 30 07:44:15.492: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.192240409s
May 30 07:44:17.497: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197803094s
May 30 07:44:19.504: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.204429999s
May 30 07:44:21.510: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 8.210781191s
May 30 07:44:23.518: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 10.218726125s
May 30 07:44:25.525: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 12.225011577s
May 30 07:44:27.530: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 14.23057387s
May 30 07:44:29.535: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 16.235704523s
May 30 07:44:31.540: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 18.240628705s
May 30 07:44:33.547: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 20.247412486s
May 30 07:44:35.560: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 22.260579768s
May 30 07:44:37.572: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 24.272157709s
May 30 07:44:39.581: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Running", Reason="", readiness=true. Elapsed: 26.28091542s
May 30 07:44:41.662: INFO: Pod "pod-subpath-test-configmap-gfxv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.362732909s
STEP: Saw pod success
May 30 07:44:41.663: INFO: Pod "pod-subpath-test-configmap-gfxv" satisfied condition "Succeeded or Failed"
May 30 07:44:41.669: INFO: Trying to get logs from node my-node pod pod-subpath-test-configmap-gfxv container test-container-subpath-configmap-gfxv: <nil>
STEP: delete the pod
May 30 07:44:41.705: INFO: Waiting for pod pod-subpath-test-configmap-gfxv to disappear
May 30 07:44:41.715: INFO: Pod pod-subpath-test-configmap-gfxv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gfxv
May 30 07:44:41.717: INFO: Deleting pod "pod-subpath-test-configmap-gfxv" in namespace "subpath-6231"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:44:41.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6231" for this suite.

• [SLOW TEST:28.794 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":303,"completed":218,"skipped":3974,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:44:41.776: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-15
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-15 to expose endpoints map[]
May 30 07:44:41.992: INFO: successfully validated that service endpoint-test2 in namespace services-15 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-15
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-15 to expose endpoints map[pod1:[80]]
May 30 07:44:46.078: INFO: successfully validated that service endpoint-test2 in namespace services-15 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-15
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-15 to expose endpoints map[pod1:[80] pod2:[80]]
May 30 07:44:50.215: INFO: Unexpected endpoints: found map[7892aff7-4498-456f-9d83-45a156656b13:[80]], expected map[pod1:[80] pod2:[80]], will retry
May 30 07:44:51.215: INFO: successfully validated that service endpoint-test2 in namespace services-15 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-15
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-15 to expose endpoints map[pod2:[80]]
May 30 07:44:51.318: INFO: successfully validated that service endpoint-test2 in namespace services-15 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-15
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-15 to expose endpoints map[]
May 30 07:44:51.372: INFO: successfully validated that service endpoint-test2 in namespace services-15 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:44:51.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-15" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:9.681 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":303,"completed":219,"skipped":4009,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:44:51.457: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 30 07:44:52.680: INFO: Waiting up to 5m0s for pod "pod-8cf2341d-274c-4127-ad29-38276b720f91" in namespace "emptydir-5148" to be "Succeeded or Failed"
May 30 07:44:52.771: INFO: Pod "pod-8cf2341d-274c-4127-ad29-38276b720f91": Phase="Pending", Reason="", readiness=false. Elapsed: 90.951033ms
May 30 07:44:55.138: INFO: Pod "pod-8cf2341d-274c-4127-ad29-38276b720f91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.458546748s
May 30 07:44:57.141: INFO: Pod "pod-8cf2341d-274c-4127-ad29-38276b720f91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.460993584s
May 30 07:44:59.145: INFO: Pod "pod-8cf2341d-274c-4127-ad29-38276b720f91": Phase="Pending", Reason="", readiness=false. Elapsed: 6.465527267s
May 30 07:45:01.293: INFO: Pod "pod-8cf2341d-274c-4127-ad29-38276b720f91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.613568058s
STEP: Saw pod success
May 30 07:45:01.293: INFO: Pod "pod-8cf2341d-274c-4127-ad29-38276b720f91" satisfied condition "Succeeded or Failed"
May 30 07:45:01.651: INFO: Trying to get logs from node my-node pod pod-8cf2341d-274c-4127-ad29-38276b720f91 container test-container: <nil>
STEP: delete the pod
May 30 07:45:01.965: INFO: Waiting for pod pod-8cf2341d-274c-4127-ad29-38276b720f91 to disappear
May 30 07:45:02.021: INFO: Pod pod-8cf2341d-274c-4127-ad29-38276b720f91 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:45:02.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5148" for this suite.

• [SLOW TEST:10.582 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":220,"skipped":4010,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:45:02.039: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-a02ceeba-7579-4793-9df4-61b51d5e2167
STEP: Creating a pod to test consume configMaps
May 30 07:45:02.196: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a" in namespace "projected-3589" to be "Succeeded or Failed"
May 30 07:45:02.207: INFO: Pod "pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.931772ms
May 30 07:45:04.226: INFO: Pod "pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029696189s
May 30 07:45:06.375: INFO: Pod "pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.179051007s
May 30 07:45:08.383: INFO: Pod "pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.186809416s
May 30 07:45:10.393: INFO: Pod "pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.19692906s
STEP: Saw pod success
May 30 07:45:10.394: INFO: Pod "pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a" satisfied condition "Succeeded or Failed"
May 30 07:45:10.397: INFO: Trying to get logs from node my-node pod pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 30 07:45:10.431: INFO: Waiting for pod pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a to disappear
May 30 07:45:10.435: INFO: Pod pod-projected-configmaps-42947737-2d70-4150-8f45-24046de8fe3a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:45:10.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3589" for this suite.

• [SLOW TEST:8.406 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":221,"skipped":4032,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:45:10.457: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 07:45:11.389: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 30 07:45:13.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:45:15.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493511, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:45:18.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
May 30 07:45:18.931: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
May 30 07:45:19.131: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:45:19.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8062" for this suite.
STEP: Destroying namespace "webhook-8062-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.854 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":303,"completed":222,"skipped":4035,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:45:19.310: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-489
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-489
May 30 07:45:19.457: INFO: Found 0 stateful pods, waiting for 1
May 30 07:45:29.474: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 30 07:45:29.555: INFO: Deleting all statefulset in ns statefulset-489
May 30 07:45:29.561: INFO: Scaling statefulset ss to 0
May 30 07:45:39.622: INFO: Waiting for statefulset status.replicas updated to 0
May 30 07:45:39.638: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:45:39.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-489" for this suite.

• [SLOW TEST:20.361 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":303,"completed":223,"skipped":4040,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:45:39.671: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:45:39.766: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 30 07:45:39.783: INFO: Pod name sample-pod: Found 0 pods out of 1
May 30 07:45:44.788: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 30 07:45:44.788: INFO: Creating deployment "test-rolling-update-deployment"
May 30 07:45:44.804: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 30 07:45:44.812: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 30 07:45:46.822: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 30 07:45:46.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:45:48.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493544, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:45:51.226: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 30 07:45:51.464: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7822 /apis/apps/v1/namespaces/deployment-7822/deployments/test-rolling-update-deployment 89d332b4-cf22-4e34-9773-4d129b79866c 63252 1 2022-05-30 07:45:44 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-05-30 07:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-05-30 07:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cbc968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-30 07:45:44 +0000 UTC,LastTransitionTime:2022-05-30 07:45:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2022-05-30 07:45:49 +0000 UTC,LastTransitionTime:2022-05-30 07:45:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 30 07:45:51.471: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-7822 /apis/apps/v1/namespaces/deployment-7822/replicasets/test-rolling-update-deployment-c4cb8d6d9 baf0fc36-d63d-48c2-96e3-89b86232a15c 63241 1 2022-05-30 07:45:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 89d332b4-cf22-4e34-9773-4d129b79866c 0xc003cbcf80 0xc003cbcf81}] []  [{kube-controller-manager Update apps/v1 2022-05-30 07:45:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89d332b4-cf22-4e34-9773-4d129b79866c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cbcff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 30 07:45:51.471: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 30 07:45:51.472: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7822 /apis/apps/v1/namespaces/deployment-7822/replicasets/test-rolling-update-controller 64259735-718d-4dd5-8f46-4683c444692b 63250 2 2022-05-30 07:45:39 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 89d332b4-cf22-4e34-9773-4d129b79866c 0xc003cbce77 0xc003cbce78}] []  [{e2e.test Update apps/v1 2022-05-30 07:45:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-05-30 07:45:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89d332b4-cf22-4e34-9773-4d129b79866c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003cbcf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 30 07:45:51.495: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-nrmqw" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-nrmqw test-rolling-update-deployment-c4cb8d6d9- deployment-7822 /api/v1/namespaces/deployment-7822/pods/test-rolling-update-deployment-c4cb8d6d9-nrmqw e400a57b-2860-479f-be75-0ca5823a771d 63240 0 2022-05-30 07:45:44 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 baf0fc36-d63d-48c2-96e3-89b86232a15c 0xc003cbd500 0xc003cbd501}] []  [{kube-controller-manager Update v1 2022-05-30 07:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"baf0fc36-d63d-48c2-96e3-89b86232a15c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 07:45:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bwpj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bwpj5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bwpj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:45:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:45:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 07:45:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.85,StartTime:2022-05-30 07:45:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 07:45:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://18740e79f21b897472e0e4be4f706ae6265a57d2e373cdb70ce9e58a1e241242,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:45:51.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7822" for this suite.

• [SLOW TEST:11.837 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":303,"completed":224,"skipped":4053,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:45:51.514: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-5ad53d9d-128e-45b4-a02e-16b401bbaed3
STEP: Creating a pod to test consume secrets
May 30 07:45:51.694: INFO: Waiting up to 5m0s for pod "pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d" in namespace "secrets-3910" to be "Succeeded or Failed"
May 30 07:45:51.727: INFO: Pod "pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d": Phase="Pending", Reason="", readiness=false. Elapsed: 33.133146ms
May 30 07:45:53.731: INFO: Pod "pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036607941s
May 30 07:45:55.850: INFO: Pod "pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.15623164s
May 30 07:45:59.297: INFO: Pod "pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.602501609s
May 30 07:46:01.408: INFO: Pod "pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.713778841s
STEP: Saw pod success
May 30 07:46:01.408: INFO: Pod "pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d" satisfied condition "Succeeded or Failed"
May 30 07:46:01.784: INFO: Trying to get logs from node my-node pod pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d container secret-volume-test: <nil>
STEP: delete the pod
May 30 07:46:02.891: INFO: Waiting for pod pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d to disappear
May 30 07:46:03.044: INFO: Pod pod-secrets-82b00992-f64e-40dd-94c1-81eea560c13d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:46:03.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3910" for this suite.

• [SLOW TEST:11.545 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":225,"skipped":4056,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:46:03.059: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
May 30 07:46:03.199: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-2608 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:46:03.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2608" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":303,"completed":226,"skipped":4070,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:46:03.769: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 30 07:46:04.114: INFO: Created pod &Pod{ObjectMeta:{dns-9419  dns-9419 /api/v1/namespaces/dns-9419/pods/dns-9419 02fa7d8c-7c41-4d85-8247-fdb73152e5df 63328 0 2022-05-30 07:46:04 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-05-30 07:46:04 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tfg4b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tfg4b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tfg4b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 07:46:04.186: INFO: The status of Pod dns-9419 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:46:06.195: INFO: The status of Pod dns-9419 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:46:08.292: INFO: The status of Pod dns-9419 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:46:10.702: INFO: The status of Pod dns-9419 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:46:12.359: INFO: The status of Pod dns-9419 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:46:14.402: INFO: The status of Pod dns-9419 is Pending, waiting for it to be Running (with Ready = true)
May 30 07:46:16.195: INFO: The status of Pod dns-9419 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 30 07:46:16.283: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9419 PodName:dns-9419 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 07:46:16.283: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Verifying customized DNS server is configured on pod...
May 30 07:46:16.958: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9419 PodName:dns-9419 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 30 07:46:16.958: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 07:46:17.315: INFO: Deleting pod dns-9419...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:46:17.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9419" for this suite.

• [SLOW TEST:13.622 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":303,"completed":227,"skipped":4088,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:46:17.394: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 30 07:46:26.940: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:46:27.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-369" for this suite.

• [SLOW TEST:10.150 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":303,"completed":228,"skipped":4111,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:46:27.544: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
May 30 07:46:27.579: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:46:40.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4639" for this suite.

• [SLOW TEST:12.953 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":303,"completed":229,"skipped":4154,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:46:40.511: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-77a4a8ef-6139-4fa9-ae8d-cdf03771c288
STEP: Creating a pod to test consume configMaps
May 30 07:46:40.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d" in namespace "configmap-2028" to be "Succeeded or Failed"
May 30 07:46:40.598: INFO: Pod "pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.226801ms
May 30 07:46:42.646: INFO: Pod "pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063887523s
May 30 07:46:44.666: INFO: Pod "pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083694102s
May 30 07:46:46.679: INFO: Pod "pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d": Phase="Running", Reason="", readiness=true. Elapsed: 6.097224041s
May 30 07:46:48.688: INFO: Pod "pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.105940345s
STEP: Saw pod success
May 30 07:46:48.689: INFO: Pod "pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d" satisfied condition "Succeeded or Failed"
May 30 07:46:48.720: INFO: Trying to get logs from node my-node pod pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d container configmap-volume-test: <nil>
STEP: delete the pod
May 30 07:46:48.773: INFO: Waiting for pod pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d to disappear
May 30 07:46:48.781: INFO: Pod pod-configmaps-535ec471-8d4a-4b34-970b-6a3e27af923d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:46:48.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2028" for this suite.

• [SLOW TEST:8.336 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":303,"completed":230,"skipped":4177,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:46:48.864: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 30 07:46:48.978: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:46:56.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9769" for this suite.

• [SLOW TEST:8.216 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":303,"completed":231,"skipped":4178,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:46:57.081: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0530 07:47:08.418128      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 07:48:10.621: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:48:10.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1014" for this suite.

• [SLOW TEST:73.551 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":303,"completed":232,"skipped":4181,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:48:10.653: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7165
STEP: creating service affinity-clusterip in namespace services-7165
STEP: creating replication controller affinity-clusterip in namespace services-7165
I0530 07:48:10.936868      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-7165, replica count: 3
I0530 07:48:13.987596      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:48:16.990105      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:48:19.990943      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 07:48:22.991652      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 07:48:23.006: INFO: Creating new exec pod
May 30 07:48:30.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-7165 exec execpod-affinity49vrm -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 30 07:48:31.692: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 30 07:48:31.692: INFO: stdout: ""
May 30 07:48:31.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-7165 exec execpod-affinity49vrm -- /bin/sh -x -c nc -zv -t -w 2 10.110.217.183 80'
May 30 07:48:32.113: INFO: stderr: "+ nc -zv -t -w 2 10.110.217.183 80\nConnection to 10.110.217.183 80 port [tcp/http] succeeded!\n"
May 30 07:48:32.113: INFO: stdout: ""
May 30 07:48:32.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-7165 exec execpod-affinity49vrm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.217.183:80/ ; done'
May 30 07:48:32.709: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.217.183:80/\n"
May 30 07:48:32.709: INFO: stdout: "\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp\naffinity-clusterip-fz9lp"
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Received response from host: affinity-clusterip-fz9lp
May 30 07:48:32.709: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7165, will wait for the garbage collector to delete the pods
May 30 07:48:32.900: INFO: Deleting ReplicationController affinity-clusterip took: 6.648496ms
May 30 07:48:34.000: INFO: Terminating ReplicationController affinity-clusterip pods took: 1.100636462s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:48:55.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7165" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:44.926 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":303,"completed":233,"skipped":4196,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:48:55.579: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 07:48:55.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697" in namespace "projected-8066" to be "Succeeded or Failed"
May 30 07:48:55.704: INFO: Pod "downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697": Phase="Pending", Reason="", readiness=false. Elapsed: 15.474796ms
May 30 07:48:57.710: INFO: Pod "downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021127415s
May 30 07:48:59.714: INFO: Pod "downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025432923s
May 30 07:49:01.717: INFO: Pod "downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02893108s
STEP: Saw pod success
May 30 07:49:01.718: INFO: Pod "downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697" satisfied condition "Succeeded or Failed"
May 30 07:49:01.721: INFO: Trying to get logs from node my-node pod downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697 container client-container: <nil>
STEP: delete the pod
May 30 07:49:01.897: INFO: Waiting for pod downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697 to disappear
May 30 07:49:01.908: INFO: Pod downwardapi-volume-45b2b933-929b-4623-b40b-ac13b5bed697 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:49:01.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8066" for this suite.

• [SLOW TEST:6.340 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":234,"skipped":4205,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:49:01.920: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:49:02.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9228" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":303,"completed":235,"skipped":4207,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:49:02.039: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-b27b42a3-8793-4dfe-a00f-8ae952e786a6
STEP: Creating a pod to test consume configMaps
May 30 07:49:02.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b" in namespace "configmap-3280" to be "Succeeded or Failed"
May 30 07:49:02.089: INFO: Pod "pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.159424ms
May 30 07:49:04.113: INFO: Pod "pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02732379s
May 30 07:49:06.121: INFO: Pod "pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03510279s
May 30 07:49:08.129: INFO: Pod "pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043355969s
STEP: Saw pod success
May 30 07:49:08.129: INFO: Pod "pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b" satisfied condition "Succeeded or Failed"
May 30 07:49:08.135: INFO: Trying to get logs from node my-node pod pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b container configmap-volume-test: <nil>
STEP: delete the pod
May 30 07:49:08.187: INFO: Waiting for pod pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b to disappear
May 30 07:49:08.190: INFO: Pod pod-configmaps-28bbe447-65fd-49b0-b807-7fb05777b21b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:49:08.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3280" for this suite.

• [SLOW TEST:6.166 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":236,"skipped":4209,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:49:08.204: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-9495a60c-70d7-4e16-8d57-16b41d46a35c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:49:16.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3822" for this suite.

• [SLOW TEST:8.157 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":303,"completed":237,"skipped":4248,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:49:16.362: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 30 07:49:21.068: INFO: Successfully updated pod "annotationupdatec43a649b-a836-4ca5-a2c8-65825be7605a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:49:23.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1371" for this suite.

• [SLOW TEST:6.823 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":303,"completed":238,"skipped":4255,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:49:23.184: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-833.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-833.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 12.15.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.15.12_udp@PTR;check="$$(dig +tcp +noall +answer +search 12.15.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.15.12_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-833.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-833.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-833.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 12.15.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.15.12_udp@PTR;check="$$(dig +tcp +noall +answer +search 12.15.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.15.12_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 07:49:34.454: INFO: Unable to read wheezy_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:34.460: INFO: Unable to read wheezy_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:34.464: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:34.504: INFO: Unable to read jessie_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:34.510: INFO: Unable to read jessie_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:34.555: INFO: Lookups using dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad failed for: [wheezy_udp@dns-test-service.dns-833.svc.cluster.local wheezy_tcp@dns-test-service.dns-833.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-833.svc.cluster.local jessie_udp@dns-test-service.dns-833.svc.cluster.local jessie_tcp@dns-test-service.dns-833.svc.cluster.local]

May 30 07:49:39.562: INFO: Unable to read wheezy_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:39.566: INFO: Unable to read wheezy_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:39.603: INFO: Unable to read jessie_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:39.607: INFO: Unable to read jessie_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:39.638: INFO: Lookups using dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad failed for: [wheezy_udp@dns-test-service.dns-833.svc.cluster.local wheezy_tcp@dns-test-service.dns-833.svc.cluster.local jessie_udp@dns-test-service.dns-833.svc.cluster.local jessie_tcp@dns-test-service.dns-833.svc.cluster.local]

May 30 07:49:44.579: INFO: Unable to read wheezy_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:44.582: INFO: Unable to read wheezy_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:44.622: INFO: Unable to read jessie_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:44.627: INFO: Unable to read jessie_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:44.657: INFO: Lookups using dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad failed for: [wheezy_udp@dns-test-service.dns-833.svc.cluster.local wheezy_tcp@dns-test-service.dns-833.svc.cluster.local jessie_udp@dns-test-service.dns-833.svc.cluster.local jessie_tcp@dns-test-service.dns-833.svc.cluster.local]

May 30 07:49:49.576: INFO: Unable to read wheezy_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:49.585: INFO: Unable to read wheezy_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:49.645: INFO: Unable to read jessie_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:49.649: INFO: Unable to read jessie_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:49.686: INFO: Lookups using dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad failed for: [wheezy_udp@dns-test-service.dns-833.svc.cluster.local wheezy_tcp@dns-test-service.dns-833.svc.cluster.local jessie_udp@dns-test-service.dns-833.svc.cluster.local jessie_tcp@dns-test-service.dns-833.svc.cluster.local]

May 30 07:49:54.566: INFO: Unable to read wheezy_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:54.574: INFO: Unable to read wheezy_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:54.631: INFO: Unable to read jessie_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:54.636: INFO: Unable to read jessie_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:54.693: INFO: Lookups using dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad failed for: [wheezy_udp@dns-test-service.dns-833.svc.cluster.local wheezy_tcp@dns-test-service.dns-833.svc.cluster.local jessie_udp@dns-test-service.dns-833.svc.cluster.local jessie_tcp@dns-test-service.dns-833.svc.cluster.local]

May 30 07:49:59.569: INFO: Unable to read wheezy_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:59.580: INFO: Unable to read wheezy_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:59.619: INFO: Unable to read jessie_udp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:59.625: INFO: Unable to read jessie_tcp@dns-test-service.dns-833.svc.cluster.local from pod dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad: the server could not find the requested resource (get pods dns-test-11e1e4b6-5892-4474-a775-0410471141ad)
May 30 07:49:59.668: INFO: Lookups using dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad failed for: [wheezy_udp@dns-test-service.dns-833.svc.cluster.local wheezy_tcp@dns-test-service.dns-833.svc.cluster.local jessie_udp@dns-test-service.dns-833.svc.cluster.local jessie_tcp@dns-test-service.dns-833.svc.cluster.local]

May 30 07:50:04.903: INFO: DNS probes using dns-833/dns-test-11e1e4b6-5892-4474-a775-0410471141ad succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:50:07.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-833" for this suite.

• [SLOW TEST:44.374 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":303,"completed":239,"skipped":4260,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:50:07.559: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 07:50:12.831: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 30 07:50:16.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:18.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:21.487: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:22.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:24.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:26.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493813, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:50:29.636: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:50:31.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4536" for this suite.
STEP: Destroying namespace "webhook-4536-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:24.585 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":303,"completed":240,"skipped":4260,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:50:32.153: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 07:50:34.992: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 07:50:37.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493834, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:39.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493834, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:41.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493834, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:50:43.849: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493835, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493834, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:50:46.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:50:47.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3142" for this suite.
STEP: Destroying namespace "webhook-3142-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.686 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":303,"completed":241,"skipped":4261,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:50:47.861: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
May 30 07:50:48.035: INFO: Waiting up to 5m0s for pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d" in namespace "downward-api-8915" to be "Succeeded or Failed"
May 30 07:50:48.130: INFO: Pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 94.900477ms
May 30 07:50:50.213: INFO: Pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178158402s
May 30 07:50:52.749: INFO: Pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.71471127s
May 30 07:50:54.755: INFO: Pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.72068831s
May 30 07:50:56.778: INFO: Pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.743196437s
May 30 07:50:58.800: INFO: Pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.765802391s
STEP: Saw pod success
May 30 07:50:58.801: INFO: Pod "downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d" satisfied condition "Succeeded or Failed"
May 30 07:50:58.805: INFO: Trying to get logs from node my-node pod downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d container dapi-container: <nil>
STEP: delete the pod
May 30 07:50:59.045: INFO: Waiting for pod downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d to disappear
May 30 07:50:59.052: INFO: Pod downward-api-625b098f-f2b7-4343-8b06-2b8149bc0f2d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:50:59.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8915" for this suite.

• [SLOW TEST:11.211 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":303,"completed":242,"skipped":4265,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:50:59.090: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 30 07:50:59.248: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:51:07.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4824" for this suite.

• [SLOW TEST:8.191 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":303,"completed":243,"skipped":4280,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:51:07.287: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:51:23.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4094" for this suite.

• [SLOW TEST:16.675 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":303,"completed":244,"skipped":4297,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:51:24.013: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:51:24.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-594" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":303,"completed":245,"skipped":4324,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:51:24.407: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
May 30 07:51:24.455: INFO: Waiting up to 5m0s for pod "pod-dc319155-a542-431b-b0f9-268067313f89" in namespace "emptydir-7699" to be "Succeeded or Failed"
May 30 07:51:24.477: INFO: Pod "pod-dc319155-a542-431b-b0f9-268067313f89": Phase="Pending", Reason="", readiness=false. Elapsed: 21.738626ms
May 30 07:51:26.482: INFO: Pod "pod-dc319155-a542-431b-b0f9-268067313f89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027058359s
May 30 07:51:28.702: INFO: Pod "pod-dc319155-a542-431b-b0f9-268067313f89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247108413s
May 30 07:51:30.710: INFO: Pod "pod-dc319155-a542-431b-b0f9-268067313f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.254703718s
STEP: Saw pod success
May 30 07:51:30.710: INFO: Pod "pod-dc319155-a542-431b-b0f9-268067313f89" satisfied condition "Succeeded or Failed"
May 30 07:51:30.717: INFO: Trying to get logs from node my-node pod pod-dc319155-a542-431b-b0f9-268067313f89 container test-container: <nil>
STEP: delete the pod
May 30 07:51:30.765: INFO: Waiting for pod pod-dc319155-a542-431b-b0f9-268067313f89 to disappear
May 30 07:51:30.770: INFO: Pod pod-dc319155-a542-431b-b0f9-268067313f89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:51:30.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7699" for this suite.

• [SLOW TEST:6.378 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":246,"skipped":4332,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:51:30.785: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:51:36.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8761" for this suite.

• [SLOW TEST:6.103 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":247,"skipped":4341,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:51:36.888: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:51:36.952: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a" in namespace "security-context-test-9545" to be "Succeeded or Failed"
May 30 07:51:36.957: INFO: Pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.871895ms
May 30 07:51:38.961: INFO: Pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00902714s
May 30 07:51:40.975: INFO: Pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022623961s
May 30 07:51:42.978: INFO: Pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a": Phase="Running", Reason="", readiness=true. Elapsed: 6.026245786s
May 30 07:51:44.982: INFO: Pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029606814s
May 30 07:51:44.982: INFO: Pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a" satisfied condition "Succeeded or Failed"
May 30 07:51:44.991: INFO: Got logs for pod "busybox-privileged-false-a3023b72-3b08-4fa4-a2f8-366c95a5ef2a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:51:44.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9545" for this suite.

• [SLOW TEST:8.111 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":248,"skipped":4342,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:51:45.007: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:51:45.191: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 30 07:51:49.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 create -f -'
May 30 07:51:53.495: INFO: stderr: ""
May 30 07:51:53.495: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 30 07:51:53.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 delete e2e-test-crd-publish-openapi-1359-crds test-foo'
May 30 07:51:53.898: INFO: stderr: ""
May 30 07:51:53.898: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 30 07:51:53.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 apply -f -'
May 30 07:51:54.505: INFO: stderr: ""
May 30 07:51:54.505: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 30 07:51:54.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 delete e2e-test-crd-publish-openapi-1359-crds test-foo'
May 30 07:51:54.706: INFO: stderr: ""
May 30 07:51:54.706: INFO: stdout: "e2e-test-crd-publish-openapi-1359-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 30 07:51:54.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 create -f -'
May 30 07:51:55.100: INFO: rc: 1
May 30 07:51:55.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 apply -f -'
May 30 07:51:55.620: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 30 07:51:55.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 create -f -'
May 30 07:51:56.339: INFO: rc: 1
May 30 07:51:56.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 --namespace=crd-publish-openapi-2187 apply -f -'
May 30 07:51:56.785: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 30 07:51:56.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 explain e2e-test-crd-publish-openapi-1359-crds'
May 30 07:51:57.336: INFO: stderr: ""
May 30 07:51:57.336: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1359-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 30 07:51:57.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 explain e2e-test-crd-publish-openapi-1359-crds.metadata'
May 30 07:51:57.933: INFO: stderr: ""
May 30 07:51:57.933: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1359-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 30 07:51:57.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 explain e2e-test-crd-publish-openapi-1359-crds.spec'
May 30 07:51:58.416: INFO: stderr: ""
May 30 07:51:58.416: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1359-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 30 07:51:58.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 explain e2e-test-crd-publish-openapi-1359-crds.spec.bars'
May 30 07:51:58.863: INFO: stderr: ""
May 30 07:51:58.863: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1359-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 30 07:51:58.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-2187 explain e2e-test-crd-publish-openapi-1359-crds.spec.bars2'
May 30 07:51:59.237: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:52:02.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2187" for this suite.

• [SLOW TEST:17.305 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":303,"completed":249,"skipped":4343,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:52:02.329: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-8e7ffa57-f52c-4e70-a748-e2e6dd1f11a8
STEP: Creating secret with name secret-projected-all-test-volume-17b874f4-4bec-4d92-8db2-9ed6889cbf1e
STEP: Creating a pod to test Check all projections for projected volume plugin
May 30 07:52:02.548: INFO: Waiting up to 5m0s for pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321" in namespace "projected-8394" to be "Succeeded or Failed"
May 30 07:52:02.556: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321": Phase="Pending", Reason="", readiness=false. Elapsed: 8.13169ms
May 30 07:52:04.584: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036092222s
May 30 07:52:06.863: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321": Phase="Pending", Reason="", readiness=false. Elapsed: 4.314396112s
May 30 07:52:08.965: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321": Phase="Pending", Reason="", readiness=false. Elapsed: 6.416664141s
May 30 07:52:10.972: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321": Phase="Pending", Reason="", readiness=false. Elapsed: 8.423384547s
May 30 07:52:12.981: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321": Phase="Running", Reason="", readiness=true. Elapsed: 10.432824612s
May 30 07:52:15.005: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.456885238s
STEP: Saw pod success
May 30 07:52:15.005: INFO: Pod "projected-volume-0067ab62-7594-49fc-a539-954bdc16f321" satisfied condition "Succeeded or Failed"
May 30 07:52:15.033: INFO: Trying to get logs from node my-node pod projected-volume-0067ab62-7594-49fc-a539-954bdc16f321 container projected-all-volume-test: <nil>
STEP: delete the pod
May 30 07:52:17.122: INFO: Waiting for pod projected-volume-0067ab62-7594-49fc-a539-954bdc16f321 to disappear
May 30 07:52:17.553: INFO: Pod projected-volume-0067ab62-7594-49fc-a539-954bdc16f321 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:52:17.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8394" for this suite.

• [SLOW TEST:15.384 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":303,"completed":250,"skipped":4343,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:52:17.773: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 30 07:52:18.972: INFO: starting watch
STEP: patching
STEP: updating
May 30 07:52:19.156: INFO: waiting for watch events with expected annotations
May 30 07:52:19.156: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:52:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6541" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":303,"completed":251,"skipped":4361,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:52:19.339: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 30 07:52:20.112: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
May 30 07:52:23.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:52:25.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:52:27.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:52:29.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:52:31.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789493940, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:52:34.251: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:52:34.255: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:52:36.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1179" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:17.118 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":303,"completed":252,"skipped":4385,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:52:36.584: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 30 07:52:37.300: INFO: Waiting up to 1m0s for all nodes to be ready
May 30 07:53:37.522: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:53:37.541: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 30 07:53:45.836: INFO: found a healthy node: my-node
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:54:10.474: INFO: pods created so far: [1 1 1]
May 30 07:54:10.474: INFO: length of pods created so far: 3
May 30 07:54:26.504: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:54:33.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6438" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:54:33.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4982" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:117.179 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":303,"completed":253,"skipped":4412,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:54:33.764: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-34ba4b9e-ac41-47ab-bda7-ad9b688bf8d5
STEP: Creating a pod to test consume secrets
May 30 07:54:34.269: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274" in namespace "projected-390" to be "Succeeded or Failed"
May 30 07:54:34.281: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Pending", Reason="", readiness=false. Elapsed: 11.767965ms
May 30 07:54:36.284: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014898495s
May 30 07:54:38.321: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051667561s
May 30 07:54:40.397: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Running", Reason="", readiness=true. Elapsed: 6.127175983s
May 30 07:54:42.593: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Running", Reason="", readiness=true. Elapsed: 8.323243452s
May 30 07:54:44.639: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Running", Reason="", readiness=true. Elapsed: 10.369691909s
May 30 07:54:46.643: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Running", Reason="", readiness=true. Elapsed: 12.373568345s
May 30 07:54:48.930: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.660955646s
STEP: Saw pod success
May 30 07:54:48.931: INFO: Pod "pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274" satisfied condition "Succeeded or Failed"
May 30 07:54:48.937: INFO: Trying to get logs from node my-node pod pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274 container secret-volume-test: <nil>
STEP: delete the pod
May 30 07:54:51.392: INFO: Waiting for pod pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274 to disappear
May 30 07:54:51.401: INFO: Pod pod-projected-secrets-73554c8d-a339-465f-ab1c-2954826b1274 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:54:51.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-390" for this suite.

• [SLOW TEST:17.647 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":303,"completed":254,"skipped":4419,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:54:51.489: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 07:54:51.976: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7" in namespace "projected-1962" to be "Succeeded or Failed"
May 30 07:54:52.003: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.550481ms
May 30 07:54:54.011: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034244066s
May 30 07:54:56.016: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038801892s
May 30 07:54:58.032: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054862114s
May 30 07:55:00.126: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.14899933s
May 30 07:55:02.594: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.616801468s
May 30 07:55:04.754: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.777340088s
May 30 07:55:06.820: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Running", Reason="", readiness=true. Elapsed: 14.842883965s
May 30 07:55:09.078: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 17.101148495s
STEP: Saw pod success
May 30 07:55:09.079: INFO: Pod "downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7" satisfied condition "Succeeded or Failed"
May 30 07:55:09.468: INFO: Trying to get logs from node my-node pod downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7 container client-container: <nil>
STEP: delete the pod
May 30 07:55:09.698: INFO: Waiting for pod downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7 to disappear
May 30 07:55:09.737: INFO: Pod downwardapi-volume-c6eafbe6-d5cf-45c3-bc0a-84a3096862f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:55:09.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1962" for this suite.

• [SLOW TEST:18.274 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":303,"completed":255,"skipped":4426,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:55:09.828: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
May 30 07:55:10.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 create -f -'
May 30 07:55:15.884: INFO: stderr: ""
May 30 07:55:15.884: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 30 07:55:15.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 07:55:16.185: INFO: stderr: ""
May 30 07:55:16.185: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
May 30 07:55:21.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 07:55:21.325: INFO: stderr: ""
May 30 07:55:21.325: INFO: stdout: "update-demo-nautilus-25scx update-demo-nautilus-lgc7g "
May 30 07:55:21.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods update-demo-nautilus-25scx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 07:55:21.508: INFO: stderr: ""
May 30 07:55:21.508: INFO: stdout: ""
May 30 07:55:21.508: INFO: update-demo-nautilus-25scx is created but not running
May 30 07:55:26.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 30 07:55:26.677: INFO: stderr: ""
May 30 07:55:26.677: INFO: stdout: "update-demo-nautilus-25scx update-demo-nautilus-lgc7g "
May 30 07:55:26.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods update-demo-nautilus-25scx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 07:55:26.776: INFO: stderr: ""
May 30 07:55:26.776: INFO: stdout: "true"
May 30 07:55:26.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods update-demo-nautilus-25scx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 30 07:55:26.881: INFO: stderr: ""
May 30 07:55:26.881: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 30 07:55:26.881: INFO: validating pod update-demo-nautilus-25scx
May 30 07:55:27.028: INFO: got data: {
  "image": "nautilus.jpg"
}

May 30 07:55:27.039: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 30 07:55:27.041: INFO: update-demo-nautilus-25scx is verified up and running
May 30 07:55:27.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods update-demo-nautilus-lgc7g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 30 07:55:27.157: INFO: stderr: ""
May 30 07:55:27.157: INFO: stdout: "true"
May 30 07:55:27.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods update-demo-nautilus-lgc7g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 30 07:55:27.322: INFO: stderr: ""
May 30 07:55:27.322: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 30 07:55:27.322: INFO: validating pod update-demo-nautilus-lgc7g
May 30 07:55:27.329: INFO: got data: {
  "image": "nautilus.jpg"
}

May 30 07:55:27.329: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 30 07:55:27.329: INFO: update-demo-nautilus-lgc7g is verified up and running
STEP: using delete to clean up resources
May 30 07:55:27.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 delete --grace-period=0 --force -f -'
May 30 07:55:27.555: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:55:27.555: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 30 07:55:27.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get rc,svc -l name=update-demo --no-headers'
May 30 07:55:27.760: INFO: stderr: "No resources found in kubectl-9348 namespace.\n"
May 30 07:55:27.760: INFO: stdout: ""
May 30 07:55:27.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9348 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 30 07:55:27.870: INFO: stderr: ""
May 30 07:55:27.870: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:55:27.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9348" for this suite.

• [SLOW TEST:18.053 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":303,"completed":256,"skipped":4436,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:55:27.898: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
May 30 07:55:28.023: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 30 07:55:28.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 create -f -'
May 30 07:55:28.667: INFO: stderr: ""
May 30 07:55:28.667: INFO: stdout: "service/agnhost-replica created\n"
May 30 07:55:28.669: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 30 07:55:28.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 create -f -'
May 30 07:55:29.643: INFO: stderr: ""
May 30 07:55:29.643: INFO: stdout: "service/agnhost-primary created\n"
May 30 07:55:29.643: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 30 07:55:29.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 create -f -'
May 30 07:55:30.533: INFO: stderr: ""
May 30 07:55:30.533: INFO: stdout: "service/frontend created\n"
May 30 07:55:30.562: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 30 07:55:30.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 create -f -'
May 30 07:55:31.482: INFO: stderr: ""
May 30 07:55:31.482: INFO: stdout: "deployment.apps/frontend created\n"
May 30 07:55:31.483: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 30 07:55:31.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 create -f -'
May 30 07:55:32.053: INFO: stderr: ""
May 30 07:55:32.053: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 30 07:55:32.053: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 30 07:55:32.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 create -f -'
May 30 07:55:34.492: INFO: stderr: ""
May 30 07:55:34.493: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 30 07:55:34.493: INFO: Waiting for all frontend pods to be Running.
May 30 07:56:10.443: INFO: Waiting for frontend to serve content.
May 30 07:56:10.880: INFO: Trying to add a new entry to the guestbook.
May 30 07:56:10.959: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 30 07:56:10.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 delete --grace-period=0 --force -f -'
May 30 07:56:11.398: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:56:11.398: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 30 07:56:11.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 delete --grace-period=0 --force -f -'
May 30 07:56:11.893: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:56:11.893: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 30 07:56:11.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 delete --grace-period=0 --force -f -'
May 30 07:56:12.333: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:56:12.333: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 30 07:56:12.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 delete --grace-period=0 --force -f -'
May 30 07:56:12.824: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:56:12.824: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 30 07:56:12.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 delete --grace-period=0 --force -f -'
May 30 07:56:13.195: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:56:13.195: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 30 07:56:13.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=kubectl-9667 delete --grace-period=0 --force -f -'
May 30 07:56:13.860: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 30 07:56:13.861: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:56:13.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9667" for this suite.

• [SLOW TEST:46.440 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":303,"completed":257,"skipped":4461,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:56:14.740: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 30 07:56:19.000: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:56:44.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6710" for this suite.

• [SLOW TEST:29.425 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":303,"completed":258,"skipped":4473,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:56:44.180: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
May 30 07:56:44.397: INFO: Waiting up to 5m0s for pod "client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af" in namespace "containers-4660" to be "Succeeded or Failed"
May 30 07:56:44.414: INFO: Pod "client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af": Phase="Pending", Reason="", readiness=false. Elapsed: 16.58245ms
May 30 07:56:46.422: INFO: Pod "client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024089721s
May 30 07:56:48.514: INFO: Pod "client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115856593s
May 30 07:56:50.520: INFO: Pod "client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122493511s
May 30 07:56:52.725: INFO: Pod "client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.326853428s
STEP: Saw pod success
May 30 07:56:52.725: INFO: Pod "client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af" satisfied condition "Succeeded or Failed"
May 30 07:56:52.754: INFO: Trying to get logs from node my-node pod client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af container test-container: <nil>
STEP: delete the pod
May 30 07:56:53.092: INFO: Waiting for pod client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af to disappear
May 30 07:56:53.404: INFO: Pod client-containers-8e89074e-e554-4185-98e3-c1b5c36ef0af no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:56:53.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4660" for this suite.

• [SLOW TEST:9.250 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":303,"completed":259,"skipped":4478,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:56:53.430: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:56:54.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5467" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":303,"completed":260,"skipped":4478,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:56:54.064: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 30 07:56:54.326: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7800 /api/v1/namespaces/watch-7800/configmaps/e2e-watch-test-watch-closed 4a21f776-58d2-4ba6-905e-b28a15bce6f3 66181 0 2022-05-30 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-30 07:56:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 07:56:54.520: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7800 /api/v1/namespaces/watch-7800/configmaps/e2e-watch-test-watch-closed 4a21f776-58d2-4ba6-905e-b28a15bce6f3 66182 0 2022-05-30 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-30 07:56:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 30 07:56:54.613: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7800 /api/v1/namespaces/watch-7800/configmaps/e2e-watch-test-watch-closed 4a21f776-58d2-4ba6-905e-b28a15bce6f3 66183 0 2022-05-30 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-30 07:56:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 07:56:54.614: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7800 /api/v1/namespaces/watch-7800/configmaps/e2e-watch-test-watch-closed 4a21f776-58d2-4ba6-905e-b28a15bce6f3 66184 0 2022-05-30 07:56:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-30 07:56:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:56:54.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7800" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":303,"completed":261,"skipped":4496,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:56:54.640: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-1fea421c-4b9d-469b-ae35-96ddcee631d6
STEP: Creating a pod to test consume secrets
May 30 07:56:54.814: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217" in namespace "projected-3665" to be "Succeeded or Failed"
May 30 07:56:54.827: INFO: Pod "pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217": Phase="Pending", Reason="", readiness=false. Elapsed: 11.460452ms
May 30 07:56:57.000: INFO: Pod "pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184537303s
May 30 07:56:59.005: INFO: Pod "pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217": Phase="Pending", Reason="", readiness=false. Elapsed: 4.189560938s
May 30 07:57:01.021: INFO: Pod "pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.206105818s
STEP: Saw pod success
May 30 07:57:01.021: INFO: Pod "pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217" satisfied condition "Succeeded or Failed"
May 30 07:57:01.040: INFO: Trying to get logs from node my-node pod pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 30 07:57:01.209: INFO: Waiting for pod pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217 to disappear
May 30 07:57:01.222: INFO: Pod pod-projected-secrets-1e28725e-1fce-44f4-85f4-436a3d662217 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:57:01.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3665" for this suite.

• [SLOW TEST:6.596 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":262,"skipped":4535,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:57:01.237: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:57:07.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-311" for this suite.

• [SLOW TEST:6.279 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":303,"completed":263,"skipped":4547,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:57:07.516: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 30 07:57:07.719: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 30 07:57:20.956: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
May 30 07:57:23.820: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:57:32.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5294" for this suite.

• [SLOW TEST:24.844 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":303,"completed":264,"skipped":4563,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:57:32.361: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d25b0b58-ad92-42be-85c0-404ebc75fcc9
STEP: Creating a pod to test consume configMaps
May 30 07:57:32.574: INFO: Waiting up to 5m0s for pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf" in namespace "configmap-4400" to be "Succeeded or Failed"
May 30 07:57:32.584: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.99137ms
May 30 07:57:34.597: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02286168s
May 30 07:57:36.803: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.229244118s
May 30 07:57:38.914: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.339697699s
May 30 07:57:40.922: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf": Phase="Running", Reason="", readiness=true. Elapsed: 8.348060067s
May 30 07:57:43.274: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf": Phase="Running", Reason="", readiness=true. Elapsed: 10.699865708s
May 30 07:57:45.278: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.703647375s
STEP: Saw pod success
May 30 07:57:45.278: INFO: Pod "pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf" satisfied condition "Succeeded or Failed"
May 30 07:57:46.028: INFO: Trying to get logs from node my-node pod pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf container configmap-volume-test: <nil>
STEP: delete the pod
May 30 07:57:46.803: INFO: Waiting for pod pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf to disappear
May 30 07:57:46.838: INFO: Pod pod-configmaps-15f5c6be-28c5-4787-a43b-49aadc75edcf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:57:46.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4400" for this suite.

• [SLOW TEST:14.509 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":303,"completed":265,"skipped":4574,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:57:46.971: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 30 07:57:48.984: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
May 30 07:57:51.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494269, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:57:53.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494269, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:57:56.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494269, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:57:57.888: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494269, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 07:57:59.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494270, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494269, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 07:58:02.981: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:58:02.986: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:58:04.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5093" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:17.607 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":303,"completed":266,"skipped":4580,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:58:04.578: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 07:58:04.715: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285" in namespace "downward-api-2930" to be "Succeeded or Failed"
May 30 07:58:04.719: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146122ms
May 30 07:58:06.837: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285": Phase="Pending", Reason="", readiness=false. Elapsed: 2.122587096s
May 30 07:58:09.083: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285": Phase="Pending", Reason="", readiness=false. Elapsed: 4.367832235s
May 30 07:58:11.487: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285": Phase="Pending", Reason="", readiness=false. Elapsed: 6.77271406s
May 30 07:58:13.514: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285": Phase="Pending", Reason="", readiness=false. Elapsed: 8.799501359s
May 30 07:58:15.524: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285": Phase="Running", Reason="", readiness=true. Elapsed: 10.809471564s
May 30 07:58:18.009: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285": Phase="Succeeded", Reason="", readiness=false. Elapsed: 13.29470327s
STEP: Saw pod success
May 30 07:58:18.010: INFO: Pod "downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285" satisfied condition "Succeeded or Failed"
May 30 07:58:18.020: INFO: Trying to get logs from node my-node pod downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285 container client-container: <nil>
STEP: delete the pod
May 30 07:58:18.121: INFO: Waiting for pod downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285 to disappear
May 30 07:58:18.126: INFO: Pod downwardapi-volume-78f8784f-a7a3-43c4-bffb-0deaf38a8285 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:58:18.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2930" for this suite.

• [SLOW TEST:13.584 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":303,"completed":267,"skipped":4582,"failed":2,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:58:18.236: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
May 30 07:58:18.454: INFO: Waiting up to 1m0s for all nodes to be ready
May 30 07:59:18.567: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
May 30 07:59:18.946: INFO: Created pod: pod0-sched-preemption-low-priority
May 30 07:59:19.099: FAIL: We need at least two pods to be created butall nodes are already heavily utilized, so preemption tests cannot be run

Full Stack Trace
k8s.io/kubernetes/test/e2e/scheduling.glob..func5.3()
	/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:167 +0xf86
k8s.io/kubernetes/test/e2e.RunE2ETests(0xc001dde480)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x345
k8s.io/kubernetes/test/e2e.TestE2E(0xc001dde480)
	_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x2b
testing.tRunner(0xc001dde480, 0x4df04f8)
	/usr/local/go/src/testing/testing.go:1123 +0xef
created by testing.(*T).Run
	/usr/local/go/src/testing/testing.go:1168 +0x2b3
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "sched-preemption-331".
STEP: Found 0 events.
May 30 07:59:19.272: INFO: POD                                 NODE  PHASE    GRACE  CONDITIONS
May 30 07:59:19.272: INFO: pod0-sched-preemption-low-priority        Pending         []
May 30 07:59:19.272: INFO: 
May 30 07:59:19.279: INFO: 
Logging node info for node my-node
May 30 07:59:19.283: INFO: Node Info: &Node{ObjectMeta:{my-node   /api/v1/nodes/my-node f75ed23b-2f88-4ce1-8189-6f41b657b75d 66723 0 2022-05-27 09:42:12 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:my-node kubernetes.io/os:linux node-role.kubernetes.io/master:] map[flannel.alpha.coreos.com/backend-data:{"VNI":1,"VtepMAC":"ee:c9:e8:56:92:a4"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.0.2.15 kubeadm.alpha.kubernetes.io/cri-socket:/var/run/dockershim.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2022-05-27 09:42:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/master":{}}}}} {flanneld Update v1 2022-05-27 09:44:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{},"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}}} {kube-controller-manager Update v1 2022-05-27 09:44:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.244.0.0/24\"":{}}}}} {kubelet Update v1 2022-05-30 07:54:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {e2e.test Update v1 2022-05-30 07:59:18 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{},"f:scheduling.k8s.io/foo":{}}}}}]},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.244.0.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{138856275968 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{4116410368 0} {<nil>} 4019932Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},scheduling.k8s.io/foo: {{3 0} {<nil>} 3 DecimalSI},},Allocatable:ResourceList{cpu: {{3 0} {<nil>} 3 DecimalSI},ephemeral-storage: {{124970648165 0} {<nil>} 124970648165 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{4011552768 0} {<nil>} 3917532Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2022-05-30 01:39:18 +0000 UTC,LastTransitionTime:2022-05-30 01:39:18 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2022-05-30 07:54:38 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2022-05-30 07:54:38 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2022-05-30 07:54:38 +0000 UTC,LastTransitionTime:2022-05-27 09:42:10 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2022-05-30 07:54:38 +0000 UTC,LastTransitionTime:2022-05-27 09:44:25 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:10.0.2.15,},NodeAddress{Type:Hostname,Address:my-node,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:30ab1f02f59e4093b9f48497b6753f85,SystemUUID:a3aa5fdb-d52d-9a42-8fa5-7062181ac666,BootID:67aea70c-c63d-41d9-9999-5d2b2003e8de,KernelVersion:5.13.0-44-generic,OSImage:Ubuntu 20.04.4 LTS,ContainerRuntimeVersion:docker://19.3.15,KubeletVersion:v1.19.16,KubeProxyVersion:v1.19.16,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd sonobuoy/systemd-logs:v0.4],SizeBytes:314096343,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:253392289,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:e1ec90b9a30894aa71eaa6cdf7dcaae6f5d899692460b58973cdc2c26dc72afa k8s.gcr.io/conformance:v1.19.16],SizeBytes:229709026,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils@sha256:ad583e33cb284f7ef046673809b146ec4053cda19b54a85d2b180a86169715eb gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0],SizeBytes:195659796,},ContainerImage{Names:[httpd@sha256:addd70e4ee83f3bc9a4c1c7c41e37927ba47faf639312fc936df3afad7926f5a httpd:2.4.39-alpine],SizeBytes:126894770,},ContainerImage{Names:[httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060 httpd:2.4.38-alpine],SizeBytes:123781643,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver@sha256:2cad6a4cae1713e720e00e1a1c7ef7644777fe111e0b7cbed5f50adb8a3cdf30 k8s.gcr.io/kube-apiserver:v1.19.16],SizeBytes:118899101,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0 k8s.gcr.io/e2e-test-images/agnhost:2.20],SizeBytes:113869866,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager@sha256:1d42f7d017f4ceeff46afb9cedd47b2b8140d1f2cc3bfebb57a40c3760acd482 k8s.gcr.io/kube-controller-manager:v1.19.16],SizeBytes:110870941,},ContainerImage{Names:[k8s.gcr.io/kube-proxy@sha256:092f9526686d27964d17be772c42cde086690209cc8aea10c49991456eb879c2 k8s.gcr.io/kube-proxy:v1.19.16],SizeBytes:98938510,},ContainerImage{Names:[rancher/mirrored-flannelcni-flannel@sha256:bf56c4e594948eb5c6b3bc04cdde3477d6de12a85b3bbb67ae3a518142cd8392 rancher/mirrored-flannelcni-flannel:v0.18.0],SizeBytes:62262505,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/sample-apiserver@sha256:ff02aacd9766d597883fabafc7ad604c719a57611db1bcc1564c69a45b000a55 gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.17],SizeBytes:60684726,},ContainerImage{Names:[k8s.gcr.io/kube-scheduler@sha256:1a335251eaef4e209d5757da0bf5499ecdce6e65413f7cb92ff4cc633d6fc7dd k8s.gcr.io/kube-scheduler:v1.19.16],SizeBytes:46490013,},ContainerImage{Names:[k8s.gcr.io/coredns@sha256:73ca82b4ce829766d4f1f10947c3a338888f876fbed0540dc849c89ff256e90c k8s.gcr.io/coredns:1.7.0],SizeBytes:45227747,},ContainerImage{Names:[sonobuoy/sonobuoy@sha256:ed5bd072698f80e7fa706a69e4b01eeedbddeadc4821587297a29b727b5b5f19 sonobuoy/sonobuoy:v0.56.6],SizeBytes:44096145,},ContainerImage{Names:[nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 nginx:1.14-alpine],SizeBytes:16032814,},ContainerImage{Names:[rancher/mirrored-flannelcni-flannel-cni-plugin@sha256:28d3a6be9f450282bf42e4dad143d41da23e3d91f66f19c01ee7fd21fd17cb2b rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0],SizeBytes:8087907,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nonewprivs@sha256:10066e9039219449fe3c81f38fe01928f87914150768ab81b62a468e51fa7411 gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0],SizeBytes:6757579,},ContainerImage{Names:[gcr.io/kubernetes-e2e-test-images/nautilus@sha256:33a732d4c42a266912a5091598a0f07653c9134db4b8d571690d8afd509e0bfc gcr.io/kubernetes-e2e-test-images/nautilus:1.0],SizeBytes:4753501,},ContainerImage{Names:[busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796 busybox:1.29],SizeBytes:1154361,},ContainerImage{Names:[k8s.gcr.io/pause@sha256:927d98197ec1141a368550822d18fa1c60bdae27b78b0c004f705f548c07814f k8s.gcr.io/pause:3.2],SizeBytes:682696,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
May 30 07:59:19.379: INFO: 
Logging kubelet events for node my-node
May 30 07:59:19.395: INFO: 
Logging pods the kubelet thinks is on node my-node
May 30 07:59:19.829: INFO: coredns-f9fd979d6-lrzv9 started at 2022-05-27 09:44:25 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container coredns ready: true, restart count 1
May 30 07:59:19.829: INFO: kube-scheduler-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container kube-scheduler ready: true, restart count 1
May 30 07:59:19.829: INFO: coredns-f9fd979d6-c9tcq started at 2022-05-27 09:44:35 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container coredns ready: true, restart count 1
May 30 07:59:19.829: INFO: kube-controller-manager-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 30 07:59:19.829: INFO: sonobuoy started at 2022-05-30 06:08:17 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 30 07:59:19.829: INFO: etcd-my-node started at 2022-05-27 09:42:24 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container etcd ready: true, restart count 1
May 30 07:59:19.829: INFO: sonobuoy-e2e-job-8e0fe1b369e84dc8 started at 2022-05-30 06:08:24 +0000 UTC (0+2 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container e2e ready: true, restart count 0
May 30 07:59:19.829: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:59:19.829: INFO: sonobuoy-systemd-logs-daemon-set-94b9056f9d9d46f8-7m798 started at 2022-05-30 06:08:25 +0000 UTC (0+2 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 30 07:59:19.829: INFO: 	Container systemd-logs ready: true, restart count 0
May 30 07:59:19.829: INFO: kube-proxy-4pwdh started at 2022-05-27 09:42:33 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container kube-proxy ready: true, restart count 1
May 30 07:59:19.829: INFO: kube-flannel-ds-lv5nv started at 2022-05-27 09:43:53 +0000 UTC (2+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Init container install-cni-plugin ready: true, restart count 1
May 30 07:59:19.829: INFO: 	Init container install-cni ready: true, restart count 0
May 30 07:59:19.829: INFO: 	Container kube-flannel ready: true, restart count 1
May 30 07:59:19.829: INFO: kube-apiserver-my-node started at 2022-05-30 01:38:05 +0000 UTC (0+1 container statuses recorded)
May 30 07:59:19.829: INFO: 	Container kube-apiserver ready: true, restart count 1
W0530 07:59:19.932405      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 07:59:20.621: INFO: 
Latency metrics for node my-node
May 30 07:59:20.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-331" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• Failure [62.503 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance] [It]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

  May 30 07:59:19.099: We need at least two pods to be created butall nodes are already heavily utilized, so preemption tests cannot be run

  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:167
------------------------------
{"msg":"FAILED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":303,"completed":267,"skipped":4621,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:59:20.749: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 07:59:20.866: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 30 07:59:20.920: INFO: Number of nodes with available pods: 0
May 30 07:59:20.920: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 30 07:59:21.055: INFO: Number of nodes with available pods: 0
May 30 07:59:21.055: INFO: Node my-node is running more than one daemon pod
May 30 07:59:22.386: INFO: Number of nodes with available pods: 0
May 30 07:59:22.386: INFO: Node my-node is running more than one daemon pod
May 30 07:59:23.083: INFO: Number of nodes with available pods: 0
May 30 07:59:23.083: INFO: Node my-node is running more than one daemon pod
May 30 07:59:24.089: INFO: Number of nodes with available pods: 0
May 30 07:59:24.089: INFO: Node my-node is running more than one daemon pod
May 30 07:59:25.124: INFO: Number of nodes with available pods: 0
May 30 07:59:25.124: INFO: Node my-node is running more than one daemon pod
May 30 07:59:26.068: INFO: Number of nodes with available pods: 1
May 30 07:59:26.068: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 30 07:59:26.198: INFO: Number of nodes with available pods: 1
May 30 07:59:26.198: INFO: Number of running nodes: 0, number of available pods: 1
May 30 07:59:27.204: INFO: Number of nodes with available pods: 0
May 30 07:59:27.205: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 30 07:59:27.230: INFO: Number of nodes with available pods: 0
May 30 07:59:27.230: INFO: Node my-node is running more than one daemon pod
May 30 07:59:28.698: INFO: Number of nodes with available pods: 0
May 30 07:59:28.698: INFO: Node my-node is running more than one daemon pod
May 30 07:59:29.247: INFO: Number of nodes with available pods: 0
May 30 07:59:29.248: INFO: Node my-node is running more than one daemon pod
May 30 07:59:30.234: INFO: Number of nodes with available pods: 0
May 30 07:59:30.234: INFO: Node my-node is running more than one daemon pod
May 30 07:59:31.235: INFO: Number of nodes with available pods: 0
May 30 07:59:31.235: INFO: Node my-node is running more than one daemon pod
May 30 07:59:32.237: INFO: Number of nodes with available pods: 0
May 30 07:59:32.239: INFO: Node my-node is running more than one daemon pod
May 30 07:59:33.235: INFO: Number of nodes with available pods: 0
May 30 07:59:33.235: INFO: Node my-node is running more than one daemon pod
May 30 07:59:34.236: INFO: Number of nodes with available pods: 0
May 30 07:59:34.236: INFO: Node my-node is running more than one daemon pod
May 30 07:59:35.234: INFO: Number of nodes with available pods: 0
May 30 07:59:35.234: INFO: Node my-node is running more than one daemon pod
May 30 07:59:36.237: INFO: Number of nodes with available pods: 0
May 30 07:59:36.237: INFO: Node my-node is running more than one daemon pod
May 30 07:59:37.244: INFO: Number of nodes with available pods: 0
May 30 07:59:37.246: INFO: Node my-node is running more than one daemon pod
May 30 07:59:38.617: INFO: Number of nodes with available pods: 0
May 30 07:59:38.617: INFO: Node my-node is running more than one daemon pod
May 30 07:59:39.240: INFO: Number of nodes with available pods: 0
May 30 07:59:39.240: INFO: Node my-node is running more than one daemon pod
May 30 07:59:40.236: INFO: Number of nodes with available pods: 1
May 30 07:59:40.236: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5407, will wait for the garbage collector to delete the pods
May 30 07:59:40.383: INFO: Deleting DaemonSet.extensions daemon-set took: 19.48678ms
May 30 07:59:40.971: INFO: Terminating DaemonSet.extensions daemon-set pods took: 588.339603ms
May 30 07:59:45.875: INFO: Number of nodes with available pods: 0
May 30 07:59:45.875: INFO: Number of running nodes: 0, number of available pods: 0
May 30 07:59:45.879: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5407/daemonsets","resourceVersion":"66850"},"items":null}

May 30 07:59:45.883: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5407/pods","resourceVersion":"66850"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:59:45.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5407" for this suite.

• [SLOW TEST:25.180 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":303,"completed":268,"skipped":4643,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:59:46.049: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 30 07:59:54.904: INFO: Successfully updated pod "adopt-release-4n7ht"
STEP: Checking that the Job readopts the Pod
May 30 07:59:54.904: INFO: Waiting up to 15m0s for pod "adopt-release-4n7ht" in namespace "job-1816" to be "adopted"
May 30 07:59:54.920: INFO: Pod "adopt-release-4n7ht": Phase="Running", Reason="", readiness=true. Elapsed: 15.760717ms
May 30 07:59:56.924: INFO: Pod "adopt-release-4n7ht": Phase="Running", Reason="", readiness=true. Elapsed: 2.01959305s
May 30 07:59:56.924: INFO: Pod "adopt-release-4n7ht" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 30 07:59:57.433: INFO: Successfully updated pod "adopt-release-4n7ht"
STEP: Checking that the Job releases the Pod
May 30 07:59:57.433: INFO: Waiting up to 15m0s for pod "adopt-release-4n7ht" in namespace "job-1816" to be "released"
May 30 07:59:57.459: INFO: Pod "adopt-release-4n7ht": Phase="Running", Reason="", readiness=true. Elapsed: 25.75112ms
May 30 07:59:59.790: INFO: Pod "adopt-release-4n7ht": Phase="Running", Reason="", readiness=true. Elapsed: 2.357194693s
May 30 07:59:59.790: INFO: Pod "adopt-release-4n7ht" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 07:59:59.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1816" for this suite.

• [SLOW TEST:13.754 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":303,"completed":269,"skipped":4654,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 07:59:59.803: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 30 08:00:07.565: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c0045b9e-0d99-44a4-b433-e5652783bf16"
May 30 08:00:07.566: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c0045b9e-0d99-44a4-b433-e5652783bf16" in namespace "pods-7533" to be "terminated due to deadline exceeded"
May 30 08:00:07.586: INFO: Pod "pod-update-activedeadlineseconds-c0045b9e-0d99-44a4-b433-e5652783bf16": Phase="Running", Reason="", readiness=true. Elapsed: 8.661503ms
May 30 08:00:09.645: INFO: Pod "pod-update-activedeadlineseconds-c0045b9e-0d99-44a4-b433-e5652783bf16": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.067084329s
May 30 08:00:09.645: INFO: Pod "pod-update-activedeadlineseconds-c0045b9e-0d99-44a4-b433-e5652783bf16" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:00:09.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7533" for this suite.

• [SLOW TEST:10.773 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":303,"completed":270,"skipped":4671,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:00:10.715: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-781cdc0f-2509-4d6e-8d74-e5feb95c025f
STEP: Creating a pod to test consume configMaps
May 30 08:00:10.993: INFO: Waiting up to 5m0s for pod "pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1" in namespace "configmap-1604" to be "Succeeded or Failed"
May 30 08:00:10.996: INFO: Pod "pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691416ms
May 30 08:00:13.001: INFO: Pod "pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007748821s
May 30 08:00:15.070: INFO: Pod "pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07720996s
May 30 08:00:17.355: INFO: Pod "pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.362465621s
STEP: Saw pod success
May 30 08:00:17.360: INFO: Pod "pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1" satisfied condition "Succeeded or Failed"
May 30 08:00:17.898: INFO: Trying to get logs from node my-node pod pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1 container configmap-volume-test: <nil>
STEP: delete the pod
May 30 08:00:18.318: INFO: Waiting for pod pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1 to disappear
May 30 08:00:18.325: INFO: Pod pod-configmaps-d64f7770-a430-4385-834d-36a0d1f889d1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:00:18.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1604" for this suite.

• [SLOW TEST:7.621 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":303,"completed":271,"skipped":4686,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:00:18.336: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 08:00:18.496: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 30 08:00:21.292: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:00:22.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3748" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":303,"completed":272,"skipped":4697,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:00:22.480: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0530 08:00:26.969509      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 30 08:01:29.525: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:01:29.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-384" for this suite.

• [SLOW TEST:67.057 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":303,"completed":273,"skipped":4727,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:01:29.557: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 08:01:29.629: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 30 08:01:34.673: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 30 08:01:34.701: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
May 30 08:01:35.303: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7445 /apis/apps/v1/namespaces/deployment-7445/deployments/test-cleanup-deployment 4ddd4a0e-95e5-4f60-bd8b-40a32152361a 67346 1 2022-05-30 08:01:34 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-05-30 08:01:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b7bc18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 30 08:01:35.344: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-7445 /apis/apps/v1/namespaces/deployment-7445/replicasets/test-cleanup-deployment-5d446bdd47 0508b3e9-b78d-48d3-95f2-f19d1b64fa27 67351 1 2022-05-30 08:01:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 4ddd4a0e-95e5-4f60-bd8b-40a32152361a 0xc005bec237 0xc005bec238}] []  [{kube-controller-manager Update apps/v1 2022-05-30 08:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ddd4a0e-95e5-4f60-bd8b-40a32152361a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005bec2e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 30 08:01:35.344: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 30 08:01:35.345: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7445 /apis/apps/v1/namespaces/deployment-7445/replicasets/test-cleanup-controller 9c9e4b6e-e9bf-4c52-9e72-35a3e025497a 67349 1 2022-05-30 08:01:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4ddd4a0e-95e5-4f60-bd8b-40a32152361a 0xc005bec127 0xc005bec128}] []  [{e2e.test Update apps/v1 2022-05-30 08:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-05-30 08:01:34 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4ddd4a0e-95e5-4f60-bd8b-40a32152361a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005bec1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 30 08:01:36.119: INFO: Pod "test-cleanup-controller-nh8cx" is available:
&Pod{ObjectMeta:{test-cleanup-controller-nh8cx test-cleanup-controller- deployment-7445 /api/v1/namespaces/deployment-7445/pods/test-cleanup-controller-nh8cx 47818763-cfcd-4ef0-bab3-c3b8e062cdee 67336 0 2022-05-30 08:01:29 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 9c9e4b6e-e9bf-4c52-9e72-35a3e025497a 0xc005bec7a7 0xc005bec7a8}] []  [{kube-controller-manager Update v1 2022-05-30 08:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c9e4b6e-e9bf-4c52-9e72-35a3e025497a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 08:01:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m4vb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m4vb8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m4vb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:01:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:01:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:01:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.145,StartTime:2022-05-30 08:01:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 08:01:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8d9743dc677416813700ae4f98dc15140e4cb7c3979af440deac3ae6f9bdf689,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 30 08:01:36.119: INFO: Pod "test-cleanup-deployment-5d446bdd47-sb47d" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-sb47d test-cleanup-deployment-5d446bdd47- deployment-7445 /api/v1/namespaces/deployment-7445/pods/test-cleanup-deployment-5d446bdd47-sb47d 3663023b-2bb1-4448-8ed0-3b7425d4acfe 67354 0 2022-05-30 08:01:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 0508b3e9-b78d-48d3-95f2-f19d1b64fa27 0xc005bec957 0xc005bec958}] []  [{kube-controller-manager Update v1 2022-05-30 08:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0508b3e9-b78d-48d3-95f2-f19d1b64fa27\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m4vb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m4vb8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m4vb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:01:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:01:36.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7445" for this suite.

• [SLOW TEST:6.656 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":303,"completed":274,"skipped":4752,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:01:36.230: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
May 30 08:01:36.376: INFO: Waiting up to 5m0s for pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757" in namespace "emptydir-9279" to be "Succeeded or Failed"
May 30 08:01:36.402: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757": Phase="Pending", Reason="", readiness=false. Elapsed: 25.163663ms
May 30 08:01:38.535: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.15848569s
May 30 08:01:40.538: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757": Phase="Pending", Reason="", readiness=false. Elapsed: 4.161667472s
May 30 08:01:42.559: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757": Phase="Pending", Reason="", readiness=false. Elapsed: 6.182263889s
May 30 08:01:44.568: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757": Phase="Pending", Reason="", readiness=false. Elapsed: 8.190814704s
May 30 08:01:46.851: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757": Phase="Pending", Reason="", readiness=false. Elapsed: 10.474613258s
May 30 08:01:49.004: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.627383795s
STEP: Saw pod success
May 30 08:01:49.004: INFO: Pod "pod-9472bdea-ae5c-42a5-9ab4-529fdccce757" satisfied condition "Succeeded or Failed"
May 30 08:01:49.011: INFO: Trying to get logs from node my-node pod pod-9472bdea-ae5c-42a5-9ab4-529fdccce757 container test-container: <nil>
STEP: delete the pod
May 30 08:01:50.101: INFO: Waiting for pod pod-9472bdea-ae5c-42a5-9ab4-529fdccce757 to disappear
May 30 08:01:50.162: INFO: Pod pod-9472bdea-ae5c-42a5-9ab4-529fdccce757 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:01:50.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9279" for this suite.

• [SLOW TEST:13.950 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":275,"skipped":4762,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:01:50.179: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
May 30 08:01:50.246: INFO: Waiting up to 5m0s for pod "var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802" in namespace "var-expansion-4136" to be "Succeeded or Failed"
May 30 08:01:50.267: INFO: Pod "var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802": Phase="Pending", Reason="", readiness=false. Elapsed: 21.467404ms
May 30 08:01:52.272: INFO: Pod "var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026692372s
May 30 08:01:54.278: INFO: Pod "var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03194271s
May 30 08:01:56.281: INFO: Pod "var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035610367s
STEP: Saw pod success
May 30 08:01:56.281: INFO: Pod "var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802" satisfied condition "Succeeded or Failed"
May 30 08:01:56.292: INFO: Trying to get logs from node my-node pod var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802 container dapi-container: <nil>
STEP: delete the pod
May 30 08:01:56.581: INFO: Waiting for pod var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802 to disappear
May 30 08:01:56.651: INFO: Pod var-expansion-bb6bf7bc-b7d8-4343-94bb-98fb90de1802 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:01:56.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4136" for this suite.

• [SLOW TEST:6.521 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":303,"completed":276,"skipped":4783,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:01:56.701: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 08:03:56.832: INFO: Deleting pod "var-expansion-eafa7b34-dcb3-4bef-8db6-dd3d436077db" in namespace "var-expansion-3222"
May 30 08:03:56.841: INFO: Wait up to 5m0s for pod "var-expansion-eafa7b34-dcb3-4bef-8db6-dd3d436077db" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:03:58.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3222" for this suite.

• [SLOW TEST:122.294 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":303,"completed":277,"skipped":4806,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:03:58.996: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 08:03:59.725: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 08:04:01.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 08:04:03.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494639, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 08:04:06.822: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:04:06.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1132" for this suite.
STEP: Destroying namespace "webhook-1132-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.025 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":303,"completed":278,"skipped":4819,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:04:07.021: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-f232794b-6656-4fbd-b936-03820bdf144e
STEP: Creating a pod to test consume secrets
May 30 08:04:07.428: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a" in namespace "projected-9264" to be "Succeeded or Failed"
May 30 08:04:07.497: INFO: Pod "pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a": Phase="Pending", Reason="", readiness=false. Elapsed: 68.803475ms
May 30 08:04:09.555: INFO: Pod "pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126928198s
May 30 08:04:11.560: INFO: Pod "pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131338527s
May 30 08:04:13.582: INFO: Pod "pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.153103118s
May 30 08:04:15.601: INFO: Pod "pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.173065331s
STEP: Saw pod success
May 30 08:04:15.602: INFO: Pod "pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a" satisfied condition "Succeeded or Failed"
May 30 08:04:15.608: INFO: Trying to get logs from node my-node pod pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 30 08:04:16.562: INFO: Waiting for pod pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a to disappear
May 30 08:04:16.602: INFO: Pod pod-projected-secrets-1da90740-a36f-49e9-b287-d43f9ccd831a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:04:16.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9264" for this suite.

• [SLOW TEST:9.598 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":303,"completed":279,"skipped":4822,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:04:16.621: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-17174657-e7f4-4542-9e78-757804d51056
STEP: Creating a pod to test consume configMaps
May 30 08:04:16.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6" in namespace "configmap-5482" to be "Succeeded or Failed"
May 30 08:04:16.786: INFO: Pod "pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.841597ms
May 30 08:04:18.793: INFO: Pod "pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010825425s
May 30 08:04:20.802: INFO: Pod "pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019273508s
May 30 08:04:22.817: INFO: Pod "pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034314745s
STEP: Saw pod success
May 30 08:04:22.817: INFO: Pod "pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6" satisfied condition "Succeeded or Failed"
May 30 08:04:22.820: INFO: Trying to get logs from node my-node pod pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6 container configmap-volume-test: <nil>
STEP: delete the pod
May 30 08:04:23.964: INFO: Waiting for pod pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6 to disappear
May 30 08:04:24.131: INFO: Pod pod-configmaps-4537379d-aa6c-4b73-b9c7-2a8374fc54b6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:04:24.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5482" for this suite.

• [SLOW TEST:7.541 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":280,"skipped":4866,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:04:24.162: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 30 08:04:24.257: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8961 /api/v1/namespaces/watch-8961/configmaps/e2e-watch-test-resource-version aeec6291-0b2a-497a-88dc-9ac2bc620315 67964 0 2022-05-30 08:04:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-30 08:04:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 08:04:24.257: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8961 /api/v1/namespaces/watch-8961/configmaps/e2e-watch-test-resource-version aeec6291-0b2a-497a-88dc-9ac2bc620315 67965 0 2022-05-30 08:04:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-30 08:04:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:04:24.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8961" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":303,"completed":281,"skipped":4873,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:04:24.273: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 30 08:04:24.323: INFO: Waiting up to 5m0s for pod "pod-36bfe87a-d803-4c7f-a09a-81f244d4042e" in namespace "emptydir-1369" to be "Succeeded or Failed"
May 30 08:04:24.338: INFO: Pod "pod-36bfe87a-d803-4c7f-a09a-81f244d4042e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.083841ms
May 30 08:04:26.352: INFO: Pod "pod-36bfe87a-d803-4c7f-a09a-81f244d4042e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028622978s
May 30 08:04:28.474: INFO: Pod "pod-36bfe87a-d803-4c7f-a09a-81f244d4042e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.150947933s
May 30 08:04:30.492: INFO: Pod "pod-36bfe87a-d803-4c7f-a09a-81f244d4042e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.168207898s
STEP: Saw pod success
May 30 08:04:30.492: INFO: Pod "pod-36bfe87a-d803-4c7f-a09a-81f244d4042e" satisfied condition "Succeeded or Failed"
May 30 08:04:30.499: INFO: Trying to get logs from node my-node pod pod-36bfe87a-d803-4c7f-a09a-81f244d4042e container test-container: <nil>
STEP: delete the pod
May 30 08:04:30.555: INFO: Waiting for pod pod-36bfe87a-d803-4c7f-a09a-81f244d4042e to disappear
May 30 08:04:30.558: INFO: Pod pod-36bfe87a-d803-4c7f-a09a-81f244d4042e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:04:30.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1369" for this suite.

• [SLOW TEST:6.304 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":303,"completed":282,"skipped":4886,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:04:30.578: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
May 30 08:04:37.219: INFO: Successfully updated pod "labelsupdateb3622708-e682-4c6f-9043-ea61ce03c7af"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:04:39.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-754" for this suite.

• [SLOW TEST:8.719 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":303,"completed":283,"skipped":4887,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:04:39.298: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:04:52.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-145" for this suite.

• [SLOW TEST:13.419 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":303,"completed":284,"skipped":4890,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:04:52.722: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9197.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9197.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9197.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9197.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9197.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9197.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 30 08:05:01.081: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9197/dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f: the server could not find the requested resource (get pods dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f)
May 30 08:05:01.086: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-9197.svc.cluster.local from pod dns-9197/dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f: the server could not find the requested resource (get pods dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f)
May 30 08:05:01.092: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-9197/dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f: the server could not find the requested resource (get pods dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f)
May 30 08:05:01.107: INFO: Unable to read jessie_udp@PodARecord from pod dns-9197/dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f: the server could not find the requested resource (get pods dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f)
May 30 08:05:01.115: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9197/dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f: the server could not find the requested resource (get pods dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f)
May 30 08:05:01.115: INFO: Lookups using dns-9197/dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f failed for: [wheezy_tcp@PodARecord jessie_hosts@dns-querier-1.dns-test-service.dns-9197.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

May 30 08:05:06.168: INFO: DNS probes using dns-9197/dns-test-5773b6b0-5273-419d-92b5-db29de4bc54f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:05:06.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9197" for this suite.

• [SLOW TEST:13.514 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":303,"completed":285,"skipped":4920,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:05:06.237: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
May 30 08:05:15.429: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3317 pod-service-account-65671bd3-79d1-4b94-b021-6a6a22217484 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 30 08:05:18.161: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3317 pod-service-account-65671bd3-79d1-4b94-b021-6a6a22217484 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 30 08:05:18.455: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3317 pod-service-account-65671bd3-79d1-4b94-b021-6a6a22217484 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:05:18.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3317" for this suite.

• [SLOW TEST:12.675 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":303,"completed":286,"skipped":4926,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:05:18.912: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6210
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6210
I0530 08:05:19.257380      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6210, replica count: 2
I0530 08:05:22.308544      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 08:05:25.312032      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0530 08:05:28.312325      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 30 08:05:28.312: INFO: Creating new exec pod
May 30 08:05:35.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-6210 exec execpodq8cdh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 30 08:05:35.960: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 30 08:05:35.960: INFO: stdout: ""
May 30 08:05:35.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-6210 exec execpodq8cdh -- /bin/sh -x -c nc -zv -t -w 2 10.101.38.178 80'
May 30 08:05:36.258: INFO: stderr: "+ nc -zv -t -w 2 10.101.38.178 80\nConnection to 10.101.38.178 80 port [tcp/http] succeeded!\n"
May 30 08:05:36.258: INFO: stdout: ""
May 30 08:05:36.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=services-6210 exec execpodq8cdh -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.15 30667'
May 30 08:05:36.659: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.15 30667\nConnection to 10.0.2.15 30667 port [tcp/30667] succeeded!\n"
May 30 08:05:36.659: INFO: stdout: ""
May 30 08:05:36.659: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:05:36.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6210" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.853 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":303,"completed":287,"skipped":4934,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:05:36.765: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
May 30 08:05:36.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9" in namespace "projected-6800" to be "Succeeded or Failed"
May 30 08:05:36.850: INFO: Pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.27596ms
May 30 08:05:38.868: INFO: Pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033271846s
May 30 08:05:40.983: INFO: Pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.148265752s
May 30 08:05:43.222: INFO: Pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.386929583s
May 30 08:05:45.396: INFO: Pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.56066991s
May 30 08:05:47.496: INFO: Pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.660967473s
STEP: Saw pod success
May 30 08:05:47.496: INFO: Pod "downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9" satisfied condition "Succeeded or Failed"
May 30 08:05:47.902: INFO: Trying to get logs from node my-node pod downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9 container client-container: <nil>
STEP: delete the pod
May 30 08:05:50.393: INFO: Waiting for pod downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9 to disappear
May 30 08:05:50.401: INFO: Pod downwardapi-volume-781b27bd-e1dd-4747-ba97-ea667afb88b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:05:50.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6800" for this suite.

• [SLOW TEST:13.702 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":303,"completed":288,"skipped":4940,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:05:50.467: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:06:34.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2747" for this suite.

• [SLOW TEST:44.050 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":303,"completed":289,"skipped":4984,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:06:34.521: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-2996
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2996
STEP: Deleting pre-stop pod
May 30 08:06:51.646: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:06:51.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2996" for this suite.

• [SLOW TEST:17.220 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":303,"completed":290,"skipped":5029,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:06:51.741: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:06:51.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2371" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":303,"completed":291,"skipped":5029,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:06:51.836: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
May 30 08:06:52.430: INFO: PodSpec: initContainers in spec.initContainers
May 30 08:07:49.623: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-506d7c80-6477-43b1-aeee-d708e8dab2cc", GenerateName:"", Namespace:"init-container-7892", SelfLink:"/api/v1/namespaces/init-container-7892/pods/pod-init-506d7c80-6477-43b1-aeee-d708e8dab2cc", UID:"04dce34f-889a-4459-ae89-4469114bf223", ResourceVersion:"68823", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63789494812, loc:(*time.Location)(0x771eac0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"430260453"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004606440), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004606460)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004606500), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004606520)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7qpzz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001c6adc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qpzz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qpzz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7qpzz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c8f648), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"my-node", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002c52a80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c8f6d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c8f6f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c8f6f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c8f6fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003e52600), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494813, loc:(*time.Location)(0x771eac0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494813, loc:(*time.Location)(0x771eac0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494813, loc:(*time.Location)(0x771eac0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494812, loc:(*time.Location)(0x771eac0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.2.15", PodIP:"10.244.0.166", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.0.166"}}, StartTime:(*v1.Time)(0xc004606540), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c52b60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c52bd0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://150ce1e0a6032c197b4dbc3d8779bc5a3262a4a68d4099406182513716b7e111", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004606580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004606560), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc002c8f77f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:07:49.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7892" for this suite.

• [SLOW TEST:57.949 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":303,"completed":292,"skipped":5058,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:07:49.785: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
May 30 08:07:49.825: INFO: created test-event-1
May 30 08:07:49.859: INFO: created test-event-2
May 30 08:07:49.864: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 30 08:07:49.868: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 30 08:07:49.898: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:07:49.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3999" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":303,"completed":293,"skipped":5072,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:07:49.909: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:07:49.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2163" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":303,"completed":294,"skipped":5080,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:07:50.000: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:08:07.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7006" for this suite.

• [SLOW TEST:17.111 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":303,"completed":295,"skipped":5111,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:08:07.111: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 30 08:08:07.949: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 30 08:08:09.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 30 08:08:12.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63789494887, loc:(*time.Location)(0x771eac0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 30 08:08:15.018: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
May 30 08:08:15.359: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:08:15.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6363" for this suite.
STEP: Destroying namespace "webhook-6363-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.978 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":303,"completed":296,"skipped":5134,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:08:16.090: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 30 08:08:16.199: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 68996 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 08:08:16.199: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 68996 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 30 08:08:26.223: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 69042 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 08:08:26.241: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 69042 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 30 08:08:36.257: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 69066 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 08:08:36.257: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 69066 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 30 08:08:46.270: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 69090 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 08:08:46.270: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-a 7e9b41f4-fafe-4b4a-876d-55ac57cf3947 69090 0 2022-05-30 08:08:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 30 08:08:56.347: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-b 12b1d362-db2d-4e96-b759-d0353bc5a710 69111 0 2022-05-30 08:08:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 08:08:56.347: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-b 12b1d362-db2d-4e96-b759-d0353bc5a710 69111 0 2022-05-30 08:08:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 30 08:09:06.361: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-b 12b1d362-db2d-4e96-b759-d0353bc5a710 69135 0 2022-05-30 08:08:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 30 08:09:06.361: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8127 /api/v1/namespaces/watch-8127/configmaps/e2e-watch-test-configmap-b 12b1d362-db2d-4e96-b759-d0353bc5a710 69135 0 2022-05-30 08:08:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-30 08:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:09:16.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8127" for this suite.

• [SLOW TEST:60.292 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":303,"completed":297,"skipped":5142,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:09:16.382: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
May 30 08:09:16.548: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 30 08:09:19.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 create -f -'
May 30 08:09:22.899: INFO: stderr: ""
May 30 08:09:22.899: INFO: stdout: "e2e-test-crd-publish-openapi-2571-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 30 08:09:22.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 delete e2e-test-crd-publish-openapi-2571-crds test-cr'
May 30 08:09:23.242: INFO: stderr: ""
May 30 08:09:23.242: INFO: stdout: "e2e-test-crd-publish-openapi-2571-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 30 08:09:23.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 apply -f -'
May 30 08:09:23.871: INFO: stderr: ""
May 30 08:09:23.871: INFO: stdout: "e2e-test-crd-publish-openapi-2571-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 30 08:09:23.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-8620 --namespace=crd-publish-openapi-8620 delete e2e-test-crd-publish-openapi-2571-crds test-cr'
May 30 08:09:24.000: INFO: stderr: ""
May 30 08:09:24.000: INFO: stdout: "e2e-test-crd-publish-openapi-2571-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 30 08:09:24.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-592517539 --namespace=crd-publish-openapi-8620 explain e2e-test-crd-publish-openapi-2571-crds'
May 30 08:09:24.364: INFO: stderr: ""
May 30 08:09:24.364: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2571-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:09:27.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8620" for this suite.

• [SLOW TEST:10.779 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":303,"completed":298,"skipped":5144,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:09:27.162: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 30 08:09:37.413: INFO: &Pod{ObjectMeta:{send-events-0bec7214-386f-4a6c-bc50-e0ebe436f9d2  events-2324 /api/v1/namespaces/events-2324/pods/send-events-0bec7214-386f-4a6c-bc50-e0ebe436f9d2 dee172f0-b8ce-4aef-9c19-5f5c9c48711f 69242 0 2022-05-30 08:09:27 +0000 UTC <nil> <nil> map[name:foo time:242434591] map[] [] []  [{e2e.test Update v1 2022-05-30 08:09:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-05-30 08:09:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-crscs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-crscs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-crscs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:my-node,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:09:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:09:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:09:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-30 08:09:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.15,PodIP:10.244.0.168,StartTime:2022-05-30 08:09:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-30 08:09:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://a42035cc89fbeb5e23719bdcd038ecd8b8585f514b8ec30d3fbfea7799d13f28,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 30 08:09:39.805: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 30 08:09:41.815: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:09:41.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2324" for this suite.

• [SLOW TEST:14.695 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":303,"completed":299,"skipped":5144,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 30 08:09:41.887: INFO: >>> kubeConfig: /tmp/kubeconfig-592517539
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-957d8840-181b-4c36-a05d-d9f70e53e12d
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 30 08:09:42.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4950" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":303,"completed":300,"skipped":5175,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}
SSSSSSMay 30 08:09:42.087: INFO: Running AfterSuite actions on all nodes
May 30 08:09:42.138: INFO: Running AfterSuite actions on node 1
May 30 08:09:42.138: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":303,"completed":300,"skipped":5181,"failed":3,"failures":["[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]"]}


Summarizing 3 Failures:

[Fail] [sig-apps] Daemon set [Serial] [It] should rollback without unnecessary restarts [Conformance] 
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:423

[Fail] [sig-scheduling] SchedulerPreemption [Serial] [It] validates lower priority pod preemption by critical pod [Conformance] 
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:253

[Fail] [sig-scheduling] SchedulerPreemption [Serial] [It] validates basic preemption works [Conformance] 
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:167

Ran 303 of 5484 Specs in 7267.419 seconds
FAIL! -- 300 Passed | 3 Failed | 0 Pending | 5181 Skipped
--- FAIL: TestE2E (7267.62s)
FAIL

Ginkgo ran 1 suite in 2h1m12.164141273s
Test Suite Failed
2022/05/30 08:09:42 Saving results at /tmp/sonobuoy/results
2022/05/30 08:09:43 running command: exit status 1
+ ret=1
+ exit 1
